{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"01-linear-algebra-machine-learning/","text":"Linear Algebra Foundations for Machine Learning Updated on 10/26/2019 Linear algebra is a tool to help manage large data sets. It comes with techniques that makes manipulation of data very convenient. However, it makes more sense when described using geometry. I will describe it using only a two dimensional plane for sake of visual clarity. A 2-D plane means a data set with only two entries. But same ideas are application to n-dimensional plane or data sets with n entries. Most of the heavy work in linear algebra can be handled using computers. But these fundamental ideas are key to understanding what is happening under the hood. I drew in heavily from the excellent YouTube playlist called The Essence of Linear Algebra from 3Blue1Brown . In this playlist, Grant Sanderson explains the ideas using simple geometric operations using lines and dots on paper, i.e., a two dimensional plane. Here is what I learnt. Linear algebra concepts Linear algebra involves writing numbers in columns (), called vectors and boxes (), called matrices. Vector shows a location A vector, single column of numbers (3, 5) represents a point on a cartesian 2-D plane, or any n-D space for that matter. You need two things to make this work- 1. Define a origin (0, 0) from where the vector points to the location of the point. 2. Define the units of direction in x and y axes. The basis vectors The unit vectors are called basis vectors. They tell you how far to move from the origin and in which direction. The basis vectors along the x-axis, y-axis and z-axis are called i, j and k respectively. Linear combination of vectors and span Any point on the 2-D plane can be reached with sum of two or more vectors. However, its not possible when two vectors are parallel. In that case, all possible linear combinations are on a straight line. There is also a third possibility, when the two vectors are zeros, no linear combination will get you anywhere from zero. The span of vectors v and w is the set of all their linear combinations. In a 3-D space, a span of 2 vectors will most likely be a 2-D plane, or a line if they are parallel. A span of 3 vectors in the same plane will cover the entire 3-D space, or a plane, or a line, or, in the most extreme case, just the origin. Linear transformation of the plane If you stretch, skew, rotate or invert the vector plane keeping- * the same origin same, and * grid lines parallel to one another, then it is called a linear transformation of the plane. The 2-D plane doesn't have to be with all square grids. We can define a plane where the axes are not perpendicular to one another. Linear transformation lets us define a vector on one plane using basis vectors of another plane. Matrix A matrix is just a row of vectors that defines a linear transformation. The first column tells where i lands, the second column defines where j lands, and so forth. Vector dot products When you multiple two vectors using dot product, you get the area of the parallelogram enclosed by the vectors. For a 3-D plane, this will be the volume. Determinant When you transform a plane using a matrix, for example, skew it. The area enclosed by vectors in that plane changes. Determinant tells you by how much the area changes. When talking about a 3-D plane, the determinant shows the change of volume. When the output of a transformation reduces the 2-D plane into a line, the determinant is zero. Rank The number of dimensions of a the output of transformation is called rank. When you apply a matrix on a 2-D plane and the output is a line, we call it has a rank of 1. Inverse matrices An inverse matrix completes the transformation in reverse. The inverse cannot be found if- 1. The rank of transformation < initial dimensions in the space (for example, you squish a 2-D plane into a line) 2. The determinant is zero Change of basis Matrices transform your vector space. In other words, if there is a vector space defined by your friend L, vectors in your vector space. How do you explain your vectors in terms of Victor's world? Lets say matrix A transforms your basis to Victor's world. Therefore, A-1 does the opposite, it tells you where any vector in Victor's world lands in your world. Eigenvectors and eigenvalues When you apply a matrix on a plane, the vectors change direction and are scaled. The vectors that remain on their own span, are called eigenvectors. The value by which these special vectors are scaled is called a eigenvalues. For example, if the transformation is stretching 2x along the horizontal axis, the x-axis is an eigenvector with eigenvalue of 2. Additionally, for a 3-D plane, the eigenvectors with eigenvalues of 1 are rotational axis. Av = lambda*v, where v is eigenvector and lambda is the eigenvalue. Extending to abstract world of data These ideas are explained using a 2-D, and sometimes, a 3-D plane. But you can (somewhat) easily extend it to n-D environment. Even abstract functions such as file compression or dimension reduction, when thought in light of geometric manipulation of space, starts making sense! This is part of ongoing series on my machine learning training. Please let me know what you think :)","title":"Linear Algebra for Machine Learning"},{"location":"01-linear-algebra-machine-learning/#linear-algebra-foundations-for-machine-learning","text":"Updated on 10/26/2019 Linear algebra is a tool to help manage large data sets. It comes with techniques that makes manipulation of data very convenient. However, it makes more sense when described using geometry. I will describe it using only a two dimensional plane for sake of visual clarity. A 2-D plane means a data set with only two entries. But same ideas are application to n-dimensional plane or data sets with n entries. Most of the heavy work in linear algebra can be handled using computers. But these fundamental ideas are key to understanding what is happening under the hood. I drew in heavily from the excellent YouTube playlist called The Essence of Linear Algebra from 3Blue1Brown . In this playlist, Grant Sanderson explains the ideas using simple geometric operations using lines and dots on paper, i.e., a two dimensional plane. Here is what I learnt.","title":"Linear Algebra Foundations for Machine Learning"},{"location":"01-linear-algebra-machine-learning/#linear-algebra-concepts","text":"Linear algebra involves writing numbers in columns (), called vectors and boxes (), called matrices.","title":"Linear algebra concepts"},{"location":"01-linear-algebra-machine-learning/#vector-shows-a-location","text":"A vector, single column of numbers (3, 5) represents a point on a cartesian 2-D plane, or any n-D space for that matter. You need two things to make this work- 1. Define a origin (0, 0) from where the vector points to the location of the point. 2. Define the units of direction in x and y axes.","title":"Vector shows a location"},{"location":"01-linear-algebra-machine-learning/#the-basis-vectors","text":"The unit vectors are called basis vectors. They tell you how far to move from the origin and in which direction. The basis vectors along the x-axis, y-axis and z-axis are called i, j and k respectively.","title":"The basis vectors"},{"location":"01-linear-algebra-machine-learning/#linear-combination-of-vectors-and-span","text":"Any point on the 2-D plane can be reached with sum of two or more vectors. However, its not possible when two vectors are parallel. In that case, all possible linear combinations are on a straight line. There is also a third possibility, when the two vectors are zeros, no linear combination will get you anywhere from zero. The span of vectors v and w is the set of all their linear combinations. In a 3-D space, a span of 2 vectors will most likely be a 2-D plane, or a line if they are parallel. A span of 3 vectors in the same plane will cover the entire 3-D space, or a plane, or a line, or, in the most extreme case, just the origin.","title":"Linear combination of vectors and span"},{"location":"01-linear-algebra-machine-learning/#linear-transformation-of-the-plane","text":"If you stretch, skew, rotate or invert the vector plane keeping- * the same origin same, and * grid lines parallel to one another, then it is called a linear transformation of the plane. The 2-D plane doesn't have to be with all square grids. We can define a plane where the axes are not perpendicular to one another. Linear transformation lets us define a vector on one plane using basis vectors of another plane.","title":"Linear transformation of the plane"},{"location":"01-linear-algebra-machine-learning/#matrix","text":"A matrix is just a row of vectors that defines a linear transformation. The first column tells where i lands, the second column defines where j lands, and so forth.","title":"Matrix"},{"location":"01-linear-algebra-machine-learning/#vector-dot-products","text":"When you multiple two vectors using dot product, you get the area of the parallelogram enclosed by the vectors. For a 3-D plane, this will be the volume.","title":"Vector dot products"},{"location":"01-linear-algebra-machine-learning/#determinant","text":"When you transform a plane using a matrix, for example, skew it. The area enclosed by vectors in that plane changes. Determinant tells you by how much the area changes. When talking about a 3-D plane, the determinant shows the change of volume. When the output of a transformation reduces the 2-D plane into a line, the determinant is zero.","title":"Determinant"},{"location":"01-linear-algebra-machine-learning/#rank","text":"The number of dimensions of a the output of transformation is called rank. When you apply a matrix on a 2-D plane and the output is a line, we call it has a rank of 1.","title":"Rank"},{"location":"01-linear-algebra-machine-learning/#inverse-matrices","text":"An inverse matrix completes the transformation in reverse. The inverse cannot be found if- 1. The rank of transformation < initial dimensions in the space (for example, you squish a 2-D plane into a line) 2. The determinant is zero","title":"Inverse matrices"},{"location":"01-linear-algebra-machine-learning/#change-of-basis","text":"Matrices transform your vector space. In other words, if there is a vector space defined by your friend L, vectors in your vector space. How do you explain your vectors in terms of Victor's world? Lets say matrix A transforms your basis to Victor's world. Therefore, A-1 does the opposite, it tells you where any vector in Victor's world lands in your world.","title":"Change of basis "},{"location":"01-linear-algebra-machine-learning/#eigenvectors-and-eigenvalues","text":"When you apply a matrix on a plane, the vectors change direction and are scaled. The vectors that remain on their own span, are called eigenvectors. The value by which these special vectors are scaled is called a eigenvalues. For example, if the transformation is stretching 2x along the horizontal axis, the x-axis is an eigenvector with eigenvalue of 2. Additionally, for a 3-D plane, the eigenvectors with eigenvalues of 1 are rotational axis. Av = lambda*v, where v is eigenvector and lambda is the eigenvalue.","title":"Eigenvectors and eigenvalues"},{"location":"01-linear-algebra-machine-learning/#extending-to-abstract-world-of-data","text":"These ideas are explained using a 2-D, and sometimes, a 3-D plane. But you can (somewhat) easily extend it to n-D environment. Even abstract functions such as file compression or dimension reduction, when thought in light of geometric manipulation of space, starts making sense! This is part of ongoing series on my machine learning training. Please let me know what you think :)","title":"Extending to abstract world of data"},{"location":"02-git_workflow/","text":"Creating a repo git init git add file-name git commit -m \"comment\" git pull repo-name branch-name git push repo-name branch-name git remote set-url origin git@github.com:username/repo.git Remove a repo git remote remove repo-name Git and Github are like Porn and Pornhub. You cannot create a git repo from the bash.","title":"Creating a repo"},{"location":"02-git_workflow/#creating-a-repo","text":"git init git add file-name git commit -m \"comment\" git pull repo-name branch-name git push repo-name branch-name git remote set-url origin git@github.com:username/repo.git","title":"Creating a repo"},{"location":"02-git_workflow/#remove-a-repo","text":"git remote remove repo-name Git and Github are like Porn and Pornhub. You cannot create a git repo from the bash.","title":"Remove a repo"},{"location":"03-python-virtual-environments-with-jupyter/","text":"What are Python Virtual Environments Python virtual environment is a tool that helps to isolate packages by projects. This is useful to keep the list of installed packages clean. pip list Installing virtual environment in Ubuntu Install the python3 virtual environment package: sudo apt-get install python3-venv Creating a python virtual environment It is good practice to create a folder to keep all the environments. In that folder: python3 -m venv my-env Activate and deactivate a python virtual environment Activate the environment using: source myenv/bin/activate The terminal prompt will show name of the environment. To deactivate: deactivate Delete a virtual environment Deleting the virtual environment folder deletes the environment. rm -d my-env List packages in the environment pip freeze This will do pip list but in a format which can be used by the pip tool to install the packages. Saving the list to a requirement.txt: pip freeze > requirement.txt Replicate packages from an environment Using pip to install the packages listed in the requirement.txt file: pip install -r requirement.txt Using virtual environments in Jupyter Lab Enable the virtual environment and install the ipykernel package. pip install --user ipykernel Add the environment in jupyter using: python -m ipykernel install --user --name=my-env This will show the following in terminal if done correctly. Installed kernelspec myenv in /home/user/.local/share/jupyter/kernels/my-env This will add a new kernel my-env on the Jupyter Lab welcome page. Removing environment from Jupyter Delete the environment from the environment folder and then remove from the Jupyter lab. View a list of kernels available: jupyter kernelspec list Now, to uninstall the kernel, you can type: jupyter kernelspec uninstall my-env References https://janakiev.com/blog/jupyter-virtual-envs/ https://www.youtube.com/watch?v=Kg1Yvry_Ydk","title":"What are Python Virtual Environments"},{"location":"03-python-virtual-environments-with-jupyter/#what-are-python-virtual-environments","text":"Python virtual environment is a tool that helps to isolate packages by projects. This is useful to keep the list of installed packages clean. pip list","title":"What are Python Virtual Environments"},{"location":"03-python-virtual-environments-with-jupyter/#installing-virtual-environment-in-ubuntu","text":"Install the python3 virtual environment package: sudo apt-get install python3-venv","title":"Installing virtual environment in Ubuntu"},{"location":"03-python-virtual-environments-with-jupyter/#creating-a-python-virtual-environment","text":"It is good practice to create a folder to keep all the environments. In that folder: python3 -m venv my-env","title":"Creating a python virtual environment"},{"location":"03-python-virtual-environments-with-jupyter/#activate-and-deactivate-a-python-virtual-environment","text":"Activate the environment using: source myenv/bin/activate The terminal prompt will show name of the environment. To deactivate: deactivate","title":"Activate and deactivate a python virtual environment"},{"location":"03-python-virtual-environments-with-jupyter/#delete-a-virtual-environment","text":"Deleting the virtual environment folder deletes the environment. rm -d my-env","title":"Delete a virtual environment"},{"location":"03-python-virtual-environments-with-jupyter/#list-packages-in-the-environment","text":"pip freeze This will do pip list but in a format which can be used by the pip tool to install the packages. Saving the list to a requirement.txt: pip freeze > requirement.txt","title":"List packages in the environment"},{"location":"03-python-virtual-environments-with-jupyter/#replicate-packages-from-an-environment","text":"Using pip to install the packages listed in the requirement.txt file: pip install -r requirement.txt","title":"Replicate packages from an environment"},{"location":"03-python-virtual-environments-with-jupyter/#using-virtual-environments-in-jupyter-lab","text":"Enable the virtual environment and install the ipykernel package. pip install --user ipykernel Add the environment in jupyter using: python -m ipykernel install --user --name=my-env This will show the following in terminal if done correctly. Installed kernelspec myenv in /home/user/.local/share/jupyter/kernels/my-env This will add a new kernel my-env on the Jupyter Lab welcome page.","title":"Using virtual environments in Jupyter Lab"},{"location":"03-python-virtual-environments-with-jupyter/#removing-environment-from-jupyter","text":"Delete the environment from the environment folder and then remove from the Jupyter lab. View a list of kernels available: jupyter kernelspec list Now, to uninstall the kernel, you can type: jupyter kernelspec uninstall my-env","title":"Removing environment from Jupyter"},{"location":"03-python-virtual-environments-with-jupyter/#references","text":"https://janakiev.com/blog/jupyter-virtual-envs/ https://www.youtube.com/watch?v=Kg1Yvry_Ydk","title":"References"},{"location":"04-ecommerce-payments/","text":"Structure Focus of the article - stakeholders involved and flow of information List of all the parties, what they do Elaborate on each section with Core purpose served Key players Security - list of tools PCI SSL Tokenization Others [?] Resource Ecommerce payments Resources https://www.netsolutions.com/insights/12-factors-to-consider-while-choosing-a-payment-gateway-for-your-e-commerce-store/ https://docs.google.com/spreadsheets/d/1pOC0rN_ahQZVpmNNR3dVqkX4eNxdR4xT-h25uSBJaVk/edit#gid=0 https://www.skyverge.com/blog/woocommerce-pci-compliance/ https://www.bigcommerce.com/blog/payment-gateways/ https://www.ecommerce-nation.com/payment-processing-work/ https://ecommerce-platforms.com/ecommerce-selling-advice/what-is-difference-between-a-payment-gateway-payment-processor-and-a-merchant-account https://jilt.com/blog/beginners-guide-to-payment-processing/ https://www.comalytics.com/e-commerce-payment-gateways/ https://www.quora.com/What-are-the-best-resources-to-learn-about-the-intricacies-of-payments-processing","title":"Structure"},{"location":"04-ecommerce-payments/#structure","text":"Focus of the article - stakeholders involved and flow of information List of all the parties, what they do Elaborate on each section with Core purpose served Key players Security - list of tools PCI SSL Tokenization Others [?] Resource","title":"Structure"},{"location":"04-ecommerce-payments/#ecommerce-payments","text":"","title":"Ecommerce payments"},{"location":"04-ecommerce-payments/#resources","text":"https://www.netsolutions.com/insights/12-factors-to-consider-while-choosing-a-payment-gateway-for-your-e-commerce-store/ https://docs.google.com/spreadsheets/d/1pOC0rN_ahQZVpmNNR3dVqkX4eNxdR4xT-h25uSBJaVk/edit#gid=0 https://www.skyverge.com/blog/woocommerce-pci-compliance/ https://www.bigcommerce.com/blog/payment-gateways/ https://www.ecommerce-nation.com/payment-processing-work/ https://ecommerce-platforms.com/ecommerce-selling-advice/what-is-difference-between-a-payment-gateway-payment-processor-and-a-merchant-account https://jilt.com/blog/beginners-guide-to-payment-processing/ https://www.comalytics.com/e-commerce-payment-gateways/ https://www.quora.com/What-are-the-best-resources-to-learn-about-the-intricacies-of-payments-processing","title":"Resources"},{"location":"05-pubsub-google-clouds/","text":"Why Pub/Sub Pub/Sub seems to be the zapier of Google Cloud Console. It connects senders of data, Publishers , with receivers of data, Subscribers . The is a fundamental problem to solve in any project involving streaming analytics. Why not use the zapier, instead? * Cost savings * Compatibility with GCP tools The quick start guide shows how to create a topic , subscriber and publisher under a Google cloud project. This does not mention anything about connecting apps using APIs or webhooks. Another guide shows how to make a useless app using a number of python scripts and pubsub. Downloaded the Google Cloud SDK . After installation, ran the commands: gcloud pubsub topics list gcloud pubsub subscriptions list This document seems to be the most relevant one, but its pre-requisite is to build the Hello worlld cloud app . How to write the webhook / API endpoints to connect the apps? https://YOUR_PROJECT_ID.REGION_ID.r.appspot.com/pubsub/push?token=YOUR_TOKEN \\ Dataflow lets you create SQL Jobs and connect them with Pub/Sub topic. This becomes the publisher. NOt sure how to connect the subscriber with a webhook. Dataflow also has jupyter notebook that can connect with Cloud Storage. Data Studio BigQuery Table connection is also able to pull accurate order cout and sum of total. However, we need to find out how refunds are coming in Big Query. Resources YouTube playlist YouTube playlist video 2 Python library github Python library documentation Official documentation","title":"05 pubsub google clouds"},{"location":"05-pubsub-google-clouds/#why-pubsub","text":"Pub/Sub seems to be the zapier of Google Cloud Console. It connects senders of data, Publishers , with receivers of data, Subscribers . The is a fundamental problem to solve in any project involving streaming analytics. Why not use the zapier, instead? * Cost savings * Compatibility with GCP tools","title":"Why Pub/Sub"},{"location":"05-pubsub-google-clouds/#_1","text":"The quick start guide shows how to create a topic , subscriber and publisher under a Google cloud project. This does not mention anything about connecting apps using APIs or webhooks. Another guide shows how to make a useless app using a number of python scripts and pubsub. Downloaded the Google Cloud SDK . After installation, ran the commands: gcloud pubsub topics list gcloud pubsub subscriptions list This document seems to be the most relevant one, but its pre-requisite is to build the Hello worlld cloud app . How to write the webhook / API endpoints to connect the apps? https://YOUR_PROJECT_ID.REGION_ID.r.appspot.com/pubsub/push?token=YOUR_TOKEN \\ Dataflow lets you create SQL Jobs and connect them with Pub/Sub topic. This becomes the publisher. NOt sure how to connect the subscriber with a webhook. Dataflow also has jupyter notebook that can connect with Cloud Storage. Data Studio BigQuery Table connection is also able to pull accurate order cout and sum of total. However, we need to find out how refunds are coming in Big Query.","title":""},{"location":"05-pubsub-google-clouds/#resources","text":"YouTube playlist YouTube playlist video 2 Python library github Python library documentation Official documentation","title":"Resources"},{"location":"07-tmux/","text":"Tmux Tmux is a popular terminal multiplexer that helps you be more productive in terminal. Installation sudo apt install tmux Resources Linuxize The missing semester Ham Vocke Screen and Tmux documentation","title":"Tmux"},{"location":"07-tmux/#tmux","text":"Tmux is a popular terminal multiplexer that helps you be more productive in terminal. Installation sudo apt install tmux","title":"Tmux"},{"location":"07-tmux/#resources","text":"Linuxize The missing semester Ham Vocke Screen and Tmux documentation","title":"Resources"},{"location":"08-using-system-clipboard-in-vim/","text":"If you are facing trouble in copying texts to and from VIM and another software, your VIM is not configured to access the system clipboard. Ubuntu 20.04 You need to have the +clipboard feature turned on to access the system clipboard from VIM. In terminal, write vim --version and look for +clipboard If it shows -clipboard , it means that the feature is not installed. I did sudo apt-get vim-gtk to upgrade vim and ensured clipboard feature is added. With this enabled, you should be able to use \"+y to copy and \"+p to paste. In my Ubuntu, Ctrl+Shift+V worked for pasting in Ubuntu. But \"+y is too many keys to press just for copying. I added the following in the .vimrc - set clipboard=unnamedplus This makes default y access the clipboard like the \"+y combo.","title":"08 using system clipboard in vim"},{"location":"08-using-system-clipboard-in-vim/#ubuntu-2004","text":"You need to have the +clipboard feature turned on to access the system clipboard from VIM. In terminal, write vim --version and look for +clipboard If it shows -clipboard , it means that the feature is not installed. I did sudo apt-get vim-gtk to upgrade vim and ensured clipboard feature is added. With this enabled, you should be able to use \"+y to copy and \"+p to paste. In my Ubuntu, Ctrl+Shift+V worked for pasting in Ubuntu. But \"+y is too many keys to press just for copying. I added the following in the .vimrc - set clipboard=unnamedplus This makes default y access the clipboard like the \"+y combo.","title":"Ubuntu 20.04"},{"location":"09-padi-openwater-sec1/","text":"Introduction 5 sections relate to 5 key elements of training. PADI has courses to train for diving in new environments, for example - exploring a wrekage diving with suits in cold water Section 1 Being a diver Every 10m/33ft, the water pressure increases by 1 bar/ata 1 bar = 1 atmospheric pressure For example, 40m/132ft, 5 bar Formula is = (depth / 10) + 1 bar Volume is inversely proportional to pressure Squeeze is when water pressure increases and air volume decreases in - Ears Sinuses Masks Equalize pressure by pinching nose and blowing gently against it. Equalization adds more air, maintiaining the volume Equalize gently and every meter Never use ear plug while diving because they make it hard to equalize During ascend, decreasing pressure can cause lung rupture injuries To avoid, breathe at all times and never hold your breathe Reverse squeeze/block can happen if expanding air is trapped Never dive with any congestion Breathe slowly and gently, never stop Bouyancy control device (BCD) helps control bouyancy With more depth, bouyancy decreases or diver becomes negatively bouyant BCD needs to be adjusted every few meters and tested for neutrality Descend and ascend at a slow and controllable rate In recreational diving, there is always a buddy Equipment 1 Manufacturers make multiple models of gears, visit local PADI shop for guidance Primary consideration for gears - Suitability for the diver and the dive Fit is the size and adjustment for the diver Comfort means the diver can use it without issues Fit the mask strap above ears, over the crown of head Snorkel goes on the left side because regulator is on the right side Adjustible strap fins are often used with wet suit boots Hand fins are called webbed gloves DPV or Diver Propulsion Vehicles are scooters under water Scuba Kit Scuba kit contains four items - BCD Holds kit together Allows for adjusting bouyancy Regulator Delivers breathing air at surrounding pressure Directs exhale air into the water Cyinder Holds air in high pressure Weight system Holds lead weight Mechanism to drop in an emergency Its useful to buy package of all four items together BCD Elements of BCD - Inflatable bladder Cylinder band and harness/jacket Low pressure inflator (LPI) mechanism - a button to inflate, deflate, manually inflate Overpressure / quick exhaust valves or Quick dump valves Weight system Options in BCD - Bouyancy capacity or lift Pockets and D-rings Soulder quick release BCDs can be stylish BCD should have enough capacity to lift diver and equipments to surface Regulators Parts of the regulator Hub - The first stage is the \u201chub\u201d of your regulator. It is a simple and reliable device that supplies air to all the other components. It connects to the cylinder valve either by screwing into it, or with a yoke (clamp system). Either way, an o-ring forms the air-tight seal between the first stage and the cylinder valve. The first stage reduces cylinder pressure to an intermediate pressure, which is 7-10 bar/100-150 psi above the surrounding pressure. Second stage - You breathe from the second stage. It reduces the first stage intermediate pressure to the pressure around you and delivers air only when you inhale \u2013 on demand. It has one-way valves that vent your exhalation. The purge button lets you manually release air from your cylinder. Alternate air source - The alternate air source (or \u201coctopus\u201d) is an extra second stage you use for sharing air with a buddy should the need arise. The most popular alternate air source is a standard second stage on a longer hose. If needed, you pass it to your buddy to share air. Many are brightly colored so a buddy can locate them easily. Low pressure inflator - The low-pressure inflator (LPI) hose is the hose that supplies air to your BCD inflator. When diving with a dry suit, you have two. You use the second to add air to the suit as you descend. SPG - The Submersible Pressure Gauge (SPG) indicates the air pressure remaining in the cylinder. Regulator options include - Yoke or DIN \u2013 The yoke system holds the first stage to the cylinder with a clamp system. With the DIN system, the regulator threads into the valve. The yoke system is older and more widely established, while the DIN system \u2013 which is growing in popularity \u2013 is widely used in Europe and Asia, and has a higher pressure rating. Your instructor or dive shop can advise you which system is the most popular where you are, but you can be ready for both systems. To do this, a common option is to choose a DIN regulator with a yoke adapter, useable on either type valve. Adjustable second stage \u2013 A knob allows small air flow adjustments; this option lets you keep the regulator breathing its best over the course of its maintenance cycle. Dive/Predive switch \u2013 This switch reduces freeflow (air released without control) when the second stage isn\u2019t in your mouth. Cold-water first stage \u2013 In cooler climates, the first stage can freeze, resulting in a freeflow. Special cold-water regulators reduce the likelihood of this by surrounding the first stage with a special liquid. * Regulators require professional overhaul as set by the manufacturers Cylinders Have markings saying date of last visual inspection and other details Cylinders with an Enriched Air Nitrox (EANx) or Nitrox have more oxygen which let longer dive. PADI Enriched Air Diver course teaches how to use these. PADI TecRec program trains technical diving with advanced gears. Burst disk is safety device that releases air before cylinder fails. Cylinders require pressure testing every few years and annual visual testing. Weight system Right distribution of weight is called trim. Trim is your orientation and balance in the water \u2013 generally, the desired trim is a natural horizontal swimming position with your feet parallel to the bottom or slightly elevated. Some BCDs have nonreleasable weight pockets. You use these for trim, but you should have enough releasable weight to assure positive buoyancy. Diver Skills 1 Defog the mask Setting up the scuba kit Gearing up with buddies Inflating and deflating the BCD It is used constantly Inflate before going to water Practise breathing and fine-tuning bouyancy together Breathing underwater Slowly and deeply Never stop Hand signals Regulator clearing Exhalation method Purge method Regulator recovery Clearing water out of mask - hold the top and blow through nose Managing air supply Descending and equalizing - Equalize gently and often \u2013 every metre/few feet. Swimming underwater Alternate Air Source (AAS) use Ascents Swim up Breathe normally Vent expanding air from BCD in small amounts Emergency weight drop BCD oral inflation at surface Exiting the water After the dive","title":"09 padi openwater sec1"},{"location":"09-padi-openwater-sec1/#introduction","text":"5 sections relate to 5 key elements of training. PADI has courses to train for diving in new environments, for example - exploring a wrekage diving with suits in cold water","title":"Introduction"},{"location":"09-padi-openwater-sec1/#section-1","text":"","title":"Section 1"},{"location":"09-padi-openwater-sec1/#being-a-diver","text":"Every 10m/33ft, the water pressure increases by 1 bar/ata 1 bar = 1 atmospheric pressure For example, 40m/132ft, 5 bar Formula is = (depth / 10) + 1 bar Volume is inversely proportional to pressure Squeeze is when water pressure increases and air volume decreases in - Ears Sinuses Masks Equalize pressure by pinching nose and blowing gently against it. Equalization adds more air, maintiaining the volume Equalize gently and every meter Never use ear plug while diving because they make it hard to equalize During ascend, decreasing pressure can cause lung rupture injuries To avoid, breathe at all times and never hold your breathe Reverse squeeze/block can happen if expanding air is trapped Never dive with any congestion Breathe slowly and gently, never stop Bouyancy control device (BCD) helps control bouyancy With more depth, bouyancy decreases or diver becomes negatively bouyant BCD needs to be adjusted every few meters and tested for neutrality Descend and ascend at a slow and controllable rate In recreational diving, there is always a buddy","title":"Being a diver"},{"location":"09-padi-openwater-sec1/#equipment-1","text":"Manufacturers make multiple models of gears, visit local PADI shop for guidance Primary consideration for gears - Suitability for the diver and the dive Fit is the size and adjustment for the diver Comfort means the diver can use it without issues Fit the mask strap above ears, over the crown of head Snorkel goes on the left side because regulator is on the right side Adjustible strap fins are often used with wet suit boots Hand fins are called webbed gloves DPV or Diver Propulsion Vehicles are scooters under water","title":"Equipment 1"},{"location":"09-padi-openwater-sec1/#scuba-kit","text":"Scuba kit contains four items - BCD Holds kit together Allows for adjusting bouyancy Regulator Delivers breathing air at surrounding pressure Directs exhale air into the water Cyinder Holds air in high pressure Weight system Holds lead weight Mechanism to drop in an emergency Its useful to buy package of all four items together","title":"Scuba Kit"},{"location":"09-padi-openwater-sec1/#bcd","text":"Elements of BCD - Inflatable bladder Cylinder band and harness/jacket Low pressure inflator (LPI) mechanism - a button to inflate, deflate, manually inflate Overpressure / quick exhaust valves or Quick dump valves Weight system Options in BCD - Bouyancy capacity or lift Pockets and D-rings Soulder quick release BCDs can be stylish BCD should have enough capacity to lift diver and equipments to surface","title":"BCD"},{"location":"09-padi-openwater-sec1/#regulators","text":"Parts of the regulator Hub - The first stage is the \u201chub\u201d of your regulator. It is a simple and reliable device that supplies air to all the other components. It connects to the cylinder valve either by screwing into it, or with a yoke (clamp system). Either way, an o-ring forms the air-tight seal between the first stage and the cylinder valve. The first stage reduces cylinder pressure to an intermediate pressure, which is 7-10 bar/100-150 psi above the surrounding pressure. Second stage - You breathe from the second stage. It reduces the first stage intermediate pressure to the pressure around you and delivers air only when you inhale \u2013 on demand. It has one-way valves that vent your exhalation. The purge button lets you manually release air from your cylinder. Alternate air source - The alternate air source (or \u201coctopus\u201d) is an extra second stage you use for sharing air with a buddy should the need arise. The most popular alternate air source is a standard second stage on a longer hose. If needed, you pass it to your buddy to share air. Many are brightly colored so a buddy can locate them easily. Low pressure inflator - The low-pressure inflator (LPI) hose is the hose that supplies air to your BCD inflator. When diving with a dry suit, you have two. You use the second to add air to the suit as you descend. SPG - The Submersible Pressure Gauge (SPG) indicates the air pressure remaining in the cylinder. Regulator options include - Yoke or DIN \u2013 The yoke system holds the first stage to the cylinder with a clamp system. With the DIN system, the regulator threads into the valve. The yoke system is older and more widely established, while the DIN system \u2013 which is growing in popularity \u2013 is widely used in Europe and Asia, and has a higher pressure rating. Your instructor or dive shop can advise you which system is the most popular where you are, but you can be ready for both systems. To do this, a common option is to choose a DIN regulator with a yoke adapter, useable on either type valve. Adjustable second stage \u2013 A knob allows small air flow adjustments; this option lets you keep the regulator breathing its best over the course of its maintenance cycle. Dive/Predive switch \u2013 This switch reduces freeflow (air released without control) when the second stage isn\u2019t in your mouth. Cold-water first stage \u2013 In cooler climates, the first stage can freeze, resulting in a freeflow. Special cold-water regulators reduce the likelihood of this by surrounding the first stage with a special liquid. * Regulators require professional overhaul as set by the manufacturers","title":"Regulators"},{"location":"09-padi-openwater-sec1/#cylinders","text":"Have markings saying date of last visual inspection and other details Cylinders with an Enriched Air Nitrox (EANx) or Nitrox have more oxygen which let longer dive. PADI Enriched Air Diver course teaches how to use these. PADI TecRec program trains technical diving with advanced gears. Burst disk is safety device that releases air before cylinder fails. Cylinders require pressure testing every few years and annual visual testing.","title":"Cylinders"},{"location":"09-padi-openwater-sec1/#weight-system","text":"Right distribution of weight is called trim. Trim is your orientation and balance in the water \u2013 generally, the desired trim is a natural horizontal swimming position with your feet parallel to the bottom or slightly elevated. Some BCDs have nonreleasable weight pockets. You use these for trim, but you should have enough releasable weight to assure positive buoyancy.","title":"Weight system"},{"location":"09-padi-openwater-sec1/#diver-skills-1","text":"Defog the mask Setting up the scuba kit Gearing up with buddies Inflating and deflating the BCD It is used constantly Inflate before going to water Practise breathing and fine-tuning bouyancy together Breathing underwater Slowly and deeply Never stop Hand signals Regulator clearing Exhalation method Purge method Regulator recovery Clearing water out of mask - hold the top and blow through nose Managing air supply Descending and equalizing - Equalize gently and often \u2013 every metre/few feet. Swimming underwater Alternate Air Source (AAS) use Ascents Swim up Breathe normally Vent expanding air from BCD in small amounts Emergency weight drop BCD oral inflation at surface Exiting the water After the dive","title":"Diver Skills 1"},{"location":"10-padi-openwater-sec2/","text":"Being a diver 2 Seeing and hearing Things appear closer than they are Colors are lost at different depths Hard to tell the direction of sound under water Sound travels 4x speed of air Swimming and moving Relax, don't rush Trim properly for streamlined swim Staying warm Dry suit vs wet suit Hypothermia - chill and shivering When uncomfortable, end the dive Breathing underwater Air is denser underwater, takes time to breathe Dead Air Space is air filled spaces out of lungs, where respiration isnt happening. The first air you inhale (\u201cdead\u201d air) with each breath is the air from your last exhalation, so it is higher in carbon dioxide. Breathing slowly and deeply reduces the proportion of dead air in each breath. Airway control is the skill of breathing past the residual water in regulator / snorkel Inhale slowly Tough tongue to the roof to stop water below Look a bit downward Overexertion symptoms Fatigue Labored breathing A feeling of suffocation or air starvation Weakness Anxiety Headache Muscle cramping A tendency to panic Always breath slowly and deeply. If not, stop to relax or abort dive. Peak Performance Buoyancy course trains about controlling bouyancy using center to gravity, center of bouyancy, streamlined body position and breathing techniques. Managing air pressure with buddy Calculate turn pressure - the SPG pressure at which divers turn around Swimming at the surface Use mask, snorkel Descent in open water Five steps of descent Confirm that your buddies are ready. Orient yourselves to something at the surface or underwater, such as the boat or a landmark. Switch from your snorkel to your regulator. Check, and if necessary activate, your dive computer or timer. Signal \u201cdescend\u201d and, with your buddies, slowly deflate your BCD. Begin equalizing immediately, as soon as your head goes underwater. You descend faster as you go deeper. Add to BCD to compensate, in small ammounts and frequently. Ideally, you rech target depth with neutral bouyancy Ascent in open water Five steps of ascent Signal \u201cup\u201d and confirm that your buddies are ready. Check your dive computer to be sure you\u2019re within its limits. Look up and hold up your BCD deflator hose. Do not add air to your BCD. If you\u2019re properly weighted and neutrally buoyant, you only need to start swimming up gently. Ascend slowly \u2013 no faster than your dive computer\u2019s maximum rate. Release air expanding in your BCD to control your buoyancy so you don\u2019t start to rise too fast. Ascend no faster than 3 metres/10 feet each 10 seconds Look up and turn as you ascend Ascend without reference and navigating One common technique is for one buddy to navigate while the other controls the ascent. Safety stops are normal part of ascend for prudent divers. It is the pause in ascent between 6 metres/20 feet and 3 metres/10 feet (commonly 5 metres/15 feet) for three to five minutes. Helps avoid decompression sickness (DCS) Keep your hand up as you break the surface. Continue to breathe from your regulator as you inflate your BCD. After making sure you\u2019re floating comfortably, switch to your snorkel. Equipment 2 Exposure suits 1 Wet suits Dry suits Skin suits 10C to 30C Below 18C No insulation 50F to 86F Below 65F Gloves and hoods for cold water Cutting tools Four types Dive knife Dive tool Shears Z knife Mounting in accesible positions Dive instruments SPG Compass Dive watches Dive computer Thermometer Skills as diver 2 Deep water entry - giant stride Enter the water fully equipped with a partially inflated BCD. Remember to breathe from your regulator and hold it, and your mask, as you enter. Hold them until you\u2019re stable on the surface. Look straight ahead and step in \u2013 don\u2019t jump. Signal that you\u2019re \u201cokay\u201d and clear the entry area. Switch to your snorkel. Weight check You should float at eye level with an empty BCD and holding a normal breath. When you exhale, you should slowly sink. After adjusting so you float at eye level, add two kilograms/five pounds if you checked with a full cylinder. This is because the air in it has weight, so you will become lighter as you use up your air. Two kilograms/five pounds adjusts for the air in a typical cylinder. Dealing with loose cylinder band If it comes loose while exiting, it is often simplest to ease back into the water, slip out of the kit and then lift or carry it out of the water by the cylinder valve. If it comes loose before the dive, help the diver remove the scuba kit for retightening. If it comes loose during a dive, it is not usually an emergency, because the cylinder tends to stay with the diver. Signal the affected diver so you can tighten it, or signal your buddy if it is your cylinder band that\u2019s loose. Snorkel/Regulator exchange Swap, clear water, breath slowly. Practice until effortless. Neutral bouyancy Fin pivot - pivot up fin when inhaling, drop when exhaling Adjust by adding Mask removal, replacement and no mask breathing Clear a fully flooded mask just as you do a partially flooded one; you just exhale a little longer. Train for a minute without mask to simulate the time of getting back to surface. Disconnect Low-pressure Inflator An unlikely but possible malfunction is for your BCD (or dry suit) inflator to stick. Your response is to disconnect the hose supplying air to it If disconnecting is taking time, get in a vertical position and hold open the exhaust valve while continuing to disconnect the inflator hose. For colder water diving, you can get low-pressure hoses with oversized release fittings to make this easier while wearing gloves. Air (Gas) Depletion exercise This exercise lets you experience what it feels like to run out of air, which is part of learning to respond appropriately. Air awareness and managin air supply Besides knowing how to read your SPG and plan your air use, develop the habit of knowing approximately how much air you have at all times. Should be able to answer within 20 bar/300 psi without rechecking. Deep water exit Switch back to your regulator before exiting.","title":"10 padi openwater sec2"},{"location":"10-padi-openwater-sec2/#being-a-diver-2","text":"","title":"Being a diver 2"},{"location":"10-padi-openwater-sec2/#seeing-and-hearing","text":"Things appear closer than they are Colors are lost at different depths Hard to tell the direction of sound under water Sound travels 4x speed of air","title":"Seeing and hearing"},{"location":"10-padi-openwater-sec2/#swimming-and-moving","text":"Relax, don't rush Trim properly for streamlined swim","title":"Swimming and moving"},{"location":"10-padi-openwater-sec2/#staying-warm","text":"Dry suit vs wet suit Hypothermia - chill and shivering When uncomfortable, end the dive","title":"Staying warm"},{"location":"10-padi-openwater-sec2/#breathing-underwater","text":"Air is denser underwater, takes time to breathe Dead Air Space is air filled spaces out of lungs, where respiration isnt happening. The first air you inhale (\u201cdead\u201d air) with each breath is the air from your last exhalation, so it is higher in carbon dioxide. Breathing slowly and deeply reduces the proportion of dead air in each breath. Airway control is the skill of breathing past the residual water in regulator / snorkel Inhale slowly Tough tongue to the roof to stop water below Look a bit downward Overexertion symptoms Fatigue Labored breathing A feeling of suffocation or air starvation Weakness Anxiety Headache Muscle cramping A tendency to panic Always breath slowly and deeply. If not, stop to relax or abort dive. Peak Performance Buoyancy course trains about controlling bouyancy using center to gravity, center of bouyancy, streamlined body position and breathing techniques.","title":"Breathing underwater"},{"location":"10-padi-openwater-sec2/#managing-air-pressure-with-buddy","text":"Calculate turn pressure - the SPG pressure at which divers turn around","title":"Managing air pressure with buddy"},{"location":"10-padi-openwater-sec2/#swimming-at-the-surface","text":"Use mask, snorkel","title":"Swimming at the surface"},{"location":"10-padi-openwater-sec2/#descent-in-open-water","text":"Five steps of descent Confirm that your buddies are ready. Orient yourselves to something at the surface or underwater, such as the boat or a landmark. Switch from your snorkel to your regulator. Check, and if necessary activate, your dive computer or timer. Signal \u201cdescend\u201d and, with your buddies, slowly deflate your BCD. Begin equalizing immediately, as soon as your head goes underwater. You descend faster as you go deeper. Add to BCD to compensate, in small ammounts and frequently. Ideally, you rech target depth with neutral bouyancy","title":"Descent in open water"},{"location":"10-padi-openwater-sec2/#ascent-in-open-water","text":"Five steps of ascent Signal \u201cup\u201d and confirm that your buddies are ready. Check your dive computer to be sure you\u2019re within its limits. Look up and hold up your BCD deflator hose. Do not add air to your BCD. If you\u2019re properly weighted and neutrally buoyant, you only need to start swimming up gently. Ascend slowly \u2013 no faster than your dive computer\u2019s maximum rate. Release air expanding in your BCD to control your buoyancy so you don\u2019t start to rise too fast. Ascend no faster than 3 metres/10 feet each 10 seconds Look up and turn as you ascend Ascend without reference and navigating One common technique is for one buddy to navigate while the other controls the ascent. Safety stops are normal part of ascend for prudent divers. It is the pause in ascent between 6 metres/20 feet and 3 metres/10 feet (commonly 5 metres/15 feet) for three to five minutes. Helps avoid decompression sickness (DCS) Keep your hand up as you break the surface. Continue to breathe from your regulator as you inflate your BCD. After making sure you\u2019re floating comfortably, switch to your snorkel.","title":"Ascent in open water"},{"location":"10-padi-openwater-sec2/#equipment-2","text":"","title":"Equipment 2"},{"location":"10-padi-openwater-sec2/#exposure-suits-1","text":"Wet suits Dry suits Skin suits 10C to 30C Below 18C No insulation 50F to 86F Below 65F Gloves and hoods for cold water","title":"Exposure suits 1"},{"location":"10-padi-openwater-sec2/#cutting-tools","text":"Four types Dive knife Dive tool Shears Z knife Mounting in accesible positions","title":"Cutting tools"},{"location":"10-padi-openwater-sec2/#dive-instruments","text":"SPG Compass Dive watches Dive computer Thermometer","title":"Dive instruments"},{"location":"10-padi-openwater-sec2/#skills-as-diver-2","text":"","title":"Skills as diver 2"},{"location":"10-padi-openwater-sec2/#deep-water-entry-giant-stride","text":"Enter the water fully equipped with a partially inflated BCD. Remember to breathe from your regulator and hold it, and your mask, as you enter. Hold them until you\u2019re stable on the surface. Look straight ahead and step in \u2013 don\u2019t jump. Signal that you\u2019re \u201cokay\u201d and clear the entry area. Switch to your snorkel.","title":"Deep water entry - giant stride"},{"location":"10-padi-openwater-sec2/#weight-check","text":"You should float at eye level with an empty BCD and holding a normal breath. When you exhale, you should slowly sink. After adjusting so you float at eye level, add two kilograms/five pounds if you checked with a full cylinder. This is because the air in it has weight, so you will become lighter as you use up your air. Two kilograms/five pounds adjusts for the air in a typical cylinder.","title":"Weight check"},{"location":"10-padi-openwater-sec2/#dealing-with-loose-cylinder-band","text":"If it comes loose while exiting, it is often simplest to ease back into the water, slip out of the kit and then lift or carry it out of the water by the cylinder valve. If it comes loose before the dive, help the diver remove the scuba kit for retightening. If it comes loose during a dive, it is not usually an emergency, because the cylinder tends to stay with the diver. Signal the affected diver so you can tighten it, or signal your buddy if it is your cylinder band that\u2019s loose.","title":"Dealing with loose cylinder band"},{"location":"10-padi-openwater-sec2/#snorkelregulator-exchange","text":"Swap, clear water, breath slowly. Practice until effortless.","title":"Snorkel/Regulator exchange"},{"location":"10-padi-openwater-sec2/#neutral-bouyancy","text":"Fin pivot - pivot up fin when inhaling, drop when exhaling Adjust by adding","title":"Neutral bouyancy"},{"location":"10-padi-openwater-sec2/#mask-removal-replacement-and-no-mask-breathing","text":"Clear a fully flooded mask just as you do a partially flooded one; you just exhale a little longer. Train for a minute without mask to simulate the time of getting back to surface.","title":"Mask removal, replacement and no mask breathing"},{"location":"10-padi-openwater-sec2/#disconnect-low-pressure-inflator","text":"An unlikely but possible malfunction is for your BCD (or dry suit) inflator to stick. Your response is to disconnect the hose supplying air to it If disconnecting is taking time, get in a vertical position and hold open the exhaust valve while continuing to disconnect the inflator hose. For colder water diving, you can get low-pressure hoses with oversized release fittings to make this easier while wearing gloves.","title":"Disconnect Low-pressure Inflator"},{"location":"10-padi-openwater-sec2/#air-gas-depletion-exercise","text":"This exercise lets you experience what it feels like to run out of air, which is part of learning to respond appropriately.","title":"Air (Gas) Depletion exercise"},{"location":"10-padi-openwater-sec2/#air-awareness-and-managin-air-supply","text":"Besides knowing how to read your SPG and plan your air use, develop the habit of knowing approximately how much air you have at all times. Should be able to answer within 20 bar/300 psi without rechecking.","title":"Air awareness and managin air supply"},{"location":"10-padi-openwater-sec2/#deep-water-exit","text":"Switch back to your regulator before exiting.","title":"Deep water exit"},{"location":"11-padi-openwater-sec3/","text":"Introduction This section is about planning the dive considering environment, personal limits and acquatic conditions. Dive environments and conditions Six conditions affect divers - Temperature Visibility Water movement Bottom composition Aquatic life Sunlight Temperature Climate and depth affects temperature It is important to base exposure protection on the temperature at the deepest part of the dive Visibility Defined by how far you can see a horizontal diver Ranges from 0 to 60m Factors affecting visibility Water current Weather Plankton Nature and composition of particles in water - heavy particles sediment faster than lighter ones like mud and clay. Diving in reduced visibility Difficult to stay close to buddy Losing orientation while navigating Losing orientation while ascending or descending PADI Underwater Navigator and Search and Recovery Diver courses Diving in clear water - stay within 2 sec of buddies Water movement Wave or current PADI Drift diving course Only swim against weak current Particularly at surface, swim perpendicular to the current Swim against the current to the diving location so you can return along the current Bottom composition Types - Silt/Mud Sand Rock Coral vegetation Sunlight Sunburn is the most common diving injury Sunscreens that do to affect aquatic life Fresh water vs salt water Less bouyant in fresh water Thermoclines are more distinct in fresh water Overhead environments Places where divers can swim into but cannot swim up to the surface Requires special training This course is open water diving - where the diver can directly go to surface Assessing conditions Assess conditions based on - The weather The season Water motion Water appearance Reports online and from other divers Dives made at similar sites in the area Experience Diver should make the decision to continue with the dive or not Dive with limits Don't exceed the limits set by diver's experience and training PADI Advanced Open Water Diver course Acquatic life Active vs passive interaction Hazardous organisms Stings or punctures - Jelly fish Sting rays Sea urchins Portuguese man-o-war Lion fish and scorpion fish Cone shells Fire coral and other hydroids Sea nettles Bites Moray eels Barracida and other fish Clawed lobsters/crabs Trigger fish Crocodilians Some sharks Snakes Octopus Avoid wearing dangling, shiny jewelries Avoid dead body parts - jellyfish or its body parts can sting even when dead Managing aquatic injury Assure breathing While diving in salt water, rinse stings with salt water Don't rub stings Vinegar on jellyfish, fire coral and other hydroid stings Immerse stings from fish spines, scorpion/lion fish and stingrays in hot <50C/120F water Use forcepts to remove spines Treat bites like an wound PADI Rescue Diver, Emergency First Response Primary Care (CPR) and Secondary Care (First Aid) courses Potentially aggressive animals Watch and enjoy This is rare Don't swim towards If uncomfortable, slowly swim away while keeping an eye on it Aquatic plant hazards Keep the kit streamlined Avoid dense areas Try swimming up instead of turning around if stuck Don't struggle, pull or twist Use knife or cutting tool if necessary Project AWARE Non-profit organization of scuba divers to protect the environment Diving from shore Typically, one puts on all equipment except masks, fins, snorkel and gloves, then conducts the safety check Keep your mask on the surface Walk till chest depth then the BCD can support, and wear the fin Wear the fin in the beginning and walk backwards Ascend or descend along the line from the surface float that divers can tow Shore diving through mild surf Some shore diving involves entering and exiting through breaking waves - surf Surf or the surf zome is the area in which waves break Waves break in shallow areas because the bottom stops the water, making the top \"tip over\" and break If waves break along a continuous line but with significant gap, its a channel or a rip current Surge and undertow - avoid Longshore current Waves approaching shore at an angle causes a current parallel to the shoreline add the image Plan the exit considering the current Rip current Occurs when waves push water over a long obstruction, such as reef or sandbar add the image Can be very strong, so don't swim against it Establish bouyancy and swim parallel to shore until out of the current Upwelling Wind from shore can push surface water away bringing cooler deeper water to the top Tides Differences between high and low tides can be from unoticable to 6m/20ft in some areas Tide affects three environmental factors related to diving - Currents Depth Visibility Tides are regional. Ask, research or look at tide tables Diving from boats - preparations Avoid seasickness Bow, stern, port, starboard, windward and leeward Diving from boats - procedures Roll calls Dive site briefing Recall signal and procedure Dive planning includes getting in and getting out of the water Stay where visible to crew of the boat Do not swim just below the surface Boat diving exit procedure Stay out from underneath divers climbing th ladder Hang on to the rope when there is a current and wait for turn Switch from snorkel to regulator while climbing Hand up equipments, like camera Hold ladder, remove fins Breathe on regulator and keep masks on until all the way up to the deck Boat diving with moderate current The trail line The swim line The moor/anchor line Dive boat not in sight Stay calm Use surface signalling devices Wait, or swim towards safety, if within reasonable distance, slowly Dive planning Advance planning - deciding to go on the dive Buddy Dive site Objective Logistics Check dive conditions - weather, surf, tides, etc. Preparation planning Inspect gear Cylinder filled Recheck gear Pack Last minute preparation Recheck weather, surf, conditions Create a safety contact Predive planning Actual dive plan Evaluate the conditions Decide whether to continue on diving Agree on technique where/how to enter the course to follow techniques during the dive where/how to exit Review signals and communication Buddy separation procedures Time, depth and air supply limits Discuss emergency procedures Example dive planning slate.","title":"11 padi openwater sec3"},{"location":"11-padi-openwater-sec3/#introduction","text":"This section is about planning the dive considering environment, personal limits and acquatic conditions.","title":"Introduction"},{"location":"11-padi-openwater-sec3/#dive-environments-and-conditions","text":"Six conditions affect divers - Temperature Visibility Water movement Bottom composition Aquatic life Sunlight","title":"Dive environments and conditions"},{"location":"11-padi-openwater-sec3/#temperature","text":"Climate and depth affects temperature It is important to base exposure protection on the temperature at the deepest part of the dive","title":"Temperature"},{"location":"11-padi-openwater-sec3/#visibility","text":"Defined by how far you can see a horizontal diver Ranges from 0 to 60m Factors affecting visibility Water current Weather Plankton Nature and composition of particles in water - heavy particles sediment faster than lighter ones like mud and clay. Diving in reduced visibility Difficult to stay close to buddy Losing orientation while navigating Losing orientation while ascending or descending PADI Underwater Navigator and Search and Recovery Diver courses Diving in clear water - stay within 2 sec of buddies","title":"Visibility"},{"location":"11-padi-openwater-sec3/#water-movement","text":"Wave or current PADI Drift diving course Only swim against weak current Particularly at surface, swim perpendicular to the current Swim against the current to the diving location so you can return along the current","title":"Water movement"},{"location":"11-padi-openwater-sec3/#bottom-composition","text":"Types - Silt/Mud Sand Rock Coral vegetation","title":"Bottom composition"},{"location":"11-padi-openwater-sec3/#sunlight","text":"Sunburn is the most common diving injury Sunscreens that do to affect aquatic life","title":"Sunlight"},{"location":"11-padi-openwater-sec3/#fresh-water-vs-salt-water","text":"Less bouyant in fresh water Thermoclines are more distinct in fresh water","title":"Fresh water vs salt water"},{"location":"11-padi-openwater-sec3/#overhead-environments","text":"Places where divers can swim into but cannot swim up to the surface Requires special training This course is open water diving - where the diver can directly go to surface","title":"Overhead environments"},{"location":"11-padi-openwater-sec3/#assessing-conditions","text":"Assess conditions based on - The weather The season Water motion Water appearance Reports online and from other divers Dives made at similar sites in the area Experience Diver should make the decision to continue with the dive or not","title":"Assessing conditions"},{"location":"11-padi-openwater-sec3/#dive-with-limits","text":"Don't exceed the limits set by diver's experience and training PADI Advanced Open Water Diver course","title":"Dive with limits"},{"location":"11-padi-openwater-sec3/#acquatic-life","text":"Active vs passive interaction","title":"Acquatic life"},{"location":"11-padi-openwater-sec3/#hazardous-organisms","text":"Stings or punctures - Jelly fish Sting rays Sea urchins Portuguese man-o-war Lion fish and scorpion fish Cone shells Fire coral and other hydroids Sea nettles Bites Moray eels Barracida and other fish Clawed lobsters/crabs Trigger fish Crocodilians Some sharks Snakes Octopus Avoid wearing dangling, shiny jewelries Avoid dead body parts - jellyfish or its body parts can sting even when dead Managing aquatic injury Assure breathing While diving in salt water, rinse stings with salt water Don't rub stings Vinegar on jellyfish, fire coral and other hydroid stings Immerse stings from fish spines, scorpion/lion fish and stingrays in hot <50C/120F water Use forcepts to remove spines Treat bites like an wound PADI Rescue Diver, Emergency First Response Primary Care (CPR) and Secondary Care (First Aid) courses Potentially aggressive animals Watch and enjoy This is rare Don't swim towards If uncomfortable, slowly swim away while keeping an eye on it","title":"Hazardous organisms"},{"location":"11-padi-openwater-sec3/#aquatic-plant-hazards","text":"Keep the kit streamlined Avoid dense areas Try swimming up instead of turning around if stuck Don't struggle, pull or twist Use knife or cutting tool if necessary","title":"Aquatic plant hazards"},{"location":"11-padi-openwater-sec3/#project-aware","text":"Non-profit organization of scuba divers to protect the environment","title":"Project AWARE"},{"location":"11-padi-openwater-sec3/#diving-from-shore","text":"Typically, one puts on all equipment except masks, fins, snorkel and gloves, then conducts the safety check Keep your mask on the surface Walk till chest depth then the BCD can support, and wear the fin Wear the fin in the beginning and walk backwards Ascend or descend along the line from the surface float that divers can tow","title":"Diving from shore"},{"location":"11-padi-openwater-sec3/#shore-diving-through-mild-surf","text":"Some shore diving involves entering and exiting through breaking waves - surf Surf or the surf zome is the area in which waves break Waves break in shallow areas because the bottom stops the water, making the top \"tip over\" and break If waves break along a continuous line but with significant gap, its a channel or a rip current Surge and undertow - avoid","title":"Shore diving through mild surf"},{"location":"11-padi-openwater-sec3/#longshore-current","text":"Waves approaching shore at an angle causes a current parallel to the shoreline add the image Plan the exit considering the current","title":"Longshore current"},{"location":"11-padi-openwater-sec3/#rip-current","text":"Occurs when waves push water over a long obstruction, such as reef or sandbar add the image Can be very strong, so don't swim against it Establish bouyancy and swim parallel to shore until out of the current","title":"Rip current"},{"location":"11-padi-openwater-sec3/#upwelling","text":"Wind from shore can push surface water away bringing cooler deeper water to the top","title":"Upwelling"},{"location":"11-padi-openwater-sec3/#tides","text":"Differences between high and low tides can be from unoticable to 6m/20ft in some areas Tide affects three environmental factors related to diving - Currents Depth Visibility Tides are regional. Ask, research or look at tide tables","title":"Tides"},{"location":"11-padi-openwater-sec3/#diving-from-boats-preparations","text":"Avoid seasickness Bow, stern, port, starboard, windward and leeward","title":"Diving from boats - preparations"},{"location":"11-padi-openwater-sec3/#diving-from-boats-procedures","text":"Roll calls Dive site briefing Recall signal and procedure Dive planning includes getting in and getting out of the water Stay where visible to crew of the boat Do not swim just below the surface","title":"Diving from boats - procedures"},{"location":"11-padi-openwater-sec3/#boat-diving-exit-procedure","text":"Stay out from underneath divers climbing th ladder Hang on to the rope when there is a current and wait for turn Switch from snorkel to regulator while climbing Hand up equipments, like camera Hold ladder, remove fins Breathe on regulator and keep masks on until all the way up to the deck","title":"Boat diving exit procedure"},{"location":"11-padi-openwater-sec3/#boat-diving-with-moderate-current","text":"The trail line The swim line The moor/anchor line","title":"Boat diving with moderate current"},{"location":"11-padi-openwater-sec3/#dive-boat-not-in-sight","text":"Stay calm Use surface signalling devices Wait, or swim towards safety, if within reasonable distance, slowly","title":"Dive boat not in sight"},{"location":"11-padi-openwater-sec3/#dive-planning","text":"Advance planning - deciding to go on the dive Buddy Dive site Objective Logistics Check dive conditions - weather, surf, tides, etc. Preparation planning Inspect gear Cylinder filled Recheck gear Pack Last minute preparation Recheck weather, surf, conditions Create a safety contact Predive planning Actual dive plan Evaluate the conditions Decide whether to continue on diving Agree on technique where/how to enter the course to follow techniques during the dive where/how to exit Review signals and communication Buddy separation procedures Time, depth and air supply limits Discuss emergency procedures Example dive planning slate.","title":"Dive planning"},{"location":"12-padi-openwater-sec3-problem-management/","text":"Prevention Recommended courses Advanced open water diver course Rescue diver course Emergency first response primary and secondary care courses Emergency oxygen provider course Surface problem management - Responsive diver Most diver-in-distress situations occur at the surface Establish positive buoyancy on the surface inflate BCD, or drop weights Common surface problems overexertion leg muscle cramps choking on inhaled water Assisting a responsive diver at surface A diver who is breathing, alert and active is considered responsive. Two types in control / not panicked out of control / panicked Four steps of helping Establish buoyancy for yourself and the diver. If you must make contact to help, make yourself buoyant before doing so, then inflate the victim\u2019s BCD and/or drop weights after you make contact. Calm the diver by reassuring, offering encouragement and asking the person to relax. Help the diver reestablish breathing control. As necessary, assist the diver to the boat or shore. Even divers who don\u2019t panic may need assistance due to leg cramps or being overly tired. Surface problem management - Unresponsive diver A diver may be unresponsive or unconscious for one of the following reasons inhaling water extreme fatigue heart attack lung over expansion injuries panic inefficient breathing throat blockage exhaustion, among others An unresponsive diver does not move and does not respond when tapped or spoken to. Confirm responsiveness if a diver floats without moving. A diver who surfaces alone, calls for help or shows signs of panic, then stops moving, should also be considered unresponsive until you confirm otherwise. Assisting unresponsive diver at surface Four steps Establish buoyancy Call of help Check for breathing Continue rescue breathing during the tow Underwater problem management Three ways to avoid - Relax when diving Plan air usage and monitor closely Dive withing limits of experience and training Overexertion Symptoms - feeling of air starvation exhausted Steps - Signal \u201cstop\u201d or \u201chold\u201d and indicate that you need to rest. Continue at a reduced pace after you recover. If you can\u2019t return to a relaxed state, end the dive. If conditions are adding to overexertion, it may be best to abort the dive. Freeflowing regulator If regulators fail, they release air continuously (called a freeflow). Do not seal your mouth on the mouthpiece. Hold the second stage in your hand and press the mouthpiece outside your lips. Let the excess air escape freely. You may insert only one end of the mouthpiece into your mouth if it helps. Breathe the air you need by \u201csipping\u201d it, Entanglement Prevent by - Moving slowly Watching where you are going Keeping equipment streamlined Do not turn or twist. Extreme cases might require having to slip out of scuba kit and putting it back on. Running low / out of air underwater Ascend Ascending with enough air allows you to control your ascent and to make a safety stop. Four options - Make a normal ascent Do this if you\u2019re very low on air (you feel some breathing resistance), but your cylinder isn\u2019t completely empty. As you ascend, you can get more air from your cylinder, because the surrounding water pressure decreases. Breathe lightly (but continuously) and make a controlled, continuous ascent to the surface. Do not attempt a safety stop. Ascend using an alternate air source Think of this as your best, all around choice when you have an alternate air source immediately available. During your predive safety check, confirm what alternate air sources your buddies have and how to secure them. Test breathe your alternate air source(s). To use an alternate air source supplied by a buddy, you need to stay close to your buddy(ies). Controlled Emergency Swimming Ascent (CESA) This is the best choice if you were completely out of air, no deeper than approximately 6 to 9 metres/20 to 30 feet, the surface is closer than your buddy(ies) or another diver, and you have no other alternate air source. Simply look up and swim to the surface making a continuous \u201cahhhhh\u201d sound into your regulator. The \u201cahhhhh\u201d sound assures that you exhale expanding gas, which is necessary to avoid lung overexpansion injury. Leave all your gear in place and keep the regulator in your mouth. Do not drop your weights to start your ascent. Ascend at a safe rate. The ascent gets easier as you ascend because air expanding in your BCD increases your buoyancy. Vent air as needed to maintain a proper rate. Bouyant Emergency Ascent Use this option when you are too far from your buddy(ies) or another diver, have no other alternate air source and are so deep that you doubt you can reach the surface any other way. You make a buoyant emergency ascent exactly like a Controlled Emergency Swimming Ascent, except you ditch your weights and exceed a safe rate. Again, look up and make the \u201cahhhhh\u201d sound as you ascend. Because you exceed a safe rate, this method has more risk than the other options (which is why it is your last choice), but is obviously better than staying on the bottom without air. As you near the surface, you can flare out your arms and legs to create drag and slow your ascent. You can\u2019t inflate your BCD by using the low-pressure inflator, because you have an empty cylinder. Instead, inflate your BCD orally and/or drop your weights. Assisting unresponsive diver underwater Priority is to get the diver to surface Procedure - Swim the diver to the surface. If necessary, use the diver\u2019s BCD and/or drop weights to make the victim buoyant. If the diver\u2019s regulator is in the mouth, hold it there. If it is not, don\u2019t waste time trying to replace it. Ascend at a safe rate. If the diver\u2019s buoyancy becomes too great for a safe ascent rate, let the victim go. Finish a safe ascent and resume the rescue at the surface. Keep yourself safe. You can\u2019t help someone else if you have troubles of your own. At the surface, follow the priorities and procedures for an unresponsive diver at the surface. First responded care for diving-related emergencies A diver who is or was unresponsive should be considered serious medical emergencies. Drowning or otherwise becoming unresponsive underwater, pressure-related injuries and medical conditions not directly related to diving (like heart attack) can cause these signs (what you observe) and symptoms (what the victim feels): difficulty breathing unconsciousness unclear thinking visual problems paralysis chest pain lowered alertness cardiac and respiratory arrest Assisting unresponsive diver out of water General steps Keep the diver\u2019s airway open and check for breathing. Provide rescue breathing or CPR as necessary. Do not use abdominal thrusts unless you are unable to provide rescue breathing due to a suspected obstruction. Inhaled water, if present, does not prevent rescue breaths. You don\u2019t have to try to clear it. If the diver is unresponsive but breathing, keep the diver lying level on the left side (recovery position). This position is not more important than transporting the diver to safety or providing rescue breaths or CPR if necessary. Check the diver\u2019s breathing frequently. If the diver has regained responsiveness, keep the diver lying down comfortably. Administer emergency oxygen as soon as possible. Keep the diver still and maintain a normal body temperature by protecting from heat or cold. Continue to provide care until emergency medical care arrives. If you can\u2019t accompany the diver to medical care, write down as much background information as possible about the individual and the dive, and attach it to the diver in a conspicuous place. Provide only information relevant to care, such as the dive profile, emergency care provided, emergency contact information, and any known medical conditions. Write only facts. Do not speculate or guess \u2013 bad information is worse than no information. Some conditions, such as drowning, can have delayed serious, potentially fatal, consequences hours after the incident.","title":"12 padi openwater sec3 problem management"},{"location":"12-padi-openwater-sec3-problem-management/#prevention","text":"Recommended courses Advanced open water diver course Rescue diver course Emergency first response primary and secondary care courses Emergency oxygen provider course","title":"Prevention"},{"location":"12-padi-openwater-sec3-problem-management/#surface-problem-management-responsive-diver","text":"Most diver-in-distress situations occur at the surface Establish positive buoyancy on the surface inflate BCD, or drop weights Common surface problems overexertion leg muscle cramps choking on inhaled water","title":"Surface problem management - Responsive diver"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-a-responsive-diver-at-surface","text":"A diver who is breathing, alert and active is considered responsive. Two types in control / not panicked out of control / panicked Four steps of helping Establish buoyancy for yourself and the diver. If you must make contact to help, make yourself buoyant before doing so, then inflate the victim\u2019s BCD and/or drop weights after you make contact. Calm the diver by reassuring, offering encouragement and asking the person to relax. Help the diver reestablish breathing control. As necessary, assist the diver to the boat or shore. Even divers who don\u2019t panic may need assistance due to leg cramps or being overly tired.","title":"Assisting a responsive diver at surface"},{"location":"12-padi-openwater-sec3-problem-management/#surface-problem-management-unresponsive-diver","text":"A diver may be unresponsive or unconscious for one of the following reasons inhaling water extreme fatigue heart attack lung over expansion injuries panic inefficient breathing throat blockage exhaustion, among others An unresponsive diver does not move and does not respond when tapped or spoken to. Confirm responsiveness if a diver floats without moving. A diver who surfaces alone, calls for help or shows signs of panic, then stops moving, should also be considered unresponsive until you confirm otherwise.","title":"Surface problem management - Unresponsive diver"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-at-surface","text":"Four steps Establish buoyancy Call of help Check for breathing Continue rescue breathing during the tow","title":"Assisting unresponsive diver at surface"},{"location":"12-padi-openwater-sec3-problem-management/#underwater-problem-management","text":"Three ways to avoid - Relax when diving Plan air usage and monitor closely Dive withing limits of experience and training","title":"Underwater problem management"},{"location":"12-padi-openwater-sec3-problem-management/#overexertion","text":"Symptoms - feeling of air starvation exhausted Steps - Signal \u201cstop\u201d or \u201chold\u201d and indicate that you need to rest. Continue at a reduced pace after you recover. If you can\u2019t return to a relaxed state, end the dive. If conditions are adding to overexertion, it may be best to abort the dive.","title":"Overexertion"},{"location":"12-padi-openwater-sec3-problem-management/#freeflowing-regulator","text":"If regulators fail, they release air continuously (called a freeflow). Do not seal your mouth on the mouthpiece. Hold the second stage in your hand and press the mouthpiece outside your lips. Let the excess air escape freely. You may insert only one end of the mouthpiece into your mouth if it helps. Breathe the air you need by \u201csipping\u201d it,","title":"Freeflowing regulator"},{"location":"12-padi-openwater-sec3-problem-management/#entanglement","text":"Prevent by - Moving slowly Watching where you are going Keeping equipment streamlined Do not turn or twist. Extreme cases might require having to slip out of scuba kit and putting it back on.","title":"Entanglement"},{"location":"12-padi-openwater-sec3-problem-management/#running-low-out-of-air-underwater","text":"Ascend Ascending with enough air allows you to control your ascent and to make a safety stop. Four options - Make a normal ascent Do this if you\u2019re very low on air (you feel some breathing resistance), but your cylinder isn\u2019t completely empty. As you ascend, you can get more air from your cylinder, because the surrounding water pressure decreases. Breathe lightly (but continuously) and make a controlled, continuous ascent to the surface. Do not attempt a safety stop. Ascend using an alternate air source Think of this as your best, all around choice when you have an alternate air source immediately available. During your predive safety check, confirm what alternate air sources your buddies have and how to secure them. Test breathe your alternate air source(s). To use an alternate air source supplied by a buddy, you need to stay close to your buddy(ies). Controlled Emergency Swimming Ascent (CESA) This is the best choice if you were completely out of air, no deeper than approximately 6 to 9 metres/20 to 30 feet, the surface is closer than your buddy(ies) or another diver, and you have no other alternate air source. Simply look up and swim to the surface making a continuous \u201cahhhhh\u201d sound into your regulator. The \u201cahhhhh\u201d sound assures that you exhale expanding gas, which is necessary to avoid lung overexpansion injury. Leave all your gear in place and keep the regulator in your mouth. Do not drop your weights to start your ascent. Ascend at a safe rate. The ascent gets easier as you ascend because air expanding in your BCD increases your buoyancy. Vent air as needed to maintain a proper rate. Bouyant Emergency Ascent Use this option when you are too far from your buddy(ies) or another diver, have no other alternate air source and are so deep that you doubt you can reach the surface any other way. You make a buoyant emergency ascent exactly like a Controlled Emergency Swimming Ascent, except you ditch your weights and exceed a safe rate. Again, look up and make the \u201cahhhhh\u201d sound as you ascend. Because you exceed a safe rate, this method has more risk than the other options (which is why it is your last choice), but is obviously better than staying on the bottom without air. As you near the surface, you can flare out your arms and legs to create drag and slow your ascent. You can\u2019t inflate your BCD by using the low-pressure inflator, because you have an empty cylinder. Instead, inflate your BCD orally and/or drop your weights.","title":"Running low / out of air underwater"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-underwater","text":"Priority is to get the diver to surface Procedure - Swim the diver to the surface. If necessary, use the diver\u2019s BCD and/or drop weights to make the victim buoyant. If the diver\u2019s regulator is in the mouth, hold it there. If it is not, don\u2019t waste time trying to replace it. Ascend at a safe rate. If the diver\u2019s buoyancy becomes too great for a safe ascent rate, let the victim go. Finish a safe ascent and resume the rescue at the surface. Keep yourself safe. You can\u2019t help someone else if you have troubles of your own. At the surface, follow the priorities and procedures for an unresponsive diver at the surface.","title":"Assisting unresponsive diver underwater"},{"location":"12-padi-openwater-sec3-problem-management/#first-responded-care-for-diving-related-emergencies","text":"A diver who is or was unresponsive should be considered serious medical emergencies. Drowning or otherwise becoming unresponsive underwater, pressure-related injuries and medical conditions not directly related to diving (like heart attack) can cause these signs (what you observe) and symptoms (what the victim feels): difficulty breathing unconsciousness unclear thinking visual problems paralysis chest pain lowered alertness cardiac and respiratory arrest","title":"First responded care for diving-related emergencies"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-out-of-water","text":"General steps Keep the diver\u2019s airway open and check for breathing. Provide rescue breathing or CPR as necessary. Do not use abdominal thrusts unless you are unable to provide rescue breathing due to a suspected obstruction. Inhaled water, if present, does not prevent rescue breaths. You don\u2019t have to try to clear it. If the diver is unresponsive but breathing, keep the diver lying level on the left side (recovery position). This position is not more important than transporting the diver to safety or providing rescue breaths or CPR if necessary. Check the diver\u2019s breathing frequently. If the diver has regained responsiveness, keep the diver lying down comfortably. Administer emergency oxygen as soon as possible. Keep the diver still and maintain a normal body temperature by protecting from heat or cold. Continue to provide care until emergency medical care arrives. If you can\u2019t accompany the diver to medical care, write down as much background information as possible about the individual and the dive, and attach it to the diver in a conspicuous place. Provide only information relevant to care, such as the dive profile, emergency care provided, emergency contact information, and any known medical conditions. Write only facts. Do not speculate or guess \u2013 bad information is worse than no information. Some conditions, such as drowning, can have delayed serious, potentially fatal, consequences hours after the incident.","title":"Assisting unresponsive diver out of water"},{"location":"13-padi-openwater-sec3-equipment3/","text":"Introduction Signaling devices, floats and dive flags. Surface signaling devices Diver should carry two while diving audible visual Make them a standard part of the scuba kit Examples whistles - normally attached with BCD inflator low pressure horns - they use cylinder air and louder. should have the whistle if cylinder is empty inflatable signal tube signal mirrors delayed surface marker buoys signal lights and flashers Floats and flags Surface floats - carry with a reel or line caddy to avoid entanglement Flags are required by law in some places Traditional dive flag indicates divers below and boats should keep clear Alpha flag indicates that boat flying it has divers in the water and cannot maneuver The rule of thumb of distances from the flag Do not assume that all boats understand the flag","title":"13 padi openwater sec3 equipment3"},{"location":"13-padi-openwater-sec3-equipment3/#introduction","text":"Signaling devices, floats and dive flags.","title":"Introduction"},{"location":"13-padi-openwater-sec3-equipment3/#surface-signaling-devices","text":"Diver should carry two while diving audible visual Make them a standard part of the scuba kit Examples whistles - normally attached with BCD inflator low pressure horns - they use cylinder air and louder. should have the whistle if cylinder is empty inflatable signal tube signal mirrors delayed surface marker buoys signal lights and flashers","title":"Surface signaling devices"},{"location":"13-padi-openwater-sec3-equipment3/#floats-and-flags","text":"Surface floats - carry with a reel or line caddy to avoid entanglement Flags are required by law in some places Traditional dive flag indicates divers below and boats should keep clear Alpha flag indicates that boat flying it has divers in the water and cannot maneuver The rule of thumb of distances from the flag Do not assume that all boats understand the flag","title":"Floats and flags"},{"location":"14-padi-openwater-sec3-skills2/","text":"Deep water entry - seated back roll Used when close to water Steps Enter with your BCD partially inflated, holding your mask and breathing from your regulator. Tuck your chin toward your chest. Lean backward until your weight rolls you in. After you\u2019re stable on the surface, signal you\u2019re okay (assuming you are) and clear the area so your buddy(ies) can enter. Remove and replace weights at the surface Important for adjusting weight in the water Steps The technique varies with equipment \u2013 your instructor will help you. Be careful not to drop the weights. Give your weights to someone, put them on a platform or secure them to a line. Be sure you have enough buoyancy and breathe from your regulator before taking weights handed to you. It is best if buddies put weights on one at a time, so they can assist each other if necessary. With most integrated weight systems, you remove the weights as you would in an emergency weight drop. You replace them the same way you install them when setting up your kit, but with some types you may have to remove and replace the entire kit. With many systems, it helps to tilt back in the water, almost face up, so the weight pockets tend to slide into place. When removing a standard nylon web belt, be sure to hold the free end and release the buckle end so the weights can\u2019t slide off. Cramp release Painful, involuntary muscle contractions Key points If you have or start to have a cramp, signal your buddy and stop. Allow the muscle to rest. It may help to gently stretch and massage the cramped muscle. After relieving a cramp, rest for a few minutes before continuing at a slower pace. Dehydration, cold, restricted circulation and working a muscle beyond its fitness level can all cause cramping. Neutral bouyancy Use breath control to fine-tune your buoyancy. If you start to rise a bit, exhale and breathe with a slightly lower lung volume. If you start to sink, inhale and breathe with a slightly higher lung volume for a moment. If you can\u2019t hover by adjusting with breathing, make small adjustments to your BCD. Fine tuning trim Trim is important for streamline Relax in midwater to check your natural position. Usually, you want trim for a normal swimming position. You want to feel balanced and stable, both side to side and front to back. Your instructor will help you reposition gear and weight for good trim. You will learn to fine-tune your trim from one dive to the next. Ask your instructor about the PADI Peak Performance Buoyancy course to learn more about mastering buoyancy and trim. Air depletion/Alternate air source Simulates how it feels and how to respond to out-of-air-emergency Signal \"out of air\" or \"share air\" On surface, practice manual inflation of BCD because LPI is not available in an out-of-air emergency Keep using the alternate air source until bouyant Controlled emergency swimming ascent Take a breath and exhale it continuously making a loud \u201cahhhhh\u201d sound as you swim no faster than 18 metres/60 feet per minute for at least 9 metres/30 feet (a 30-second swim).","title":"14 padi openwater sec3 skills2"},{"location":"14-padi-openwater-sec3-skills2/#deep-water-entry-seated-back-roll","text":"Used when close to water Steps Enter with your BCD partially inflated, holding your mask and breathing from your regulator. Tuck your chin toward your chest. Lean backward until your weight rolls you in. After you\u2019re stable on the surface, signal you\u2019re okay (assuming you are) and clear the area so your buddy(ies) can enter.","title":"Deep water entry - seated back roll"},{"location":"14-padi-openwater-sec3-skills2/#remove-and-replace-weights-at-the-surface","text":"Important for adjusting weight in the water Steps The technique varies with equipment \u2013 your instructor will help you. Be careful not to drop the weights. Give your weights to someone, put them on a platform or secure them to a line. Be sure you have enough buoyancy and breathe from your regulator before taking weights handed to you. It is best if buddies put weights on one at a time, so they can assist each other if necessary. With most integrated weight systems, you remove the weights as you would in an emergency weight drop. You replace them the same way you install them when setting up your kit, but with some types you may have to remove and replace the entire kit. With many systems, it helps to tilt back in the water, almost face up, so the weight pockets tend to slide into place. When removing a standard nylon web belt, be sure to hold the free end and release the buckle end so the weights can\u2019t slide off.","title":"Remove and replace weights at the surface"},{"location":"14-padi-openwater-sec3-skills2/#cramp-release","text":"Painful, involuntary muscle contractions Key points If you have or start to have a cramp, signal your buddy and stop. Allow the muscle to rest. It may help to gently stretch and massage the cramped muscle. After relieving a cramp, rest for a few minutes before continuing at a slower pace. Dehydration, cold, restricted circulation and working a muscle beyond its fitness level can all cause cramping.","title":"Cramp release"},{"location":"14-padi-openwater-sec3-skills2/#neutral-bouyancy","text":"Use breath control to fine-tune your buoyancy. If you start to rise a bit, exhale and breathe with a slightly lower lung volume. If you start to sink, inhale and breathe with a slightly higher lung volume for a moment. If you can\u2019t hover by adjusting with breathing, make small adjustments to your BCD.","title":"Neutral bouyancy"},{"location":"14-padi-openwater-sec3-skills2/#fine-tuning-trim","text":"Trim is important for streamline Relax in midwater to check your natural position. Usually, you want trim for a normal swimming position. You want to feel balanced and stable, both side to side and front to back. Your instructor will help you reposition gear and weight for good trim. You will learn to fine-tune your trim from one dive to the next. Ask your instructor about the PADI Peak Performance Buoyancy course to learn more about mastering buoyancy and trim.","title":"Fine tuning trim"},{"location":"14-padi-openwater-sec3-skills2/#air-depletionalternate-air-source","text":"Simulates how it feels and how to respond to out-of-air-emergency Signal \"out of air\" or \"share air\" On surface, practice manual inflation of BCD because LPI is not available in an out-of-air emergency Keep using the alternate air source until bouyant","title":"Air depletion/Alternate air source"},{"location":"14-padi-openwater-sec3-skills2/#controlled-emergency-swimming-ascent","text":"Take a breath and exhale it continuously making a loud \u201cahhhhh\u201d sound as you swim no faster than 18 metres/60 feet per minute for at least 9 metres/30 feet (a 30-second swim).","title":"Controlled emergency swimming ascent"},{"location":"15-padi-openwater-sec4-equipment4/","text":"Introduction Description of few more equipment. Mesh utility bag Mesh helps it drain Made of nylon or any other synthetic material Hand carry the bad while diving so it can be easily dropped in case of an emergency Slates and wet books Uses communication note down dive plan record information during the dive, such as found something for revisit, how much air is used to reach a destination, etc There are special masks that allow radio communication, and dive computers with texting feature Slates are reusable but wet books are not Dive lights PADI Nightdiver course Specialized options for technical diving Can be useful even during the day Attach a clip on light to secure it with BCD when not in use. Especially for larger lights, a wrist lanyard helps avoid loss, plus allows you to release the light without losing it. Logbooks and elogs Useful for reference for future dives Recording can be useful to meet experience requirements of higher training Dive operators can provide better support if they know my experience Share online Related information such as local emergency contact, contact of dive buddies, instructor etc. can be stored PADI paper logbook is available eLog apps are available Minimum recommended information: Date Dive site (name or location) Dive buddy(ies) Dive depth and duration Objective/description Dive planning software Plans air usage, nitrogen toxicity, etc Some versions interface with dive computer and includes elog functions Mainly mobile apps but desktop apps are also available Spare parts kit Also known as save-a-dive kit Helps prepare for last minute replacements Typically contains Spare mask strap, fin strap and snorkel keeper (Tip: The VelcroTM-type mask straps make good spares, because they\u2019re easy to put in place and fit almost any mask.) Harness/weight belt buckle Cable (pull) ties Adjustable wrench (spanner), pliers, screw drivers, hex wrenches (allen keys) or scuba tool Regulator mouthpiece Accessory clip Slate/wet book pencil Various sized cylinder valve/DIN valve o-rings Cement or glue appropriate for exposure suit repair Sunscreen and spare sunglasses (not really dive parts, but can come in handy)","title":"15 padi openwater sec4 equipment4"},{"location":"15-padi-openwater-sec4-equipment4/#introduction","text":"Description of few more equipment.","title":"Introduction"},{"location":"15-padi-openwater-sec4-equipment4/#mesh-utility-bag","text":"Mesh helps it drain Made of nylon or any other synthetic material Hand carry the bad while diving so it can be easily dropped in case of an emergency","title":"Mesh utility bag"},{"location":"15-padi-openwater-sec4-equipment4/#slates-and-wet-books","text":"Uses communication note down dive plan record information during the dive, such as found something for revisit, how much air is used to reach a destination, etc There are special masks that allow radio communication, and dive computers with texting feature Slates are reusable but wet books are not","title":"Slates and wet books"},{"location":"15-padi-openwater-sec4-equipment4/#dive-lights","text":"PADI Nightdiver course Specialized options for technical diving Can be useful even during the day Attach a clip on light to secure it with BCD when not in use. Especially for larger lights, a wrist lanyard helps avoid loss, plus allows you to release the light without losing it.","title":"Dive lights"},{"location":"15-padi-openwater-sec4-equipment4/#logbooks-and-elogs","text":"Useful for reference for future dives Recording can be useful to meet experience requirements of higher training Dive operators can provide better support if they know my experience Share online Related information such as local emergency contact, contact of dive buddies, instructor etc. can be stored PADI paper logbook is available eLog apps are available Minimum recommended information: Date Dive site (name or location) Dive buddy(ies) Dive depth and duration Objective/description","title":"Logbooks and elogs"},{"location":"15-padi-openwater-sec4-equipment4/#dive-planning-software","text":"Plans air usage, nitrogen toxicity, etc Some versions interface with dive computer and includes elog functions Mainly mobile apps but desktop apps are also available","title":"Dive planning software"},{"location":"15-padi-openwater-sec4-equipment4/#spare-parts-kit","text":"Also known as save-a-dive kit Helps prepare for last minute replacements Typically contains Spare mask strap, fin strap and snorkel keeper (Tip: The VelcroTM-type mask straps make good spares, because they\u2019re easy to put in place and fit almost any mask.) Harness/weight belt buckle Cable (pull) ties Adjustable wrench (spanner), pliers, screw drivers, hex wrenches (allen keys) or scuba tool Regulator mouthpiece Accessory clip Slate/wet book pencil Various sized cylinder valve/DIN valve o-rings Cement or glue appropriate for exposure suit repair Sunscreen and spare sunglasses (not really dive parts, but can come in handy)","title":"Spare parts kit"},{"location":"16-padi-openwater-sec4-diver4/","text":"Health and fitness Maintain a general level of physical fitness - meaning adequate fitness plus physical reserve Keep immunization current - especially tetanus and typhoid Well balanced diet and rest well before diving Regular physical examination. The RSTC Medical Statement provides guidelines developed by dive medical experts that any physician can use to conduct dive physicals. Your instructor will give you this form, or you can download it from padi.com Heart health is critical to avoid heart attack during diving Avoid alcohol and tobacco before diving Use prescription drugs with caution Menstruation not a problem Avoid diving while pregnancy If you feel ill before a dive, cancel it Staying current and active diver Online forums including PADI Club, dive magazines PADI ReActivate program to review training after a break longer than 6 months Continue training The air your breathe Four issues that relate to the composition of air: Oxygen toxicity Contaminated air Decompression sickness Gas narcosis Enriched air nitrox (EANx), also called Nitrox, is any blend of O2 and N with 22%+ O2 Common blends in recreational diving are 32% and 36% O2 Tec divers can use even more % of O2 Increasing the oxygen content and decreasing the nitrogen content has advantages and disadvantages with respect to decompression sickness and oxygen toxicity problems PADI enriched air diver course, or instructor may train during the open water diver course Oxygen issues Under high pressure, oxygen is toxic If a gas has oxygen in it, oxygen toxicity can result from breathing it deeper than a specific depth The higher the oxygen content, the shallower the limit for using it while diving High oxygen percentages can also create some fire/combustion risks with respect to the equipment with which it must be used To avoid oxygen toxicity when diving with air, don\u2019t exceed the maximum depth for recreational diving (40 metres/130 feet). Fire/combustion problems aren\u2019t issues when using air with standard scuba equipment. Recreational divers don\u2019t use 100 percent oxygen, but tec divers often do (shallower than 6 metres/20 feet, as part of their ascent procedures) Rebreather divers use them and trained in the PADI Rebreather diver course Contaminated air Compressors for filling cylinders use valves to keep away impurities Trace amount of CO or Oil vapor is harmless on ground but can be toxic when breathed under pressure Possible causes Getting cylinder filled at improper source Improper maintenance of filling system More contaminant in the source than the filters can keep out Symptoms that the air could be contaminated Headache Nausea Dizziness Unconsciousness/unresponsiveness Cherry-red lips/fongernail beds (though this is difficult to see underwater) Actions to take give fresh air emergency oxygen CPR emergency medical care provider Decompression Sickness The two primary factors affecting Nitrogen absorption are dive depth and time Nitrogen dissolves in body tissue due to the pressure (depth) As you ascend, the nitrogen is released by body tissue, carried by blood to lungs for exhaling If the amount of excess nitrogen is within accepted limits, your body normally gets rid of it harmlessly over the next several hour Use dive compuers or dive tables like RDP or eRDP-ML to stay withi acceptable nitrogen limits If the excess nitrogen in your body tissues is too high, when you ascend and surface, the nitrogen may come out of solution faster than your body can eliminate it. This can cause nitrogen bubbles to form within your blood and body tissues, much like bubbles form when you open a soda bottle and release the pressure Bubbles forming in the body cause a very serious medical condition called decompression sickness , also called DCS or \"the bends\" The signs and symptoms of DCS depend upon where bubbles form in the body. They include: Paralysis Dizziness Tingling Joint and limb pain Shock Numbness Difficulty breathing Weakness and prolonged fatigue In severe cases, unconsciousness and death DCS signs and symptoms may be clear and obvious, but they may also be subtle, like a mild to moderate, dull ache (often, but not necessarily, in the joints), mild to moderate tingling or numbness, weakness and prolonged fatigue They usually occur 15 minutes to 12 hours after a dive, though they can occur before surfacing, and they can occur after 12 hours Physiologists think that when present, the following secondary factors can contribute to developing DCS: Fatigue Dehydration Cold Poor fitness/high body fat Illness Injuries Age Alcohol consumption before or after a dive Vigorous exercise before, during or immediately after the dive","title":"16 padi openwater sec4 diver4"},{"location":"16-padi-openwater-sec4-diver4/#health-and-fitness","text":"Maintain a general level of physical fitness - meaning adequate fitness plus physical reserve Keep immunization current - especially tetanus and typhoid Well balanced diet and rest well before diving Regular physical examination. The RSTC Medical Statement provides guidelines developed by dive medical experts that any physician can use to conduct dive physicals. Your instructor will give you this form, or you can download it from padi.com Heart health is critical to avoid heart attack during diving Avoid alcohol and tobacco before diving Use prescription drugs with caution Menstruation not a problem Avoid diving while pregnancy If you feel ill before a dive, cancel it","title":"Health and fitness"},{"location":"16-padi-openwater-sec4-diver4/#staying-current-and-active-diver","text":"Online forums including PADI Club, dive magazines PADI ReActivate program to review training after a break longer than 6 months Continue training","title":"Staying current and active diver"},{"location":"16-padi-openwater-sec4-diver4/#the-air-your-breathe","text":"Four issues that relate to the composition of air: Oxygen toxicity Contaminated air Decompression sickness Gas narcosis Enriched air nitrox (EANx), also called Nitrox, is any blend of O2 and N with 22%+ O2 Common blends in recreational diving are 32% and 36% O2 Tec divers can use even more % of O2 Increasing the oxygen content and decreasing the nitrogen content has advantages and disadvantages with respect to decompression sickness and oxygen toxicity problems PADI enriched air diver course, or instructor may train during the open water diver course","title":"The air your breathe"},{"location":"16-padi-openwater-sec4-diver4/#oxygen-issues","text":"Under high pressure, oxygen is toxic If a gas has oxygen in it, oxygen toxicity can result from breathing it deeper than a specific depth The higher the oxygen content, the shallower the limit for using it while diving High oxygen percentages can also create some fire/combustion risks with respect to the equipment with which it must be used To avoid oxygen toxicity when diving with air, don\u2019t exceed the maximum depth for recreational diving (40 metres/130 feet). Fire/combustion problems aren\u2019t issues when using air with standard scuba equipment. Recreational divers don\u2019t use 100 percent oxygen, but tec divers often do (shallower than 6 metres/20 feet, as part of their ascent procedures) Rebreather divers use them and trained in the PADI Rebreather diver course","title":"Oxygen issues"},{"location":"16-padi-openwater-sec4-diver4/#contaminated-air","text":"Compressors for filling cylinders use valves to keep away impurities Trace amount of CO or Oil vapor is harmless on ground but can be toxic when breathed under pressure Possible causes Getting cylinder filled at improper source Improper maintenance of filling system More contaminant in the source than the filters can keep out Symptoms that the air could be contaminated Headache Nausea Dizziness Unconsciousness/unresponsiveness Cherry-red lips/fongernail beds (though this is difficult to see underwater) Actions to take give fresh air emergency oxygen CPR emergency medical care provider","title":"Contaminated air"},{"location":"16-padi-openwater-sec4-diver4/#decompression-sickness","text":"The two primary factors affecting Nitrogen absorption are dive depth and time Nitrogen dissolves in body tissue due to the pressure (depth) As you ascend, the nitrogen is released by body tissue, carried by blood to lungs for exhaling If the amount of excess nitrogen is within accepted limits, your body normally gets rid of it harmlessly over the next several hour Use dive compuers or dive tables like RDP or eRDP-ML to stay withi acceptable nitrogen limits If the excess nitrogen in your body tissues is too high, when you ascend and surface, the nitrogen may come out of solution faster than your body can eliminate it. This can cause nitrogen bubbles to form within your blood and body tissues, much like bubbles form when you open a soda bottle and release the pressure Bubbles forming in the body cause a very serious medical condition called decompression sickness , also called DCS or \"the bends\" The signs and symptoms of DCS depend upon where bubbles form in the body. They include: Paralysis Dizziness Tingling Joint and limb pain Shock Numbness Difficulty breathing Weakness and prolonged fatigue In severe cases, unconsciousness and death DCS signs and symptoms may be clear and obvious, but they may also be subtle, like a mild to moderate, dull ache (often, but not necessarily, in the joints), mild to moderate tingling or numbness, weakness and prolonged fatigue They usually occur 15 minutes to 12 hours after a dive, though they can occur before surfacing, and they can occur after 12 hours Physiologists think that when present, the following secondary factors can contribute to developing DCS: Fatigue Dehydration Cold Poor fitness/high body fat Illness Injuries Age Alcohol consumption before or after a dive Vigorous exercise before, during or immediately after the dive","title":"Decompression Sickness"},{"location":"17-padi-openwater-sec4-divecomputers1/","text":"Introduction Mathematical models are used to estimate Nitrogen concentration in diver's body to manage the risk of DCS. Available on dive computers or dive tables like the Recreational Dive Planner (table or eRDPML electronic table versions) How dive computers and tables work Dive computers and dive tables work by using your dive time and depth information to calculate the theoretical amount of nitrogen in your body Dive computers measure depth and time throughout a dive (and after) and apply the information to the decompression model electronically. A computer constantly updates the theoretical nitrogen in your body based on your dive depths and time, and compares it to the model Dive computers have become the most common method of calculating decompression information Depth from a depth gauge and the time from a timer used to look up limit information on the RDP Table The eRDPML is a calculator-format electronic dive table. You enter your depth/time information, and it looks it up on the table for you Decompression models are highly reliable, but they cannot account for individual variations in physiology, such as the secondary factors you learned in the last subsection Do not ascend faster than 18m / 60ft per min Always stay withing the limits of dive computer Make a safety stop in 5m /15ft No stop diving No stop diving is swimming directly to surface without unacceptable risk of decompression sickness No stop limit or No decompression limit (NDL) is the maximum time diver can spend at a depth and still ascend directly to surface If no stop limit is exceeded, one or more decompression stops must be taken Decompression stops are at specific depth for prescribed times In recreational diving, decompression stops are emergency procedures No stop times for two different computers can be different Computer constantly updates your remaining no stop time based on your dive profile \u2013 your actual depths, and your times at each depth \u2013 and the limits set by the decompression model Multilevel diving is possible with computers because it updates no stop time as you ascend Using a table, divers must abide by the no stop limit of the deepest depth Repetitive diving No stop limits are less after the first dive because of nitrogen from the first dive The nitrogen left in your body after a dive is called residual nitrogen A dive made while you still have residual nitrogen is called a repetitive dive Body nitrogen level return to normal after 12 hours and the next dive is considered first dive or clean dive A surface interval is the time you spend at the surface between two dives Because your dive computer tracks your personal theoretical nitrogen levels continuously during all your dives and all your surface intervals, you must use the same computer the entire diving day, on all dives, and not share it with another diver RDP addresses repetitive dive using three tables The first table assigns a Pressure Group (as a letter) that represents the theoretical amount of residual nitrogen from your dive time and depth This Pressure Group represents having less theoretical residual nitrogen in the body. The third table shows you no stop times for each depth adjusted for your Pressure Group at the start of the dive. Planning dives with computer Four advantages of dive computers Easier to use than tables Help offset human errors Gives more time underwater Have other useful features, such as, Measure water temperature Download to dive log Dive computer information No stop (no decompression) limits - for planning Depth Elapsed time No stop time remaining Ascent rate Emergency decompression Previous dive information Air supply commonly limits your dive \u2013 not your no stop time Diving with computer Six guidelines to follow when diving with computers Dive the plan Stay within computers limit Follow the most conservative computer Watch SPG - There are air-integrated computers Start with the deepest dive Ascend slowly In case of failure Signal buddies and ascend, make a safety stop. Wait 12 hours or more before diving with another computer, or use a table If you use two computers, dive using the backup computer Underwater world's embassador Poor dive techniques, and neglect, can damage fragile aquatic life To learn more about how you can help in preserving the underwater world, see your PADI operator and visit projectaware.org","title":"17 padi openwater sec4 divecomputers1"},{"location":"17-padi-openwater-sec4-divecomputers1/#introduction","text":"Mathematical models are used to estimate Nitrogen concentration in diver's body to manage the risk of DCS. Available on dive computers or dive tables like the Recreational Dive Planner (table or eRDPML electronic table versions)","title":"Introduction"},{"location":"17-padi-openwater-sec4-divecomputers1/#how-dive-computers-and-tables-work","text":"Dive computers and dive tables work by using your dive time and depth information to calculate the theoretical amount of nitrogen in your body Dive computers measure depth and time throughout a dive (and after) and apply the information to the decompression model electronically. A computer constantly updates the theoretical nitrogen in your body based on your dive depths and time, and compares it to the model Dive computers have become the most common method of calculating decompression information Depth from a depth gauge and the time from a timer used to look up limit information on the RDP Table The eRDPML is a calculator-format electronic dive table. You enter your depth/time information, and it looks it up on the table for you Decompression models are highly reliable, but they cannot account for individual variations in physiology, such as the secondary factors you learned in the last subsection Do not ascend faster than 18m / 60ft per min Always stay withing the limits of dive computer Make a safety stop in 5m /15ft","title":"How dive computers and tables work"},{"location":"17-padi-openwater-sec4-divecomputers1/#no-stop-diving","text":"No stop diving is swimming directly to surface without unacceptable risk of decompression sickness No stop limit or No decompression limit (NDL) is the maximum time diver can spend at a depth and still ascend directly to surface If no stop limit is exceeded, one or more decompression stops must be taken Decompression stops are at specific depth for prescribed times In recreational diving, decompression stops are emergency procedures No stop times for two different computers can be different Computer constantly updates your remaining no stop time based on your dive profile \u2013 your actual depths, and your times at each depth \u2013 and the limits set by the decompression model Multilevel diving is possible with computers because it updates no stop time as you ascend Using a table, divers must abide by the no stop limit of the deepest depth","title":"No stop diving"},{"location":"17-padi-openwater-sec4-divecomputers1/#repetitive-diving","text":"No stop limits are less after the first dive because of nitrogen from the first dive The nitrogen left in your body after a dive is called residual nitrogen A dive made while you still have residual nitrogen is called a repetitive dive Body nitrogen level return to normal after 12 hours and the next dive is considered first dive or clean dive A surface interval is the time you spend at the surface between two dives Because your dive computer tracks your personal theoretical nitrogen levels continuously during all your dives and all your surface intervals, you must use the same computer the entire diving day, on all dives, and not share it with another diver RDP addresses repetitive dive using three tables The first table assigns a Pressure Group (as a letter) that represents the theoretical amount of residual nitrogen from your dive time and depth This Pressure Group represents having less theoretical residual nitrogen in the body. The third table shows you no stop times for each depth adjusted for your Pressure Group at the start of the dive.","title":"Repetitive diving"},{"location":"17-padi-openwater-sec4-divecomputers1/#planning-dives-with-computer","text":"Four advantages of dive computers Easier to use than tables Help offset human errors Gives more time underwater Have other useful features, such as, Measure water temperature Download to dive log Dive computer information No stop (no decompression) limits - for planning Depth Elapsed time No stop time remaining Ascent rate Emergency decompression Previous dive information Air supply commonly limits your dive \u2013 not your no stop time","title":"Planning dives with computer"},{"location":"17-padi-openwater-sec4-divecomputers1/#diving-with-computer","text":"Six guidelines to follow when diving with computers Dive the plan Stay within computers limit Follow the most conservative computer Watch SPG - There are air-integrated computers Start with the deepest dive Ascend slowly In case of failure Signal buddies and ascend, make a safety stop. Wait 12 hours or more before diving with another computer, or use a table If you use two computers, dive using the backup computer","title":"Diving with computer"},{"location":"17-padi-openwater-sec4-divecomputers1/#underwater-worlds-embassador","text":"Poor dive techniques, and neglect, can damage fragile aquatic life To learn more about how you can help in preserving the underwater world, see your PADI operator and visit projectaware.org","title":"Underwater world's embassador"},{"location":"18-padi-openwater-sec4-skills4/","text":"Introduction Another set of skills to learn as part of the course. Deep Water Entry Put on Scuba Kit at the Surface, Controlled Seated Entry Useful when diving from small boats without enough space to kit up Useful when entering from heights too great for giant stride Completely assemble your gear, check your SPG for enough air and be sure the cylinder valve is open Although you\u2019re not wearing your kit, you go through the steps of your predive safety check If your BCD has integrated weights but will float adequately, then weights may be in place. For a separate weight system or if your gear won\u2019t float, you will put the weights on in the water after putting on your scuba kit. Don\u2019t enter the water without your BCD while wearing your weight system After you and your buddy(ies) are in your gear, finish the F \u2013 Final Check - step of your predive safety check, making sure that all buckles are secure, no hoses are trapped, nothing dangles and everything is where it should be In many instances, the controlled seated entry is suitable for getting into the water while wearing scuba gear. Partially inflate your BCD and breathe from your regulator before entering. As you turn to ease into the water, be sure your cylinder has cleared the platform edge. Helping a Tired Buddy Establish bouyancy for yourself and the diver who needs help Use the cylinder valve tow, with you and the diver floating face up, for short distances Use the tired diver push (modified tired-swimmer carry) for longer tows Swim at a slow, steady pace to the exit Neutral Buoyancy \u2013 Visual Reference Descents, Swimming and Ascents Near Sensitive Environments Combines neutral bouyancy skill with descending Control descent by controlling bouyancy Descent to 1-1.5m above the floor Learn to touch anything only intentionally No Mask Swim You may close your eyes if you have contacts, though you would keep them open if you really had to ascend without your mask Otherwise, keep your eyes open because you can see enough to be useful Swim with your buddy at least 15 metres/50 feet. Control your buoyancy, equalize your ears, etc. as you normally would. Your buddy will guide you (especially if you have your eyes closed) Concentrate on breathing through your mouth. Exhale through your nose if the water tickles it a bit After the swim, replace your mask and clear it Freeflow Regulator Breathing Depress the purge button to simulate free flow Hold the mouthpiece against your lips \u2013 don\u2019t seal your mouth on it Allow excess air to escape; \u201csip\u201d the air you need from the flow It may help to insert just one side of the mouthpiece into your mouth, with the other side out Practice for at least 30 seconds Check your SPG after practicing. You may be surprised how much air you used. This emphasizes the need to start your ascent immediately when breathing from a freeflowing regulator BCD Oral Inflation Underwater This is similar to orally inflating at the surface, except that you take each breath from your regulator and there is no need to kick up as you inhale Blow a third to half your breath into the BCD by pressing the deflator (exhaust) button while you blow in (and only then). You won\u2019t put in as much air per breath as at the surface, because you\u2019re only adjusting to become neutrally buoyant Let go of the deflator button while you\u2019re not blowing in, so the BCD does not deflate Remember to blow a steady stream of bubbles when the regulator isn\u2019t in your mouth Replace the regulator, clear it and resume breathing after blowing into your BCD. Wait a moment to see how your buoyancy has changed before making further adjustments Do not continue a dive with a malfunctioning low pressure inflator. Use oral inflation/normal deflation to maintain buoyancy control as you abort the dive Once neutrally buoyant, hover for a least a minute using breath control and small adjustments Skin Diving Skills Skin diving skills or free diving skills is diving with mask, fins and snorkel, but no scuba Breathe from your diaphragm before making a breath-hold dive. This is sometimes called \u201cstomach breathing,\u201d because your diaphragm is the muscle over your stomach. To do this, breathe so your stomach area expands. Hyperventilation (breathing deeper and/or more rapidly than normal) is no longer preferred as a breath-hold technique because it can lower carbon dioxide levels so low that your body can run out of oxygen before you get the urge to breathe. If done improperly, it can cause you to lose consciousness and drown Although some divers do this by limiting hyperventilation to only 2 or 3 deep breaths, it is better to breath-hold using relaxed diaphragm breathing, which allows you to hold your breath just as long (some evidence suggests longer) After returning to the surface, rest until your body restores normal oxygen and carbon dioxide levels Breathe normally from your diaphragm \u2013 a strong exhalation after surfacing from a long breath hold can cause you to become faint If you feel dizzy or lightheaded, or feel tingling in your hands, arms or feet, stop diving down. Rest, relax and breathe at the surface Buddy Contact When skin diving, your buddy remains on the surface when you dive down, and vice versa Thee one-up, one-down technique allows your buddy to come to your aid if you need help Safety If skin diving deeper than 10 metres/30 feet and/or with breath-hold times longer than a minute interests you, see your instructor about specialized training Exit \u2013 Remove Scuba Kit in Water Begin by inflating your BCD to establish positive buoyancy Remove and hand up your weights, or attach them to a line. This may not be necessary if your kit will float with integrated weights still in it To exit the water without a ladder, push up and kick hard to lift your torso, then turn and sit on the swim step or boat side (as appropriate). Alternatively, lift yourself up, bend forward at the waist, and lower yourself so you\u2019re face down on the deck/boat, then roll face up Sometimes you need to secure your kit to a gear line before exiting the water. This is especially true when there\u2019s a current","title":"18 padi openwater sec4 skills4"},{"location":"18-padi-openwater-sec4-skills4/#introduction","text":"Another set of skills to learn as part of the course.","title":"Introduction"},{"location":"18-padi-openwater-sec4-skills4/#deep-water-entry","text":"Put on Scuba Kit at the Surface, Controlled Seated Entry Useful when diving from small boats without enough space to kit up Useful when entering from heights too great for giant stride Completely assemble your gear, check your SPG for enough air and be sure the cylinder valve is open Although you\u2019re not wearing your kit, you go through the steps of your predive safety check If your BCD has integrated weights but will float adequately, then weights may be in place. For a separate weight system or if your gear won\u2019t float, you will put the weights on in the water after putting on your scuba kit. Don\u2019t enter the water without your BCD while wearing your weight system After you and your buddy(ies) are in your gear, finish the F \u2013 Final Check - step of your predive safety check, making sure that all buckles are secure, no hoses are trapped, nothing dangles and everything is where it should be In many instances, the controlled seated entry is suitable for getting into the water while wearing scuba gear. Partially inflate your BCD and breathe from your regulator before entering. As you turn to ease into the water, be sure your cylinder has cleared the platform edge.","title":"Deep Water Entry"},{"location":"18-padi-openwater-sec4-skills4/#helping-a-tired-buddy","text":"Establish bouyancy for yourself and the diver who needs help Use the cylinder valve tow, with you and the diver floating face up, for short distances Use the tired diver push (modified tired-swimmer carry) for longer tows Swim at a slow, steady pace to the exit","title":"Helping a Tired Buddy"},{"location":"18-padi-openwater-sec4-skills4/#neutral-buoyancy-visual-reference-descents-swimming-and-ascents-near-sensitive-environments","text":"Combines neutral bouyancy skill with descending Control descent by controlling bouyancy Descent to 1-1.5m above the floor Learn to touch anything only intentionally","title":"Neutral Buoyancy \u2013 Visual Reference Descents, Swimming and Ascents Near Sensitive Environments"},{"location":"18-padi-openwater-sec4-skills4/#no-mask-swim","text":"You may close your eyes if you have contacts, though you would keep them open if you really had to ascend without your mask Otherwise, keep your eyes open because you can see enough to be useful Swim with your buddy at least 15 metres/50 feet. Control your buoyancy, equalize your ears, etc. as you normally would. Your buddy will guide you (especially if you have your eyes closed) Concentrate on breathing through your mouth. Exhale through your nose if the water tickles it a bit After the swim, replace your mask and clear it","title":"No Mask Swim"},{"location":"18-padi-openwater-sec4-skills4/#freeflow-regulator-breathing","text":"Depress the purge button to simulate free flow Hold the mouthpiece against your lips \u2013 don\u2019t seal your mouth on it Allow excess air to escape; \u201csip\u201d the air you need from the flow It may help to insert just one side of the mouthpiece into your mouth, with the other side out Practice for at least 30 seconds Check your SPG after practicing. You may be surprised how much air you used. This emphasizes the need to start your ascent immediately when breathing from a freeflowing regulator","title":"Freeflow Regulator Breathing"},{"location":"18-padi-openwater-sec4-skills4/#bcd-oral-inflation-underwater","text":"This is similar to orally inflating at the surface, except that you take each breath from your regulator and there is no need to kick up as you inhale Blow a third to half your breath into the BCD by pressing the deflator (exhaust) button while you blow in (and only then). You won\u2019t put in as much air per breath as at the surface, because you\u2019re only adjusting to become neutrally buoyant Let go of the deflator button while you\u2019re not blowing in, so the BCD does not deflate Remember to blow a steady stream of bubbles when the regulator isn\u2019t in your mouth Replace the regulator, clear it and resume breathing after blowing into your BCD. Wait a moment to see how your buoyancy has changed before making further adjustments Do not continue a dive with a malfunctioning low pressure inflator. Use oral inflation/normal deflation to maintain buoyancy control as you abort the dive Once neutrally buoyant, hover for a least a minute using breath control and small adjustments","title":"BCD Oral Inflation Underwater"},{"location":"18-padi-openwater-sec4-skills4/#skin-diving-skills","text":"Skin diving skills or free diving skills is diving with mask, fins and snorkel, but no scuba Breathe from your diaphragm before making a breath-hold dive. This is sometimes called \u201cstomach breathing,\u201d because your diaphragm is the muscle over your stomach. To do this, breathe so your stomach area expands. Hyperventilation (breathing deeper and/or more rapidly than normal) is no longer preferred as a breath-hold technique because it can lower carbon dioxide levels so low that your body can run out of oxygen before you get the urge to breathe. If done improperly, it can cause you to lose consciousness and drown Although some divers do this by limiting hyperventilation to only 2 or 3 deep breaths, it is better to breath-hold using relaxed diaphragm breathing, which allows you to hold your breath just as long (some evidence suggests longer) After returning to the surface, rest until your body restores normal oxygen and carbon dioxide levels Breathe normally from your diaphragm \u2013 a strong exhalation after surfacing from a long breath hold can cause you to become faint If you feel dizzy or lightheaded, or feel tingling in your hands, arms or feet, stop diving down. Rest, relax and breathe at the surface","title":"Skin Diving Skills"},{"location":"18-padi-openwater-sec4-skills4/#buddy-contact","text":"When skin diving, your buddy remains on the surface when you dive down, and vice versa Thee one-up, one-down technique allows your buddy to come to your aid if you need help","title":"Buddy Contact"},{"location":"18-padi-openwater-sec4-skills4/#safety","text":"If skin diving deeper than 10 metres/30 feet and/or with breath-hold times longer than a minute interests you, see your instructor about specialized training","title":"Safety"},{"location":"18-padi-openwater-sec4-skills4/#exit-remove-scuba-kit-in-water","text":"Begin by inflating your BCD to establish positive buoyancy Remove and hand up your weights, or attach them to a line. This may not be necessary if your kit will float with integrated weights still in it To exit the water without a ladder, push up and kick hard to lift your torso, then turn and sit on the swim step or boat side (as appropriate). Alternatively, lift yourself up, bend forward at the waist, and lower yourself so you\u2019re face down on the deck/boat, then roll face up Sometimes you need to secure your kit to a gear line before exiting the water. This is especially true when there\u2019s a current","title":"Exit \u2013 Remove Scuba Kit in Water"},{"location":"19-padi-openwater-sec5-divecomputers2/","text":"Planning a Minimum Surface Interval Planning a repetitive dive for a specific depth and time requires finding a minimum surface interval This is determining how long to wait after the first dive to have the no stop time you want at the planned depth of the repetitive dive 3 ways to find a minimum surface interval with a computer: wait and check, use the dive computer plan mode and use a tablet/smartphone Flying After Diving and Altitude Diving As of Sep 2020, the dive medical community\u2019s recommendations for flying after diving are: For no stop dives Single dives (no repetitive dive) \u2013 A minimum preflight surface interval of 12 hours is suggested. Repetitive dives or multiday dives (diving every day for several days in a row) \u2013 A minimum preflight surface interval of 18 hours is suggested. Dives requiring emergency decompression stops: A minimum preflight surface interval greater than 18 hours is suggested. These recommendations are based on a cabin altitude pressure range of 600-2400 metres/2000-8000 fee Need to use altitude diving procedures if diving at an altitude of 300 metres/1000 feet or higher Dive computer might have settings to adjust for high altitude diving Cold and/or Strenuous Dives Can dissolve more than usual levels of nitrogen in body tissue Plan cold/strenuous dives as though they are 4 metres/10 feet deeper than their actual depth Safety stops are recommended at the end of all dives, but they\u2019re especially wise after a cold and/or strenuous dive Emergency Decompression Stops Safety stops keep you well within limits, whereas emergency decompression stops return you from outside limits In recreational diving, required emergency decompression stops are emergency situations only. They mean you either failed to monitor your dive computer (or timer and depth gauge), or something forced you to overstay your time at depth If you don\u2019t have enough air to complete an emergency decompression stop, stop as long as you can, but save enough air to surface and exit safely If you didn\u2019t complete the entire emergency decompression stop (or accidentally skipped it altogether), after ascending, relax, breathe 100 percent emergency oxygen if available and monitor yourself for decompression sickness symptoms. Don\u2019t dive again for at least 24 hours First Aid and Treatment for Decompression Illness The term decompression illness (DCI) is used to describe both lung overexpansion injuries and decompression sickness Steps to help in DCI: The diver should stop all diving Check for breathing. Provide CPR as needed Contact emergency medical care. Some areas have diver emergency services for consultation and to coordinate with local medical services Keep the diver lying down and provide emergency oxygen Monitor the diver and take steps to prevent shock If the diver is unresponsive but breathing normally, lay the diver level, left side down, head supported, breathing oxygen Continue this care until emergency medical personnel arrive Almost all cases of decompression illness require treatment in a recompression chamber","title":"19 padi openwater sec5 divecomputers2"},{"location":"19-padi-openwater-sec5-divecomputers2/#planning-a-minimum-surface-interval","text":"Planning a repetitive dive for a specific depth and time requires finding a minimum surface interval This is determining how long to wait after the first dive to have the no stop time you want at the planned depth of the repetitive dive 3 ways to find a minimum surface interval with a computer: wait and check, use the dive computer plan mode and use a tablet/smartphone","title":"Planning a Minimum Surface Interval"},{"location":"19-padi-openwater-sec5-divecomputers2/#flying-after-diving-and-altitude-diving","text":"As of Sep 2020, the dive medical community\u2019s recommendations for flying after diving are: For no stop dives Single dives (no repetitive dive) \u2013 A minimum preflight surface interval of 12 hours is suggested. Repetitive dives or multiday dives (diving every day for several days in a row) \u2013 A minimum preflight surface interval of 18 hours is suggested. Dives requiring emergency decompression stops: A minimum preflight surface interval greater than 18 hours is suggested. These recommendations are based on a cabin altitude pressure range of 600-2400 metres/2000-8000 fee Need to use altitude diving procedures if diving at an altitude of 300 metres/1000 feet or higher Dive computer might have settings to adjust for high altitude diving","title":"Flying After Diving and Altitude Diving"},{"location":"19-padi-openwater-sec5-divecomputers2/#cold-andor-strenuous-dives","text":"Can dissolve more than usual levels of nitrogen in body tissue Plan cold/strenuous dives as though they are 4 metres/10 feet deeper than their actual depth Safety stops are recommended at the end of all dives, but they\u2019re especially wise after a cold and/or strenuous dive","title":"Cold and/or Strenuous Dives"},{"location":"19-padi-openwater-sec5-divecomputers2/#emergency-decompression-stops","text":"Safety stops keep you well within limits, whereas emergency decompression stops return you from outside limits In recreational diving, required emergency decompression stops are emergency situations only. They mean you either failed to monitor your dive computer (or timer and depth gauge), or something forced you to overstay your time at depth If you don\u2019t have enough air to complete an emergency decompression stop, stop as long as you can, but save enough air to surface and exit safely If you didn\u2019t complete the entire emergency decompression stop (or accidentally skipped it altogether), after ascending, relax, breathe 100 percent emergency oxygen if available and monitor yourself for decompression sickness symptoms. Don\u2019t dive again for at least 24 hours","title":"Emergency Decompression Stops"},{"location":"19-padi-openwater-sec5-divecomputers2/#first-aid-and-treatment-for-decompression-illness","text":"The term decompression illness (DCI) is used to describe both lung overexpansion injuries and decompression sickness Steps to help in DCI: The diver should stop all diving Check for breathing. Provide CPR as needed Contact emergency medical care. Some areas have diver emergency services for consultation and to coordinate with local medical services Keep the diver lying down and provide emergency oxygen Monitor the diver and take steps to prevent shock If the diver is unresponsive but breathing normally, lay the diver level, left side down, head supported, breathing oxygen Continue this care until emergency medical personnel arrive Almost all cases of decompression illness require treatment in a recompression chamber","title":"First Aid and Treatment for Decompression Illness"},{"location":"20-padi-openwater-sec5-diver5/","text":"Gas Narcosis Many gases, including oxygen and nitrogen, cause an intoxicating effect under pressure. This is called gas narcosis. Signs and symptoms may include Feeling intoxicated (drunk or \u201chigh\u201d) Loss of coordination Slowed thinking Slowed reactions Inappropriate laughter Depression False sense of security Ignoring or disregard for safety Anxiety and/or panic (when under stress at depth) Gas narcosis is not thought to be harmful itself. The hazard is that it impairs the good judgment, clear thinking and timely responses you need to avoid and manage problems underwater Some gases \u2013 such as helium \u2013 are not narcotic, and tec divers breathe gas mixes with helium, which helps manage narcosis Gas narcosis is thought to be caused by increased dissolved gases in body tissues slowing nerve impulses that travel in the brain and nervous system Most divers usually begin to notice gas narcosis at a depth of approximately 30m / 100ft Navigating underwater Hold your compass so it\u2019s relatively level (so the north needle can rotate) and so the lubber line is aligned with the centerline of your body Look over the compass, not down on it. This allows you to watch where you\u2019re going while continuing to read it PADI Underwater Navigator Course","title":"20 padi openwater sec5 diver5"},{"location":"20-padi-openwater-sec5-diver5/#gas-narcosis","text":"Many gases, including oxygen and nitrogen, cause an intoxicating effect under pressure. This is called gas narcosis. Signs and symptoms may include Feeling intoxicated (drunk or \u201chigh\u201d) Loss of coordination Slowed thinking Slowed reactions Inappropriate laughter Depression False sense of security Ignoring or disregard for safety Anxiety and/or panic (when under stress at depth) Gas narcosis is not thought to be harmful itself. The hazard is that it impairs the good judgment, clear thinking and timely responses you need to avoid and manage problems underwater Some gases \u2013 such as helium \u2013 are not narcotic, and tec divers breathe gas mixes with helium, which helps manage narcosis Gas narcosis is thought to be caused by increased dissolved gases in body tissues slowing nerve impulses that travel in the brain and nervous system Most divers usually begin to notice gas narcosis at a depth of approximately 30m / 100ft","title":"Gas Narcosis"},{"location":"20-padi-openwater-sec5-diver5/#navigating-underwater","text":"Hold your compass so it\u2019s relatively level (so the north needle can rotate) and so the lubber line is aligned with the centerline of your body Look over the compass, not down on it. This allows you to watch where you\u2019re going while continuing to read it PADI Underwater Navigator Course","title":"Navigating underwater"},{"location":"21-padi-sec5-skills5/","text":"Remove and Replace the Scuba Kit Underwater This is one of the few skills in which it may be important to be negatively buoyant, s you could kneel on the bottom if necessary In open water, if possible, do this on insensitive bottom away from fragile aquati life. If dealing with entanglement or another safety situation, however, you probably hav no choice about where you do it With a weight-integrated BCD with a small or moderate amount of weight, or using a BCD without integrated weights, completely deflate your BCD and kneel on the bottom with your left knee down and your right knee up. If you have a lot of weight (while wearing a dry suit, for example), you may want enough air in the BCD so that it is not too negative to handle once you remove it Remove the BCD starting with your left arm, so the regulator stays in your mouth Keep the kit upright and do not let go of it. Stand it on your right knee. You may keep your right arm partially through the harness for control To keep stable on the bottom with weight-integrated BCDs, keep the unit close and positioned so that you don\u2019t float away from it. Keeping it on your knee is one effective way to do this Start with the right arm when you put it back on, so the regulator stays in your mouth Before fastening the straps, check to be sure you won\u2019t trap any hoses or accessories Remove and Replace Weight System Underwater The main reasons for removing and replacing your weight system underwater are to make trim adjustments, to remove weight you can retrieve later, to replace a dislodged weight pouch or to untwist a weight belt This is the other skill in which it often helps to be negatively buoyant (again, on insensitive bottom), though it may not be necessary, depending upon your weight system and the amount of weight you\u2019re wearing Deflate your BCD and kneel Remove the weight pocket and rest it on your knee. If using a weight belt, release it and pull it from behind your back with the buckle (left side) free, and drape it over your knee Resting the weight on your knee helps you keep your balance Your instructor will show you the technique(s) suited to your kit Descents and Ascents Without Reference Equalize regularly as you descend Control your buoyancy. Slow descents and ascents are easier to control When ascending, watch your computer and be sure you ascend within its ascent rate When you ascend, make a safety stop by hovering in midwater at 5 metres/15 feet Practice this about half way to the surface in confined water If you need to navigate while descending and/or ascending, one buddy navigates while the other controls the depth and the descent/ascent rate Minidive Dive planning, including SPG pressures and time for ending the dives Predive safety checks Use the PADI Skill Practice and Dive Planning Slate for dive planning and the predive safety check Entry Five point descents Neutral buoyancy practice Practicing skills with your buddy(ies) Air supply awareness and management Ending the dives based on SPG pressure or time Five point ascents Exits from water Confined Water Dive Five Briefing Equipment assembly, dive planning, gearing up and predive safety check Entry Weight and trim check and adjustment Five point descent without reference Remove and replace scuba kit underwater Remove and replace weight system underwater Minidive Dive planning, entry Weight check and neutral buoyancy practice Avoiding simulated sensitive zone or objects Simulated emergencies, skill practice, games/objectives Five point ascent without reference Exit Debrief Open Water Dive Three Briefing and dive planning Assemble, put on and adjust equipment Predive safety check Entry Weight and trim check, adjustment Controlled five point descent with visual reference Buoyancy control \u2013 establish neutral buoyancy with oral BCD inflation and hover Remove, replace and clear mask Underwater exploration Exit Debrief and log dive Post dive equipment care Open Water Dive Four Briefing and dive planning Assemble, put on and adjust equipment Predive safety check Entry Weight and trim check, adjustment Five point free descent without reference Underwater exploration Ascent with safety stop Exit Debrief and log dive Post dive equipment care","title":"21 padi sec5 skills5"},{"location":"21-padi-sec5-skills5/#remove-and-replace-the-scuba-kit-underwater","text":"This is one of the few skills in which it may be important to be negatively buoyant, s you could kneel on the bottom if necessary In open water, if possible, do this on insensitive bottom away from fragile aquati life. If dealing with entanglement or another safety situation, however, you probably hav no choice about where you do it With a weight-integrated BCD with a small or moderate amount of weight, or using a BCD without integrated weights, completely deflate your BCD and kneel on the bottom with your left knee down and your right knee up. If you have a lot of weight (while wearing a dry suit, for example), you may want enough air in the BCD so that it is not too negative to handle once you remove it Remove the BCD starting with your left arm, so the regulator stays in your mouth Keep the kit upright and do not let go of it. Stand it on your right knee. You may keep your right arm partially through the harness for control To keep stable on the bottom with weight-integrated BCDs, keep the unit close and positioned so that you don\u2019t float away from it. Keeping it on your knee is one effective way to do this Start with the right arm when you put it back on, so the regulator stays in your mouth Before fastening the straps, check to be sure you won\u2019t trap any hoses or accessories","title":"Remove and Replace the Scuba Kit Underwater"},{"location":"21-padi-sec5-skills5/#remove-and-replace-weight-system-underwater","text":"The main reasons for removing and replacing your weight system underwater are to make trim adjustments, to remove weight you can retrieve later, to replace a dislodged weight pouch or to untwist a weight belt This is the other skill in which it often helps to be negatively buoyant (again, on insensitive bottom), though it may not be necessary, depending upon your weight system and the amount of weight you\u2019re wearing Deflate your BCD and kneel Remove the weight pocket and rest it on your knee. If using a weight belt, release it and pull it from behind your back with the buckle (left side) free, and drape it over your knee Resting the weight on your knee helps you keep your balance Your instructor will show you the technique(s) suited to your kit","title":"Remove and Replace Weight System Underwater"},{"location":"21-padi-sec5-skills5/#descents-and-ascents-without-reference","text":"Equalize regularly as you descend Control your buoyancy. Slow descents and ascents are easier to control When ascending, watch your computer and be sure you ascend within its ascent rate When you ascend, make a safety stop by hovering in midwater at 5 metres/15 feet Practice this about half way to the surface in confined water If you need to navigate while descending and/or ascending, one buddy navigates while the other controls the depth and the descent/ascent rate","title":"Descents and Ascents Without Reference"},{"location":"21-padi-sec5-skills5/#minidive","text":"Dive planning, including SPG pressures and time for ending the dives Predive safety checks Use the PADI Skill Practice and Dive Planning Slate for dive planning and the predive safety check Entry Five point descents Neutral buoyancy practice Practicing skills with your buddy(ies) Air supply awareness and management Ending the dives based on SPG pressure or time Five point ascents Exits from water","title":"Minidive"},{"location":"21-padi-sec5-skills5/#confined-water-dive-five","text":"Briefing Equipment assembly, dive planning, gearing up and predive safety check Entry Weight and trim check and adjustment Five point descent without reference Remove and replace scuba kit underwater Remove and replace weight system underwater Minidive Dive planning, entry Weight check and neutral buoyancy practice Avoiding simulated sensitive zone or objects Simulated emergencies, skill practice, games/objectives Five point ascent without reference Exit Debrief","title":"Confined Water Dive Five"},{"location":"21-padi-sec5-skills5/#open-water-dive-three","text":"Briefing and dive planning Assemble, put on and adjust equipment Predive safety check Entry Weight and trim check, adjustment Controlled five point descent with visual reference Buoyancy control \u2013 establish neutral buoyancy with oral BCD inflation and hover Remove, replace and clear mask Underwater exploration Exit Debrief and log dive Post dive equipment care","title":"Open Water Dive Three"},{"location":"21-padi-sec5-skills5/#open-water-dive-four","text":"Briefing and dive planning Assemble, put on and adjust equipment Predive safety check Entry Weight and trim check, adjustment Five point free descent without reference Underwater exploration Ascent with safety stop Exit Debrief and log dive Post dive equipment care","title":"Open Water Dive Four"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/","text":"My ubuntu 20.04 system on Dell XPS 9350 [?] could not connect with anything on bluetooth. Here is how it was fixed. Step 1: Uninstall bluetooth sudo apt-get autoremove blueman bluez-utils bluez bluetooth Step 2: Reinstall bluetooth sudo apt install blueman -y && blueman-manager","title":"22 ubuntu bluetooth uninstall reinstall"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/#step-1-uninstall-bluetooth","text":"sudo apt-get autoremove blueman bluez-utils bluez bluetooth","title":"Step 1: Uninstall bluetooth"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/#step-2-reinstall-bluetooth","text":"sudo apt install blueman -y && blueman-manager","title":"Step 2: Reinstall bluetooth"},{"location":"23-advanced-big-query/","text":"Objectives Minimizing I/O Caching results of previous queries Performing efficient joins Avoid over-whelming single workers Using approximate aggregation functions Minimize I/O A query that computes the sum of three columns will be slower than a query that computes the sum of two columns, but most of the performance difference will be due to reading more data, not the extra addition. Therefore, a query that computes the mean of a column will be nearly as fast as a query whose aggregation method is to compute the variance of the data (even though computing variance requires BigQuery to keep track of both the sum and the sum of the squares) because most of the overhead of simple queries is caused by I/O, not by computation. * Do not use SELECT * * Reduce data being read * Reduce number of expensive computations Suppose we wish to find the total distance traveled by each bicycle in our dataset. A naive way to do this would be to find the distance traveled in each trip undertaken by each bicycle and sum them up: WITH trip_distance AS ( SELECT bike_id, ST_Distance(ST_GeogPoint(s.longitude, s.latitude), ST_GeogPoint(e.longitude, e.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_hire, `bigquery-public-data`.london_bicycles.cycle_stations s, `bigquery-public-data`.london_bicycles.cycle_stations e WHERE start_station_id = s.id AND end_station_id = e.id ) SELECT bike_id, SUM(distance)/1000 AS total_distance FROM trip_distance GROUP BY bike_id ORDER BY total_distance DESC LIMIT 5 Computing the distance is a pretty expensive operation and we can avoid joining the cycle_stations table against the cycle_hire table if we precompute the distances between all pairs of stations: WITH stations AS ( SELECT s.id AS start_id, e.id AS end_id, ST_Distance(ST_GeogPoint(s.longitude, s.latitude), ST_GeogPoint(e.longitude, e.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_stations s, `bigquery-public-data`.london_bicycles.cycle_stations e ), trip_distance AS ( SELECT bike_id, distance FROM `bigquery-public-data`.london_bicycles.cycle_hire, stations WHERE start_station_id = start_id AND end_station_id = end_id ) SELECT bike_id, SUM(distance)/1000 AS total_distance FROM trip_distance GROUP BY bike_id ORDER BY total_distance DESC LIMIT 5 Cache intermediate results It is possible to improve overall performance at the expense of increased I/O by taking advantage of temporary tables and materialized views. For example, suppose you have a number of queries that start out by finding the typical duration of trips between a pair of stations. The WITH clause (also called a Common Table Expression) improves readability but does not improve query speed or cost since results are not cached. The same holds for views and subqueries as well. If you find yourself using a WITH clause, view, or a subquery often, one way to potentially improve performance is to store the result into a table (or materialized view). First you will need to create a dataset named mydataset in the EU region (where the bicycle data resides) under your project in BigQuery. CREATE OR REPLACE TABLE mydataset.typical_trip AS SELECT start_station_name, end_station_name, APPROX_QUANTILES(duration, 10)[OFFSET (5)] AS typical_duration, COUNT(duration) AS num_trips FROM `bigquery-public-data`.london_bicycles.cycle_hire GROUP BY start_station_name, end_station_name Use the table created to find days when bicycle trips are much longer than usual: SELECT EXTRACT (DATE FROM start_date) AS trip_date, APPROX_QUANTILES(duration / typical_duration, 10)[OFFSET(5)] AS ratio, COUNT(*) AS num_trips_on_day FROM `bigquery-public-data`.london_bicycles.cycle_hire AS hire JOIN mydataset.typical_trip AS trip ON hire.start_station_name = trip.start_station_name AND hire.end_station_name = trip.end_station_name AND num_trips > 10 GROUP BY trip_date HAVING num_trips_on_day > 10 ORDER BY ratio DESC LIMIT 10 Accelerate queries with BI Engine If there are tables that you access frequently in Business Intelligence (BI) settings such as dashboards with aggregations and filters, one way to speed up your queries is to employ BI Engine. It will automatically store relevant pieces of data in memory (either actual columns from the table or derived results), and will use a specialized query processor tuned for working with mostly in-memory data. You can reserve the amount of memory (up to a current maximum of 10 GB) that BigQuery should use for its cache from the BigQuery Admin Console, under BI Engine. Make sure to reserve this memory in the same region as the dataset you are querying. Then, BigQuery will start to cache tables, parts of tables, and aggregations in memory and serve results faster. A primary use case for BI Engine is for tables that are accessed from dashboard tools such as Google Data Studio. By providing memory allocation for a BI Engine reservation, we can make dashboards that rely on a BigQuery backend much more responsive. Efficient joins Joining two tables requires data coordination and is subject to limitations imposed by the communication bandwidth between slots. If it is possible to avoid a join, or reduce the amount of data being joined, do so. Denormalization One way to improve the read performance and avoid joins is to give up on storing data efficiently, and instead add redundant copies of data. This is called denormalization. Thus, instead of storing the bicycle station latitudes and longitudes separately from the cycle hire information, we could create a denormalized table: CREATE OR REPLACE TABLE mydataset.london_bicycles_denorm AS SELECT start_station_id, s.latitude AS start_latitude, s.longitude AS start_longitude, end_station_id, e.latitude AS end_latitude, e.longitude AS end_longitude FROM `bigquery-public-data`.london_bicycles.cycle_hire AS h JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s ON h.start_station_id = s.id JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS e ON h.end_station_id = e.id Then, all subsequent queries will not need to carry out the join because the table will contain the necessary location information for all trips. In this case, you are trading off storage and reading more data against the computational expense of a join. It is quite possible that the cost of reading more data from disk will outweigh the cost of the join -- you should measure whether denormalization brings performance benefits. Avoid self-joins of large tables Self-joins happen when a table is joined with itself. While BigQuery supports self-joins, they can lead to performance degradation if the table being joined with itself is very large. In many cases, you can avoid the self-join by taking advantage of SQL features such as aggregation and window functions. Let\u2019s look at an example. One of the BigQuery public datasets is the dataset of baby names published by the US Social Security Administration. It is possible to query the dataset to find the most common male names in 2015 in the state of Massachusetts (Make sure your query is running in the US region by selecting More > Query settings > Processing location): SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'M' AND year = 2015 AND state = 'MA' ORDER BY num_babies DESC LIMIT 5 What are the most common names assigned to both male and female babies in the country over all the years in the dataset? A naive way to solve this problem involves reading the input table twice and doing a self-join: WITH male_babies AS ( SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'M' ), female_babies AS ( SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'F' ), both_genders AS ( SELECT name, SUM(m.num_babies) + SUM(f.num_babies) AS num_babies, SUM(m.num_babies) / (SUM(m.num_babies) + SUM(f.num_babies)) AS frac_male FROM male_babies AS m JOIN female_babies AS f USING (name) GROUP BY name ) SELECT * FROM both_genders WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5 The result is incorrect. A faster, more elegant (and correct!) solution is to recast the query to read the input only once and avoid the self-join completely. WITH all_babies AS ( SELECT name, SUM( IF (gender = 'M', number, 0)) AS male_babies, SUM( IF (gender = 'F', number, 0)) AS female_babies FROM `bigquery-public-data.usa_names.usa_1910_current` GROUP BY name ), both_genders AS ( SELECT name, (male_babies + female_babies) AS num_babies, SAFE_DIVIDE(male_babies, male_babies + female_babies) AS frac_male FROM all_babies WHERE male_babies > 0 AND female_babies > 0 ) SELECT * FROM both_genders WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5 Reduce data being joined It is possible to carry out the query above with an efficient join as long as we reduce the amount of data being joined by grouping the data by name and gender early on: Try the following query: WITH all_names AS ( SELECT name, gender, SUM(number) AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current GROUP BY name, gender ), male_names AS ( SELECT name, num_babies FROM all_names WHERE gender = 'M' ), female_names AS ( SELECT name, num_babies FROM all_names WHERE gender = 'F' ), ratio AS ( SELECT name, (f.num_babies + m.num_babies) AS num_babies, m.num_babies / (f.num_babies + m.num_babies) AS frac_male FROM male_names AS m JOIN female_names AS f USING (name) ) SELECT * FROM ratio WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5 The early grouping served to trim the data early in the query, before the query performs a JOIN. That way, shuffling and other complex operations only executed on the much smaller data and remain quite efficient. The query above finished in 2 seconds and returned the correct result. Use a window function instead of a self-join Suppose you wish to find the duration between a bike being dropped off and it being rented again, i.e., the duration that a bicycle stays at the station. This is an example of a dependent relationship between rows. It might appear that the only way to solve this is to join the table with itself, matching the end_date of one trip against the start_date of the next. (Make sure your query is running in the EU region by selecting More > Query settings > Processing location) You can, however, avoid a self-join by using a window function: SELECT bike_id, start_date, end_date, TIMESTAMP_DIFF( start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station FROM `bigquery-public-data`.london_bicycles.cycle_hire LIMIT 5 Using this, we can compute the average time that a bicycle is unused at each station and rank stations by that measure: WITH unused AS ( SELECT bike_id, start_station_name, start_date, end_date, TIMESTAMP_DIFF(start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station FROM `bigquery-public-data`.london_bicycles.cycle_hire ) SELECT start_station_name, AVG(time_at_station) AS unused_seconds FROM unused GROUP BY start_station_name ORDER BY unused_seconds ASC LIMIT 5 Join with precomputed values Sometimes, it can be helpful to precompute functions on smaller tables, and then join with the precomputed values rather than repeat an expensive calculation each time. For example, suppose we wish to find the pair of stations between which our customers ride bicycles at the fastest pace. To compute the pace (minutes per kilometer) at which they ride, we need to divide the duration of the ride by the distance between stations. We could create a denormalized table with distances between stations and then compute the average pace: WITH denormalized_table AS ( SELECT start_station_name, end_station_name, ST_DISTANCE(ST_GeogPoint(s1.longitude, s1.latitude), ST_GeogPoint(s2.longitude, s2.latitude)) AS distance, duration FROM `bigquery-public-data`.london_bicycles.cycle_hire AS h JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s1 ON h.start_station_id = s1.id JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s2 ON h.end_station_id = s2.id ), durations AS ( SELECT start_station_name, end_station_name, MIN(distance) AS distance, AVG(duration) AS duration, COUNT(*) AS num_rides FROM denormalized_table WHERE duration > 0 AND distance > 0 GROUP BY start_station_name, end_station_name HAVING num_rides > 100 ) SELECT start_station_name, end_station_name, distance, duration, duration/distance AS pace FROM durations ORDER BY pace ASC LIMIT 5 Alternately, we can use the cycle_stations table to precompute the distance between every pair of stations (this is a self-join) and then join it with the reduced-size table of average duration between stations: WITH distances AS ( SELECT a.id AS start_station_id, a.name AS start_station_name, b.id AS end_station_id, b.name AS end_station_name, ST_DISTANCE(ST_GeogPoint(a.longitude, a.latitude), ST_GeogPoint(b.longitude, b.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_stations a CROSS JOIN `bigquery-public-data`.london_bicycles.cycle_stations b WHERE a.id != b.id ), durations AS ( SELECT start_station_id, end_station_id, AVG(duration) AS duration, COUNT(*) AS num_rides FROM `bigquery-public-data`.london_bicycles.cycle_hire WHERE duration > 0 GROUP BY start_station_id, end_station_id HAVING num_rides > 100 ) SELECT start_station_name, end_station_name, distance, duration, duration/distance AS pace FROM distances JOIN durations USING (start_station_id, end_station_id) ORDER BY pace ASC LIMIT 5 Avoid overwhelming a worker Some operations (e.g. ordering) have to be carried out on a single worker. Having to sort too much data can overwhelm a worker\u2019s memory and result in a \u201cresources exceeded\u201d error. Avoid overwhelming the worker with too much data. As the hardware in Google data centers is upgraded, what \u201ctoo much\u201d means in this context expands over time. Currently, this is on the order of one GB. Limiting large sorts Let\u2019s say that we wish to go through the rentals and number them 1, 2, 3, etc. in the order that the rental ended. We could do that using the ROW_NUMBER() function SELECT rental_id, ROW_NUMBER() OVER(ORDER BY end_date) AS rental_number FROM `bigquery-public-data.london_bicycles.cycle_hire` ORDER BY rental_number ASC LIMIT 5 We might want to consider whether it is possible to limit the large sorts and distribute them. Indeed, it is possible to extract the date from the rentals and then sort trips within each day: WITH rentals_on_day AS ( SELECT rental_id, end_date, EXTRACT(DATE FROM end_date) AS rental_date FROM `bigquery-public-data.london_bicycles.cycle_hire` ) SELECT rental_id, rental_date, ROW_NUMBER() OVER(PARTITION BY rental_date ORDER BY end_date) AS rental_number_on_day FROM rentals_on_day ORDER BY rental_date ASC, rental_number_on_day ASC LIMIT 5 This is twice as faster because the sorting can be done on just a single day of data at a time. Data skew The same problem of overwhelming a worker (in this case, overwhelm the memory of the worker) can happen during an ARRAY_AGG with GROUP BY if one of the keys is much more common than the others. Because there are more than 3 million GitHub repositories and the commits are well distributed among them, this query succeeds (make sure you execute the query in the US processing center): SELECT repo_name, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits`, UNNEST(repo_name) AS repo_name GROUP BY repo_name Most of the people using GitHub live in only a few time zones, so grouping by the timezone fails -- we are asking a single worker to sort a significant fraction of 750GB: SELECT author.tz_offset, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits` GROUP BY author.tz_offset If you do require sorting all the data, use more granular keys (i.e. distribute the group\u2019s data over more workers) and then aggregate the results corresponding to the desired key. For example, instead of grouping only by the time zone, it is possible to group by both timezone and repo_name and then aggregate across repos to get the actual answer for each timezone: SELECT repo_name, author.tz_offset, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits`, UNNEST(repo_name) AS repo_name GROUP BY repo_name, author.tz_offset Approximate aggregation functions BigQuery provides fast, low-memory approximations of aggregate functions. Instead of using COUNT(DISTINCT \u2026), we can use APPROX_COUNT_DISTINCT on large data streams when a small statistical uncertainty in the result is tolerable. Approximate count We can find the number of unique GitHub repositories using: SELECT COUNT(DISTINCT repo_name) AS num_repos FROM `bigquery-public-data`.github_repos.commits, UNNEST(repo_name) AS repo_name Using the approximate function: SELECT APPROX_COUNT_DISTINCT(repo_name) AS num_repos FROM `bigquery-public-data`.github_repos.commits, UNNEST(repo_name) AS repo_name Resources Big Query Documentation on Query Performance","title":"Objectives"},{"location":"23-advanced-big-query/#objectives","text":"Minimizing I/O Caching results of previous queries Performing efficient joins Avoid over-whelming single workers Using approximate aggregation functions","title":"Objectives"},{"location":"23-advanced-big-query/#minimize-io","text":"A query that computes the sum of three columns will be slower than a query that computes the sum of two columns, but most of the performance difference will be due to reading more data, not the extra addition. Therefore, a query that computes the mean of a column will be nearly as fast as a query whose aggregation method is to compute the variance of the data (even though computing variance requires BigQuery to keep track of both the sum and the sum of the squares) because most of the overhead of simple queries is caused by I/O, not by computation. * Do not use SELECT * * Reduce data being read * Reduce number of expensive computations Suppose we wish to find the total distance traveled by each bicycle in our dataset. A naive way to do this would be to find the distance traveled in each trip undertaken by each bicycle and sum them up: WITH trip_distance AS ( SELECT bike_id, ST_Distance(ST_GeogPoint(s.longitude, s.latitude), ST_GeogPoint(e.longitude, e.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_hire, `bigquery-public-data`.london_bicycles.cycle_stations s, `bigquery-public-data`.london_bicycles.cycle_stations e WHERE start_station_id = s.id AND end_station_id = e.id ) SELECT bike_id, SUM(distance)/1000 AS total_distance FROM trip_distance GROUP BY bike_id ORDER BY total_distance DESC LIMIT 5 Computing the distance is a pretty expensive operation and we can avoid joining the cycle_stations table against the cycle_hire table if we precompute the distances between all pairs of stations: WITH stations AS ( SELECT s.id AS start_id, e.id AS end_id, ST_Distance(ST_GeogPoint(s.longitude, s.latitude), ST_GeogPoint(e.longitude, e.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_stations s, `bigquery-public-data`.london_bicycles.cycle_stations e ), trip_distance AS ( SELECT bike_id, distance FROM `bigquery-public-data`.london_bicycles.cycle_hire, stations WHERE start_station_id = start_id AND end_station_id = end_id ) SELECT bike_id, SUM(distance)/1000 AS total_distance FROM trip_distance GROUP BY bike_id ORDER BY total_distance DESC LIMIT 5","title":"Minimize I/O"},{"location":"23-advanced-big-query/#cache-intermediate-results","text":"It is possible to improve overall performance at the expense of increased I/O by taking advantage of temporary tables and materialized views. For example, suppose you have a number of queries that start out by finding the typical duration of trips between a pair of stations. The WITH clause (also called a Common Table Expression) improves readability but does not improve query speed or cost since results are not cached. The same holds for views and subqueries as well. If you find yourself using a WITH clause, view, or a subquery often, one way to potentially improve performance is to store the result into a table (or materialized view). First you will need to create a dataset named mydataset in the EU region (where the bicycle data resides) under your project in BigQuery. CREATE OR REPLACE TABLE mydataset.typical_trip AS SELECT start_station_name, end_station_name, APPROX_QUANTILES(duration, 10)[OFFSET (5)] AS typical_duration, COUNT(duration) AS num_trips FROM `bigquery-public-data`.london_bicycles.cycle_hire GROUP BY start_station_name, end_station_name Use the table created to find days when bicycle trips are much longer than usual: SELECT EXTRACT (DATE FROM start_date) AS trip_date, APPROX_QUANTILES(duration / typical_duration, 10)[OFFSET(5)] AS ratio, COUNT(*) AS num_trips_on_day FROM `bigquery-public-data`.london_bicycles.cycle_hire AS hire JOIN mydataset.typical_trip AS trip ON hire.start_station_name = trip.start_station_name AND hire.end_station_name = trip.end_station_name AND num_trips > 10 GROUP BY trip_date HAVING num_trips_on_day > 10 ORDER BY ratio DESC LIMIT 10","title":"Cache intermediate results"},{"location":"23-advanced-big-query/#accelerate-queries-with-bi-engine","text":"If there are tables that you access frequently in Business Intelligence (BI) settings such as dashboards with aggregations and filters, one way to speed up your queries is to employ BI Engine. It will automatically store relevant pieces of data in memory (either actual columns from the table or derived results), and will use a specialized query processor tuned for working with mostly in-memory data. You can reserve the amount of memory (up to a current maximum of 10 GB) that BigQuery should use for its cache from the BigQuery Admin Console, under BI Engine. Make sure to reserve this memory in the same region as the dataset you are querying. Then, BigQuery will start to cache tables, parts of tables, and aggregations in memory and serve results faster. A primary use case for BI Engine is for tables that are accessed from dashboard tools such as Google Data Studio. By providing memory allocation for a BI Engine reservation, we can make dashboards that rely on a BigQuery backend much more responsive.","title":"Accelerate queries with BI Engine"},{"location":"23-advanced-big-query/#efficient-joins","text":"Joining two tables requires data coordination and is subject to limitations imposed by the communication bandwidth between slots. If it is possible to avoid a join, or reduce the amount of data being joined, do so. Denormalization One way to improve the read performance and avoid joins is to give up on storing data efficiently, and instead add redundant copies of data. This is called denormalization. Thus, instead of storing the bicycle station latitudes and longitudes separately from the cycle hire information, we could create a denormalized table: CREATE OR REPLACE TABLE mydataset.london_bicycles_denorm AS SELECT start_station_id, s.latitude AS start_latitude, s.longitude AS start_longitude, end_station_id, e.latitude AS end_latitude, e.longitude AS end_longitude FROM `bigquery-public-data`.london_bicycles.cycle_hire AS h JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s ON h.start_station_id = s.id JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS e ON h.end_station_id = e.id Then, all subsequent queries will not need to carry out the join because the table will contain the necessary location information for all trips. In this case, you are trading off storage and reading more data against the computational expense of a join. It is quite possible that the cost of reading more data from disk will outweigh the cost of the join -- you should measure whether denormalization brings performance benefits.","title":"Efficient joins"},{"location":"23-advanced-big-query/#avoid-self-joins-of-large-tables","text":"Self-joins happen when a table is joined with itself. While BigQuery supports self-joins, they can lead to performance degradation if the table being joined with itself is very large. In many cases, you can avoid the self-join by taking advantage of SQL features such as aggregation and window functions. Let\u2019s look at an example. One of the BigQuery public datasets is the dataset of baby names published by the US Social Security Administration. It is possible to query the dataset to find the most common male names in 2015 in the state of Massachusetts (Make sure your query is running in the US region by selecting More > Query settings > Processing location): SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'M' AND year = 2015 AND state = 'MA' ORDER BY num_babies DESC LIMIT 5 What are the most common names assigned to both male and female babies in the country over all the years in the dataset? A naive way to solve this problem involves reading the input table twice and doing a self-join: WITH male_babies AS ( SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'M' ), female_babies AS ( SELECT name, number AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current WHERE gender = 'F' ), both_genders AS ( SELECT name, SUM(m.num_babies) + SUM(f.num_babies) AS num_babies, SUM(m.num_babies) / (SUM(m.num_babies) + SUM(f.num_babies)) AS frac_male FROM male_babies AS m JOIN female_babies AS f USING (name) GROUP BY name ) SELECT * FROM both_genders WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5 The result is incorrect. A faster, more elegant (and correct!) solution is to recast the query to read the input only once and avoid the self-join completely. WITH all_babies AS ( SELECT name, SUM( IF (gender = 'M', number, 0)) AS male_babies, SUM( IF (gender = 'F', number, 0)) AS female_babies FROM `bigquery-public-data.usa_names.usa_1910_current` GROUP BY name ), both_genders AS ( SELECT name, (male_babies + female_babies) AS num_babies, SAFE_DIVIDE(male_babies, male_babies + female_babies) AS frac_male FROM all_babies WHERE male_babies > 0 AND female_babies > 0 ) SELECT * FROM both_genders WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5","title":"Avoid self-joins of large tables"},{"location":"23-advanced-big-query/#reduce-data-being-joined","text":"It is possible to carry out the query above with an efficient join as long as we reduce the amount of data being joined by grouping the data by name and gender early on: Try the following query: WITH all_names AS ( SELECT name, gender, SUM(number) AS num_babies FROM `bigquery-public-data`.usa_names.usa_1910_current GROUP BY name, gender ), male_names AS ( SELECT name, num_babies FROM all_names WHERE gender = 'M' ), female_names AS ( SELECT name, num_babies FROM all_names WHERE gender = 'F' ), ratio AS ( SELECT name, (f.num_babies + m.num_babies) AS num_babies, m.num_babies / (f.num_babies + m.num_babies) AS frac_male FROM male_names AS m JOIN female_names AS f USING (name) ) SELECT * FROM ratio WHERE frac_male BETWEEN 0.3 AND 0.7 ORDER BY num_babies DESC LIMIT 5 The early grouping served to trim the data early in the query, before the query performs a JOIN. That way, shuffling and other complex operations only executed on the much smaller data and remain quite efficient. The query above finished in 2 seconds and returned the correct result.","title":"Reduce data being joined"},{"location":"23-advanced-big-query/#use-a-window-function-instead-of-a-self-join","text":"Suppose you wish to find the duration between a bike being dropped off and it being rented again, i.e., the duration that a bicycle stays at the station. This is an example of a dependent relationship between rows. It might appear that the only way to solve this is to join the table with itself, matching the end_date of one trip against the start_date of the next. (Make sure your query is running in the EU region by selecting More > Query settings > Processing location) You can, however, avoid a self-join by using a window function: SELECT bike_id, start_date, end_date, TIMESTAMP_DIFF( start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station FROM `bigquery-public-data`.london_bicycles.cycle_hire LIMIT 5 Using this, we can compute the average time that a bicycle is unused at each station and rank stations by that measure: WITH unused AS ( SELECT bike_id, start_station_name, start_date, end_date, TIMESTAMP_DIFF(start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station FROM `bigquery-public-data`.london_bicycles.cycle_hire ) SELECT start_station_name, AVG(time_at_station) AS unused_seconds FROM unused GROUP BY start_station_name ORDER BY unused_seconds ASC LIMIT 5","title":"Use a window function instead of a self-join"},{"location":"23-advanced-big-query/#join-with-precomputed-values","text":"Sometimes, it can be helpful to precompute functions on smaller tables, and then join with the precomputed values rather than repeat an expensive calculation each time. For example, suppose we wish to find the pair of stations between which our customers ride bicycles at the fastest pace. To compute the pace (minutes per kilometer) at which they ride, we need to divide the duration of the ride by the distance between stations. We could create a denormalized table with distances between stations and then compute the average pace: WITH denormalized_table AS ( SELECT start_station_name, end_station_name, ST_DISTANCE(ST_GeogPoint(s1.longitude, s1.latitude), ST_GeogPoint(s2.longitude, s2.latitude)) AS distance, duration FROM `bigquery-public-data`.london_bicycles.cycle_hire AS h JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s1 ON h.start_station_id = s1.id JOIN `bigquery-public-data`.london_bicycles.cycle_stations AS s2 ON h.end_station_id = s2.id ), durations AS ( SELECT start_station_name, end_station_name, MIN(distance) AS distance, AVG(duration) AS duration, COUNT(*) AS num_rides FROM denormalized_table WHERE duration > 0 AND distance > 0 GROUP BY start_station_name, end_station_name HAVING num_rides > 100 ) SELECT start_station_name, end_station_name, distance, duration, duration/distance AS pace FROM durations ORDER BY pace ASC LIMIT 5 Alternately, we can use the cycle_stations table to precompute the distance between every pair of stations (this is a self-join) and then join it with the reduced-size table of average duration between stations: WITH distances AS ( SELECT a.id AS start_station_id, a.name AS start_station_name, b.id AS end_station_id, b.name AS end_station_name, ST_DISTANCE(ST_GeogPoint(a.longitude, a.latitude), ST_GeogPoint(b.longitude, b.latitude)) AS distance FROM `bigquery-public-data`.london_bicycles.cycle_stations a CROSS JOIN `bigquery-public-data`.london_bicycles.cycle_stations b WHERE a.id != b.id ), durations AS ( SELECT start_station_id, end_station_id, AVG(duration) AS duration, COUNT(*) AS num_rides FROM `bigquery-public-data`.london_bicycles.cycle_hire WHERE duration > 0 GROUP BY start_station_id, end_station_id HAVING num_rides > 100 ) SELECT start_station_name, end_station_name, distance, duration, duration/distance AS pace FROM distances JOIN durations USING (start_station_id, end_station_id) ORDER BY pace ASC LIMIT 5","title":"Join with precomputed values"},{"location":"23-advanced-big-query/#avoid-overwhelming-a-worker","text":"Some operations (e.g. ordering) have to be carried out on a single worker. Having to sort too much data can overwhelm a worker\u2019s memory and result in a \u201cresources exceeded\u201d error. Avoid overwhelming the worker with too much data. As the hardware in Google data centers is upgraded, what \u201ctoo much\u201d means in this context expands over time. Currently, this is on the order of one GB.","title":"Avoid overwhelming a worker"},{"location":"23-advanced-big-query/#limiting-large-sorts","text":"Let\u2019s say that we wish to go through the rentals and number them 1, 2, 3, etc. in the order that the rental ended. We could do that using the ROW_NUMBER() function SELECT rental_id, ROW_NUMBER() OVER(ORDER BY end_date) AS rental_number FROM `bigquery-public-data.london_bicycles.cycle_hire` ORDER BY rental_number ASC LIMIT 5 We might want to consider whether it is possible to limit the large sorts and distribute them. Indeed, it is possible to extract the date from the rentals and then sort trips within each day: WITH rentals_on_day AS ( SELECT rental_id, end_date, EXTRACT(DATE FROM end_date) AS rental_date FROM `bigquery-public-data.london_bicycles.cycle_hire` ) SELECT rental_id, rental_date, ROW_NUMBER() OVER(PARTITION BY rental_date ORDER BY end_date) AS rental_number_on_day FROM rentals_on_day ORDER BY rental_date ASC, rental_number_on_day ASC LIMIT 5 This is twice as faster because the sorting can be done on just a single day of data at a time.","title":"Limiting large sorts"},{"location":"23-advanced-big-query/#data-skew","text":"The same problem of overwhelming a worker (in this case, overwhelm the memory of the worker) can happen during an ARRAY_AGG with GROUP BY if one of the keys is much more common than the others. Because there are more than 3 million GitHub repositories and the commits are well distributed among them, this query succeeds (make sure you execute the query in the US processing center): SELECT repo_name, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits`, UNNEST(repo_name) AS repo_name GROUP BY repo_name Most of the people using GitHub live in only a few time zones, so grouping by the timezone fails -- we are asking a single worker to sort a significant fraction of 750GB: SELECT author.tz_offset, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits` GROUP BY author.tz_offset If you do require sorting all the data, use more granular keys (i.e. distribute the group\u2019s data over more workers) and then aggregate the results corresponding to the desired key. For example, instead of grouping only by the time zone, it is possible to group by both timezone and repo_name and then aggregate across repos to get the actual answer for each timezone: SELECT repo_name, author.tz_offset, ARRAY_AGG(STRUCT(author, committer, subject, message, trailer, difference, encoding) ORDER BY author.date.seconds) FROM `bigquery-public-data.github_repos.commits`, UNNEST(repo_name) AS repo_name GROUP BY repo_name, author.tz_offset","title":"Data skew"},{"location":"23-advanced-big-query/#approximate-aggregation-functions","text":"BigQuery provides fast, low-memory approximations of aggregate functions. Instead of using COUNT(DISTINCT \u2026), we can use APPROX_COUNT_DISTINCT on large data streams when a small statistical uncertainty in the result is tolerable.","title":"Approximate aggregation functions"},{"location":"23-advanced-big-query/#approximate-count","text":"We can find the number of unique GitHub repositories using: SELECT COUNT(DISTINCT repo_name) AS num_repos FROM `bigquery-public-data`.github_repos.commits, UNNEST(repo_name) AS repo_name Using the approximate function: SELECT APPROX_COUNT_DISTINCT(repo_name) AS num_repos FROM `bigquery-public-data`.github_repos.commits, UNNEST(repo_name) AS repo_name","title":"Approximate count"},{"location":"23-advanced-big-query/#resources","text":"Big Query Documentation on Query Performance","title":"Resources"},{"location":"24-freedomticket-week1-intro-to-selling/","text":"1.01 Intro Helium10 Elite: Paid content and offline events Freedom Ticket Extra: $77/mo QA session access 1.02 Good opportunity FBA: Fulfilled by amazon FBM: Fulfilled by merchants Important things to focus on: Data and Marketing Aggregator buy brands at 2.5 to 5x multiples Non-US markets are less competitive 1.03 Make money Arbitrage retail online Merchandize Kindle Direct Publishing Wholesale Dropshipping Superstore niche lifestyle Affiliates Private labels can command 20-30% profit margin There are 2 types of seller account individual [free, but higher charge per sale] professional [$40/mo, added features, recommended] 1.04 Risks Financing is important. There are lines of credit specific for amazon sellers. 1.05 Principles of success Three points Product listing Pick profitable products Drive traffic Rule of 3: amazon will take a third of sales price, a third is product cost. The remaining third is the profit. Abandon products after 6 months. Compare with a benchmark such a stock market. Reviews are the currency of amazon. The first 20 reviews are critical. 5 metrics Keywords demand Aim for 20% profit Relative rank, in relative to competition [BSR: best seller rank] ROI, 150% or more Cash flow [do not run out of stock] 3 yr plan Learn and make mistakes Optimize and add products Prepare to sell profitable amazon business 1.06 Private Label Most control, make most money Easier to sell to aggregator or to strategic buyer Differentiating by Licensing Packaging Bundling Value add by design 1.07 How to start with less money Start small Kindle direct publisher Arbitrage Use other people's money 1.08 Examples of opportunities Helium10 blackbox for keywords Use X-Ray browser plugin Supplier search to find supplier in alibaba ROI calculator to estimate profit margin 1.09 Glossary Brand gating: Restriction by amazon to stop others from using the brand. Hard to get for new brands. Child: Variant of \"parent\" product. EXW: Exworks, you pay all the expenses from factory door. FOB: Freight on board, all the way to port is paid by factory. DDP: delivery duty paid, all the way to warehouse door is paid by factory. FNSKU: Specific code on top of ASIN. Relevant for same product sold by multiple vendors. Highjacker: Someone taking advantage of your brand and piggytailing sales. Landed cost: Total cost to get the product to amazon warehouse. LTL: Less than truckload. Keywords: how seller describes the product. Search terms: how users search. Prep center: service of bundling, editing products before shipping to customers. Removal order: item shipped back to you. Ungated: get permission to sell specific brands. 1.10 Financing 2.5 times of initial inventory cost [landed cost]. Amazon pays after 3-5 weeks. plan for fund to reorder inventory. Start with 1 product, minimum variation. Don't sell below $20 Consider high value items that have low velocity. 1.11 Profit and valuation spreadsheets Spreadsheet models to estimate profit and valuation. 1.12 Intro to ProjectX Bonus case study videos 1.13 Additional resources Serious seller pod cast AM/PM podcast for general ecommerce helium10 blog Facebook group Instagram YouTube (Trusted Partner Directory)[directory.Helium10.com] (Academy)[academy.helium10.com] Helium10 Elite","title":"24 freedomticket week1 intro to selling"},{"location":"24-freedomticket-week1-intro-to-selling/#101-intro","text":"Helium10 Elite: Paid content and offline events Freedom Ticket Extra: $77/mo QA session access","title":"1.01 Intro"},{"location":"24-freedomticket-week1-intro-to-selling/#102-good-opportunity","text":"FBA: Fulfilled by amazon FBM: Fulfilled by merchants Important things to focus on: Data and Marketing Aggregator buy brands at 2.5 to 5x multiples Non-US markets are less competitive","title":"1.02 Good opportunity"},{"location":"24-freedomticket-week1-intro-to-selling/#103-make-money","text":"Arbitrage retail online Merchandize Kindle Direct Publishing Wholesale Dropshipping Superstore niche lifestyle Affiliates Private labels can command 20-30% profit margin There are 2 types of seller account individual [free, but higher charge per sale] professional [$40/mo, added features, recommended]","title":"1.03 Make money"},{"location":"24-freedomticket-week1-intro-to-selling/#104-risks","text":"Financing is important. There are lines of credit specific for amazon sellers.","title":"1.04 Risks"},{"location":"24-freedomticket-week1-intro-to-selling/#105-principles-of-success","text":"Three points Product listing Pick profitable products Drive traffic Rule of 3: amazon will take a third of sales price, a third is product cost. The remaining third is the profit. Abandon products after 6 months. Compare with a benchmark such a stock market. Reviews are the currency of amazon. The first 20 reviews are critical. 5 metrics Keywords demand Aim for 20% profit Relative rank, in relative to competition [BSR: best seller rank] ROI, 150% or more Cash flow [do not run out of stock] 3 yr plan Learn and make mistakes Optimize and add products Prepare to sell profitable amazon business","title":"1.05 Principles of success"},{"location":"24-freedomticket-week1-intro-to-selling/#106-private-label","text":"Most control, make most money Easier to sell to aggregator or to strategic buyer Differentiating by Licensing Packaging Bundling Value add by design","title":"1.06 Private Label"},{"location":"24-freedomticket-week1-intro-to-selling/#107-how-to-start-with-less-money","text":"Start small Kindle direct publisher Arbitrage Use other people's money","title":"1.07 How to start with less money"},{"location":"24-freedomticket-week1-intro-to-selling/#108-examples-of-opportunities","text":"Helium10 blackbox for keywords Use X-Ray browser plugin Supplier search to find supplier in alibaba ROI calculator to estimate profit margin","title":"1.08 Examples of opportunities"},{"location":"24-freedomticket-week1-intro-to-selling/#109-glossary","text":"Brand gating: Restriction by amazon to stop others from using the brand. Hard to get for new brands. Child: Variant of \"parent\" product. EXW: Exworks, you pay all the expenses from factory door. FOB: Freight on board, all the way to port is paid by factory. DDP: delivery duty paid, all the way to warehouse door is paid by factory. FNSKU: Specific code on top of ASIN. Relevant for same product sold by multiple vendors. Highjacker: Someone taking advantage of your brand and piggytailing sales. Landed cost: Total cost to get the product to amazon warehouse. LTL: Less than truckload. Keywords: how seller describes the product. Search terms: how users search. Prep center: service of bundling, editing products before shipping to customers. Removal order: item shipped back to you. Ungated: get permission to sell specific brands.","title":"1.09 Glossary"},{"location":"24-freedomticket-week1-intro-to-selling/#110-financing","text":"2.5 times of initial inventory cost [landed cost]. Amazon pays after 3-5 weeks. plan for fund to reorder inventory. Start with 1 product, minimum variation. Don't sell below $20 Consider high value items that have low velocity.","title":"1.10 Financing"},{"location":"24-freedomticket-week1-intro-to-selling/#111-profit-and-valuation-spreadsheets","text":"Spreadsheet models to estimate profit and valuation.","title":"1.11 Profit and valuation spreadsheets"},{"location":"24-freedomticket-week1-intro-to-selling/#112-intro-to-projectx","text":"Bonus case study videos","title":"1.12 Intro to ProjectX"},{"location":"24-freedomticket-week1-intro-to-selling/#113-additional-resources","text":"Serious seller pod cast AM/PM podcast for general ecommerce helium10 blog Facebook group Instagram YouTube (Trusted Partner Directory)[directory.Helium10.com] (Academy)[academy.helium10.com] Helium10 Elite","title":"1.13 Additional resources"},{"location":"25-gcp-essential-infrastructure-cloud-sql/","text":"Implementing Cloud SQL Objectives Create a Cloud SQL database Configure a virtual machine to run a proxy Create a connection between an application and Cloud SQL Connect an application to Cloud SQL using Private IP address By the end of this lab, we will have 2 working instances of the Wordpress frontend connected over 2 different connection types to their SQL instance backend. Step 1: Create a Cloud SQL Database In this task, you configure a SQL server according to Google Cloud best practices and create a Private IP connection. On the Navigation menu (Navigation menu), click SQL. Click Create instance. Click Choose MySQL. Specify the following, and leave the remaining settings as their defaults: Property Value Instance ID wordpress-db Root password type a password Region us-central1 Zone Any Database Version MySQL 5.7 Note the root password; it will be used in a later step and referred to as [ROOT_PASSWORD]. Expand Show configuration options. Expand the Machine type section. Provision the right amount of vCPU and memory. To choose a Machine Type, click the dropdown menu, and then explore your options. A few points to consider: Shared-core machines are good for prototyping, and are not covered by Cloud SLA. Each vCPU is subject to a 250 MB/s network throughput cap for peak performance. Each additional core increases the network cap, up to a theoretical maximum of 2000 MB/s. For performance-sensitive workloads such as online transaction processing (OLTP), a general guideline is to ensure that your instance has enough memory to contain the entire working set and accommodate the number of active connections. For this lab, select standard from the dropdown menu, and then select 1 vCPU, 3.75 GB. Next, expand the Storage section and then choose Storage type and Storage capacity. A few points to consider: SSD (solid-state drive) is the best choice for most use cases. HDD (hard-disk drive) offers lower performance, but storage costs are significantly reduced, so HDD may be preferable for storing data that is infrequently accessed and does not require very low latency. There is a direct relationship between the storage capacity and its throughput. Click each of the capacity options to see how it affects the throughput. Reset the option to 10GB. Setting your storage capacity too low without enabling an automatic storage increase can cause your instance to lose its SLA. Expand the Connections section. Select Private IP. In the Network dropdown, select default. Click the Set up Connection button that appears. In the panel to the right, click Enable API, click Use an automatically allocated IP range, click Continue, and then click Create Connection. Click Create Instance at the bottom of the page to create the database instance. Task 2: Configure a proxy on a virtual machine When your application does not reside in the same VPC connected network and region as your Cloud SQL instance, use a proxy to secure its external connection. In order to configure the proxy, you need the Cloud SQL instance connection name. The lab comes with 2 virtual machines preconfigured with Wordpress and its dependencies. You can view the startup script and service account access by clicking on a virtual machine name. Notice that we used the principle of least privilege and only allow SQL access for that VM. There's also a network tag and a firewall preconfigured to allow port 80 from any host. On the Navigation menu (Navigation menu) click Compute Engine. Click SSH next to wordpress-us-west1-proxy. Download the Cloud SQL Proxy and make it executable: wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy && chmod +x cloud_sql_proxy In order to start the proxy, you need the connection name of the Cloud SQL instance. Keep your SSH window open and return to the Cloud Console. On the Navigation menu (Navigation menu), click SQL. Click on the wordpress-db instance and wait for a green checkmark next to its name, which indicates that it is operational (this could take a couple of minutes). Note the Instance connection name; it will be used later and referred to as [SQL_CONNECTION_NAME]. In addition, for the application to work, you need to create a table. Click Databases. Click Create database, type wordpress, which is the name the application expects, and then click Create. Return to the SSH window and save the connection name in an environment variable, replacing [SQL_CONNECTION_NAME] with the unique name you copied in a previous step. export SQL_CONNECTION=[SQL_CONNECTION_NAME] To verify that the environment variable is set, run: echo $SQL_CONNECTION To activate the proxy connection to your Cloud SQL database and send the process to the background, run the following command: ./cloud_sql_proxy -instances=$SQL_CONNECTION=tcp:3306 & The expected output is Listening on 127.0.0.1:3306 for [SQL_CONNECTION_NAME] Ready for new connections Press ENTER The proxy will listen on 127.0.0.1:3306 (localhost) and proxy that connects securely to your Cloud SQL over a secure tunnel using the machine's external IP address. Task 3: Connect an application to the Cloud SQL instance In this task, you will connect a sample application to the Cloud SQL instance. Configure the Wordpress application. To find the external IP address of your virtual machine, query its metadata: curl -H \"Metadata-Flavor: Google\" http://169.254.169.254/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip && echo Go to the wordpress-us-west1-proxy external IP address in your browser and configure the Wordpress application. Click Let's Go. Specify the following, replacing [ROOT_PASSWORD] with the password you configured upon machine creation, and leave the remaining settings as their defaults: Property Value Username root Password [ROOT_PASSWORD] Database Host 127.0.0.1 You are using 127.0.0.1, localhost as the Database IP because the proxy you initiated listens on this address and redirects that traffic to your SQL server securely. When a 'Success!' window appears, remove the text after the IP address in your web browser's address bar and press ENTER. You'll be presented with a working Wordpress Blog! Task 4: Connect to Cloud SQL via internal IP If you can host your application in the same region and VPC connected network as your Cloud SQL, you can leverage a more secure and performant configuration using Private IP. By using Private IP, you will increase performance by reducing latency and minimize the attack surface of your Cloud SQL instance because you can communicate with it exclusively over internal IPs. In the Cloud Console, on the Navigation menu (Navigation menu), click SQL. Click wordpress-db. Note the Private IP address of the Cloud SQL server; it will be referred to as [SQL_PRIVATE_IP]. On the Navigation menu, click Compute Engine. Notice that wordpress-us-private-ip is located at us-central1, where your Cloud SQL is located, which enables you to leverage a more secure connection. Copy the external IP address of wordpress-us-private-ip, paste it in a browser window, and press ENTER. Click Let's Go. Specify the following, and leave the remaining settings as their defaults: Property Value Username root Password type the [ROOT_PASSWORD] configured when the Cloud SQL instance was created Database Host [SQL_PRIVATE_IP] Click Submit. Notice that this time you are creating a direct connection to a Private IP, instead of configuring a proxy. That connection is private, which means that it doesn't egress to the internet and therefore benefits from better performance and security. Click Run the installation. An 'Already Installed!' window is displayed, which means that your application is connected to the Cloud SQL server over private IP. In your web browser's address bar, remove the text after the IP address and press ENTER. You'll be presented with a working Wordpress Blog!","title":"Implementing Cloud SQL"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#implementing-cloud-sql","text":"","title":"Implementing Cloud SQL"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#objectives","text":"Create a Cloud SQL database Configure a virtual machine to run a proxy Create a connection between an application and Cloud SQL Connect an application to Cloud SQL using Private IP address By the end of this lab, we will have 2 working instances of the Wordpress frontend connected over 2 different connection types to their SQL instance backend.","title":"Objectives"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#step-1-create-a-cloud-sql-database","text":"In this task, you configure a SQL server according to Google Cloud best practices and create a Private IP connection. On the Navigation menu (Navigation menu), click SQL. Click Create instance. Click Choose MySQL. Specify the following, and leave the remaining settings as their defaults: Property Value Instance ID wordpress-db Root password type a password Region us-central1 Zone Any Database Version MySQL 5.7 Note the root password; it will be used in a later step and referred to as [ROOT_PASSWORD]. Expand Show configuration options. Expand the Machine type section. Provision the right amount of vCPU and memory. To choose a Machine Type, click the dropdown menu, and then explore your options. A few points to consider: Shared-core machines are good for prototyping, and are not covered by Cloud SLA. Each vCPU is subject to a 250 MB/s network throughput cap for peak performance. Each additional core increases the network cap, up to a theoretical maximum of 2000 MB/s. For performance-sensitive workloads such as online transaction processing (OLTP), a general guideline is to ensure that your instance has enough memory to contain the entire working set and accommodate the number of active connections. For this lab, select standard from the dropdown menu, and then select 1 vCPU, 3.75 GB. Next, expand the Storage section and then choose Storage type and Storage capacity. A few points to consider: SSD (solid-state drive) is the best choice for most use cases. HDD (hard-disk drive) offers lower performance, but storage costs are significantly reduced, so HDD may be preferable for storing data that is infrequently accessed and does not require very low latency. There is a direct relationship between the storage capacity and its throughput. Click each of the capacity options to see how it affects the throughput. Reset the option to 10GB. Setting your storage capacity too low without enabling an automatic storage increase can cause your instance to lose its SLA. Expand the Connections section. Select Private IP. In the Network dropdown, select default. Click the Set up Connection button that appears. In the panel to the right, click Enable API, click Use an automatically allocated IP range, click Continue, and then click Create Connection. Click Create Instance at the bottom of the page to create the database instance.","title":"Step 1: Create a Cloud SQL Database"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-2-configure-a-proxy-on-a-virtual-machine","text":"When your application does not reside in the same VPC connected network and region as your Cloud SQL instance, use a proxy to secure its external connection. In order to configure the proxy, you need the Cloud SQL instance connection name. The lab comes with 2 virtual machines preconfigured with Wordpress and its dependencies. You can view the startup script and service account access by clicking on a virtual machine name. Notice that we used the principle of least privilege and only allow SQL access for that VM. There's also a network tag and a firewall preconfigured to allow port 80 from any host. On the Navigation menu (Navigation menu) click Compute Engine. Click SSH next to wordpress-us-west1-proxy. Download the Cloud SQL Proxy and make it executable: wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy && chmod +x cloud_sql_proxy In order to start the proxy, you need the connection name of the Cloud SQL instance. Keep your SSH window open and return to the Cloud Console. On the Navigation menu (Navigation menu), click SQL. Click on the wordpress-db instance and wait for a green checkmark next to its name, which indicates that it is operational (this could take a couple of minutes). Note the Instance connection name; it will be used later and referred to as [SQL_CONNECTION_NAME]. In addition, for the application to work, you need to create a table. Click Databases. Click Create database, type wordpress, which is the name the application expects, and then click Create. Return to the SSH window and save the connection name in an environment variable, replacing [SQL_CONNECTION_NAME] with the unique name you copied in a previous step. export SQL_CONNECTION=[SQL_CONNECTION_NAME] To verify that the environment variable is set, run: echo $SQL_CONNECTION To activate the proxy connection to your Cloud SQL database and send the process to the background, run the following command: ./cloud_sql_proxy -instances=$SQL_CONNECTION=tcp:3306 & The expected output is Listening on 127.0.0.1:3306 for [SQL_CONNECTION_NAME] Ready for new connections Press ENTER The proxy will listen on 127.0.0.1:3306 (localhost) and proxy that connects securely to your Cloud SQL over a secure tunnel using the machine's external IP address.","title":"Task 2: Configure a proxy on a virtual machine"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-3-connect-an-application-to-the-cloud-sql-instance","text":"In this task, you will connect a sample application to the Cloud SQL instance. Configure the Wordpress application. To find the external IP address of your virtual machine, query its metadata: curl -H \"Metadata-Flavor: Google\" http://169.254.169.254/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip && echo Go to the wordpress-us-west1-proxy external IP address in your browser and configure the Wordpress application. Click Let's Go. Specify the following, replacing [ROOT_PASSWORD] with the password you configured upon machine creation, and leave the remaining settings as their defaults: Property Value Username root Password [ROOT_PASSWORD] Database Host 127.0.0.1 You are using 127.0.0.1, localhost as the Database IP because the proxy you initiated listens on this address and redirects that traffic to your SQL server securely. When a 'Success!' window appears, remove the text after the IP address in your web browser's address bar and press ENTER. You'll be presented with a working Wordpress Blog!","title":"Task 3: Connect an application to the Cloud SQL instance"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-4-connect-to-cloud-sql-via-internal-ip","text":"If you can host your application in the same region and VPC connected network as your Cloud SQL, you can leverage a more secure and performant configuration using Private IP. By using Private IP, you will increase performance by reducing latency and minimize the attack surface of your Cloud SQL instance because you can communicate with it exclusively over internal IPs. In the Cloud Console, on the Navigation menu (Navigation menu), click SQL. Click wordpress-db. Note the Private IP address of the Cloud SQL server; it will be referred to as [SQL_PRIVATE_IP]. On the Navigation menu, click Compute Engine. Notice that wordpress-us-private-ip is located at us-central1, where your Cloud SQL is located, which enables you to leverage a more secure connection. Copy the external IP address of wordpress-us-private-ip, paste it in a browser window, and press ENTER. Click Let's Go. Specify the following, and leave the remaining settings as their defaults: Property Value Username root Password type the [ROOT_PASSWORD] configured when the Cloud SQL instance was created Database Host [SQL_PRIVATE_IP] Click Submit. Notice that this time you are creating a direct connection to a Private IP, instead of configuring a proxy. That connection is private, which means that it doesn't egress to the internet and therefore benefits from better performance and security. Click Run the installation. An 'Already Installed!' window is displayed, which means that your application is connected to the Cloud SQL server over private IP. In your web browser's address bar, remove the text after the IP address and press ENTER. You'll be presented with a working Wordpress Blog!","title":"Task 4: Connect to Cloud SQL via internal IP"},{"location":"26-gcp-essential-infrastructure-monitoring/","text":"Resource Monitoring Objectives In this lab, you learn how to perform the following tasks: Explore Cloud Monitoring Add charts to dashboards Create alerts with multiple conditions Create resource groups Create uptime checks Tasks Task 1: Create a Cloud Monitoring workspace Multiple projects can be connected to a workspace. Access to the monitoring workspace will provide access to metrics of all projects connected to the workspace. As a result, we need to create workspaces to manage access of Dev Ops by project. Task 2: Custom dashboards, Metric explorer Task 3: Alerting policies, Notification channels Its better to alert for symptoms instead of errors. Email, slack channel, pub/sub, webhooks etc. Task 4: Resource groups Can make groups of resources based on name, type, labels, etc. Task 5: Uptime monitoring Check uptime at a set interval. Error Reporting and Debugging Objectives In this lab, you learn how to perform the following tasks: Launch a simple Google App Engine application Introduce an error into the application Explore Cloud Error Reporting Use Cloud Debugger to identify the error in the code Fix the bug and monitor in Cloud Operations Task 1: Create an application Get and test the application In the Cloud Console, launch Cloud Shell by clicking Activate Cloud Shell ( 857dc9d7dd799cb2.png). If prompted, click Continue. To create a local folder and get the App Engine Hello world application, run the following commands: mkdir appengine-hello cd appengine-hello gsutil cp gs://cloud-training/archinfra/gae-hello/* . To run the application using the local development server in Cloud Shell, run the following command: dev_appserver.py $(pwd) In Cloud Shell, click Web Preview > Preview on port 8080 to view the application. You may have to collapse the Navigation menu pane to access the Web Preview icon. In Cloud Shell, press Ctrl+C to exit the development server. Deploy the application to App Engine To deploy the application to App Engine, run the following command: gcloud app deploy app.yaml If prompted for a region, enter the number corresponding to a region. When prompted, type Y to continue. When the process is done, verify that the application is working by running the following command: gcloud app browse Introduce an error to break the application To examine the main.py file, run the following command: cat main.py To use the sed stream editor to change the import library to the nonexistent webapp22, run the following command: sed -i -e 's/webapp2/webapp22/' main.py To verify the change you made in the main.py file, run the cat main.py again. Re-deploy the application to App Engine To re-deploy the application to App Engine, run the following command: gcloud app deploy app.yaml --quiet The --quiet flag disables all interactive prompts when running gcloud commands. If input is required, defaults will be used. In this case, it avoids the need for you to type Y when prompted to continue the deployment. When the process is done, verify that the application is broken by running the following command: gcloud app browse If Cloud Shell does not detect your browser, click the link in the Cloud Shell output to view your app. If needed, press Ctrl+C to exit development mode. Leave Cloud Shell open. Task 2: Explore Cloud Error Reporting View Error Reporting and trigger additional errors In the Cloud Console, on the Navigation menu ( 7a91d354499ac9f1.png), click Error Reporting. You should see an error regarding the failed import of webapp22. Click Auto reload. In Cloud Shell, run the following command: gcloud app browse Click the link several times to generate more errors. View details and identify the cause Click the Error name: ImportError: No module named webapp22. Now you can see a detailed graph of the errors. The Response Code field shows the explicit error: a 500 Internal Server Error. For Stack trace sample, click Parsed. This opens the Cloud Debugger, showing the error in the code!","title":"Resource Monitoring"},{"location":"26-gcp-essential-infrastructure-monitoring/#resource-monitoring","text":"","title":"Resource Monitoring"},{"location":"26-gcp-essential-infrastructure-monitoring/#objectives","text":"In this lab, you learn how to perform the following tasks: Explore Cloud Monitoring Add charts to dashboards Create alerts with multiple conditions Create resource groups Create uptime checks","title":"Objectives"},{"location":"26-gcp-essential-infrastructure-monitoring/#tasks","text":"","title":"Tasks"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-1-create-a-cloud-monitoring-workspace","text":"Multiple projects can be connected to a workspace. Access to the monitoring workspace will provide access to metrics of all projects connected to the workspace. As a result, we need to create workspaces to manage access of Dev Ops by project.","title":"Task 1: Create a Cloud Monitoring workspace"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-2-custom-dashboards-metric-explorer","text":"","title":"Task 2: Custom dashboards, Metric explorer"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-3-alerting-policies-notification-channels","text":"Its better to alert for symptoms instead of errors. Email, slack channel, pub/sub, webhooks etc.","title":"Task 3: Alerting policies, Notification channels"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-4-resource-groups","text":"Can make groups of resources based on name, type, labels, etc.","title":"Task 4: Resource groups"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-5-uptime-monitoring","text":"Check uptime at a set interval.","title":"Task 5: Uptime monitoring"},{"location":"26-gcp-essential-infrastructure-monitoring/#error-reporting-and-debugging","text":"","title":"Error Reporting and Debugging"},{"location":"26-gcp-essential-infrastructure-monitoring/#objectives_1","text":"In this lab, you learn how to perform the following tasks: Launch a simple Google App Engine application Introduce an error into the application Explore Cloud Error Reporting Use Cloud Debugger to identify the error in the code Fix the bug and monitor in Cloud Operations","title":"Objectives"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-1-create-an-application","text":"","title":"Task 1: Create an application"},{"location":"26-gcp-essential-infrastructure-monitoring/#get-and-test-the-application","text":"In the Cloud Console, launch Cloud Shell by clicking Activate Cloud Shell ( 857dc9d7dd799cb2.png). If prompted, click Continue. To create a local folder and get the App Engine Hello world application, run the following commands: mkdir appengine-hello cd appengine-hello gsutil cp gs://cloud-training/archinfra/gae-hello/* . To run the application using the local development server in Cloud Shell, run the following command: dev_appserver.py $(pwd) In Cloud Shell, click Web Preview > Preview on port 8080 to view the application. You may have to collapse the Navigation menu pane to access the Web Preview icon. In Cloud Shell, press Ctrl+C to exit the development server.","title":"Get and test the application"},{"location":"26-gcp-essential-infrastructure-monitoring/#deploy-the-application-to-app-engine","text":"To deploy the application to App Engine, run the following command: gcloud app deploy app.yaml If prompted for a region, enter the number corresponding to a region. When prompted, type Y to continue. When the process is done, verify that the application is working by running the following command: gcloud app browse","title":"Deploy the application to App Engine"},{"location":"26-gcp-essential-infrastructure-monitoring/#introduce-an-error-to-break-the-application","text":"To examine the main.py file, run the following command: cat main.py To use the sed stream editor to change the import library to the nonexistent webapp22, run the following command: sed -i -e 's/webapp2/webapp22/' main.py To verify the change you made in the main.py file, run the cat main.py again.","title":"Introduce an error to break the application"},{"location":"26-gcp-essential-infrastructure-monitoring/#re-deploy-the-application-to-app-engine","text":"To re-deploy the application to App Engine, run the following command: gcloud app deploy app.yaml --quiet The --quiet flag disables all interactive prompts when running gcloud commands. If input is required, defaults will be used. In this case, it avoids the need for you to type Y when prompted to continue the deployment. When the process is done, verify that the application is broken by running the following command: gcloud app browse If Cloud Shell does not detect your browser, click the link in the Cloud Shell output to view your app. If needed, press Ctrl+C to exit development mode. Leave Cloud Shell open.","title":"Re-deploy the application to App Engine"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-2-explore-cloud-error-reporting","text":"","title":"Task 2: Explore Cloud Error Reporting"},{"location":"26-gcp-essential-infrastructure-monitoring/#view-error-reporting-and-trigger-additional-errors","text":"In the Cloud Console, on the Navigation menu ( 7a91d354499ac9f1.png), click Error Reporting. You should see an error regarding the failed import of webapp22. Click Auto reload. In Cloud Shell, run the following command: gcloud app browse Click the link several times to generate more errors.","title":"View Error Reporting and trigger additional errors"},{"location":"26-gcp-essential-infrastructure-monitoring/#view-details-and-identify-the-cause","text":"Click the Error name: ImportError: No module named webapp22. Now you can see a detailed graph of the errors. The Response Code field shows the explicit error: a 500 Internal Server Error. For Stack trace sample, click Parsed. This opens the Cloud Debugger, showing the error in the code!","title":"View details and identify the cause"},{"location":"27-gcp-elastic-cloud-vpn/","text":"Virtual Private Networks (VPN) Objectives Create VPN gateways in each network Create VPN tunnels between the gateways Verify VPN connectivity Task 1: Explore the networks and instances Ping VM instances external IP using ping -c 3 <external IP address> Task 2: Create the VPN gateways and tunnels Establish private communication between the two VM instances by creating VPN gateways and tunnels between the two networks. Reserve two static IP addresses Reserve one static IP address for each VPN gateway. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > External IP addresses. Click Reserve static address. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-1-static-ip IP version IPv4 Region us-central1 Click Reserve. Repeat the same for vpn-2-static-ip. Click Reserve static address. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-2-static-ip IP version IPv4 Region europe-west1 Create the vpn-1 gateway and tunnel1to2 In the Cloud Console, on the Navigation menu (Navigation menu), click Hybrid Connectivity > VPN. Click Create VPN Connection. If asked, select Classic VPN, and then click Continue. Specify the following in the VPN gateway section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-1 Network vpn-network-1 Region us-central1 IP address vpn-1-static-ip Specify the following in the Tunnels section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name tunnel1to2 Remote peer IP address [VPN-2-STATIC-IP] IKE pre-shared key gcprocks Routing options Route-based Remote network IP ranges 10.1.3.0/24 [internal ip address for vm instance 2] Equivalent command line: gcloud compute target-vpn-gateways create vpn-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --network=vpn-network-1 gcloud compute forwarding-rules create vpn-1-rule-esp --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=ESP --target-vpn-gateway=vpn-1 gcloud compute forwarding-rules create vpn-1-rule-udp500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=500 --target-vpn-gateway=vpn-1 gcloud compute forwarding-rules create vpn-1-rule-udp4500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=4500 --target-vpn-gateway=vpn-1 gcloud compute vpn-tunnels create tunnel1to2 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --peer-address=34.79.170.227 --shared-secret=gcprocks --ike-version=2 --local-traffic-selector=0.0.0.0/0 --remote-traffic-selector=0.0.0.0/0 --target-vpn-gateway=vpn-1 gcloud compute routes create tunnel1to2-route-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --network=vpn-network-1 --priority=1000 --destination-range=10.1.3.0/24 --next-hop-vpn-tunnel=tunnel1to2 --next-hop-vpn-tunnel-region=us-central1 Click Create. Create the vpn-2 gateway and tunnel2to1 Click VPN setup wizard. If asked, select Classic VPN, and then click Continue. Specify the following in the VPN gateway section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-2 Network vpn-network-2 Region europe-west1 IP address vpn-2-static-ip Specify the following in the Tunnels section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name tunnel2to1 Remote peer IP address [VPN-1-STATIC-IP] IKE pre-shared key gcprocks Routing options Route-based Remote network IP ranges 10.5.4.0/24 [internal ip address for vm instance 1] Task 3: Verify VPN connectivity Now we should be able to ping both internal and external IP addresses. Verify server-1 to server-2 connectivity In the Cloud Console, on the Navigation menu, click Compute Engine > VM instances. For server-1, click SSH to launch a terminal and connect. To test connectivity to server-2's internal IP address, run the following command: ping -c 3 <insert server-2's internal IP address here> Remove the external IP addresses Now that you verified VPN connectivity, you can remove the instances' external IP addresses. For demonstration purposes, just do this for the server-1 instance. On the Navigation menu, click Compute Engine > VM instances. Select the server-1 instance and click Stop. Wait for the instance to stop. Instances need to be stopped before you can make changes to their network interfaces. Click on the name of the server-1 instance to open the VM instance details page. Click Edit. For Network interfaces, click the Edit icon (Edit). Change External IP to None. Click Done. Click Save and wait for the instance details to update. Click Start. Click Start again to confirm that you want to start the VM instance. Return to the VM instances page and wait for the instance to start. Notice that External IP is set to None for the server-1 instance.","title":"Virtual Private Networks (VPN)"},{"location":"27-gcp-elastic-cloud-vpn/#virtual-private-networks-vpn","text":"","title":"Virtual Private Networks (VPN)"},{"location":"27-gcp-elastic-cloud-vpn/#objectives","text":"Create VPN gateways in each network Create VPN tunnels between the gateways Verify VPN connectivity","title":"Objectives"},{"location":"27-gcp-elastic-cloud-vpn/#task-1-explore-the-networks-and-instances","text":"Ping VM instances external IP using ping -c 3 <external IP address>","title":"Task 1: Explore the networks and instances"},{"location":"27-gcp-elastic-cloud-vpn/#task-2-create-the-vpn-gateways-and-tunnels","text":"Establish private communication between the two VM instances by creating VPN gateways and tunnels between the two networks.","title":"Task 2: Create the VPN gateways and tunnels"},{"location":"27-gcp-elastic-cloud-vpn/#reserve-two-static-ip-addresses","text":"Reserve one static IP address for each VPN gateway. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > External IP addresses. Click Reserve static address. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-1-static-ip IP version IPv4 Region us-central1 Click Reserve. Repeat the same for vpn-2-static-ip. Click Reserve static address. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-2-static-ip IP version IPv4 Region europe-west1","title":"Reserve two static IP addresses"},{"location":"27-gcp-elastic-cloud-vpn/#create-the-vpn-1-gateway-and-tunnel1to2","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click Hybrid Connectivity > VPN. Click Create VPN Connection. If asked, select Classic VPN, and then click Continue. Specify the following in the VPN gateway section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-1 Network vpn-network-1 Region us-central1 IP address vpn-1-static-ip Specify the following in the Tunnels section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name tunnel1to2 Remote peer IP address [VPN-2-STATIC-IP] IKE pre-shared key gcprocks Routing options Route-based Remote network IP ranges 10.1.3.0/24 [internal ip address for vm instance 2] Equivalent command line: gcloud compute target-vpn-gateways create vpn-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --network=vpn-network-1 gcloud compute forwarding-rules create vpn-1-rule-esp --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=ESP --target-vpn-gateway=vpn-1 gcloud compute forwarding-rules create vpn-1-rule-udp500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=500 --target-vpn-gateway=vpn-1 gcloud compute forwarding-rules create vpn-1-rule-udp4500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=4500 --target-vpn-gateway=vpn-1 gcloud compute vpn-tunnels create tunnel1to2 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --peer-address=34.79.170.227 --shared-secret=gcprocks --ike-version=2 --local-traffic-selector=0.0.0.0/0 --remote-traffic-selector=0.0.0.0/0 --target-vpn-gateway=vpn-1 gcloud compute routes create tunnel1to2-route-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --network=vpn-network-1 --priority=1000 --destination-range=10.1.3.0/24 --next-hop-vpn-tunnel=tunnel1to2 --next-hop-vpn-tunnel-region=us-central1 Click Create.","title":"Create the vpn-1 gateway and tunnel1to2"},{"location":"27-gcp-elastic-cloud-vpn/#create-the-vpn-2-gateway-and-tunnel2to1","text":"Click VPN setup wizard. If asked, select Classic VPN, and then click Continue. Specify the following in the VPN gateway section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name vpn-2 Network vpn-network-2 Region europe-west1 IP address vpn-2-static-ip Specify the following in the Tunnels section, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name tunnel2to1 Remote peer IP address [VPN-1-STATIC-IP] IKE pre-shared key gcprocks Routing options Route-based Remote network IP ranges 10.5.4.0/24 [internal ip address for vm instance 1]","title":"Create the vpn-2 gateway and tunnel2to1"},{"location":"27-gcp-elastic-cloud-vpn/#task-3-verify-vpn-connectivity","text":"Now we should be able to ping both internal and external IP addresses.","title":"Task 3: Verify VPN connectivity"},{"location":"27-gcp-elastic-cloud-vpn/#verify-server-1-to-server-2-connectivity","text":"In the Cloud Console, on the Navigation menu, click Compute Engine > VM instances. For server-1, click SSH to launch a terminal and connect. To test connectivity to server-2's internal IP address, run the following command: ping -c 3 <insert server-2's internal IP address here>","title":"Verify server-1 to server-2 connectivity"},{"location":"27-gcp-elastic-cloud-vpn/#remove-the-external-ip-addresses","text":"Now that you verified VPN connectivity, you can remove the instances' external IP addresses. For demonstration purposes, just do this for the server-1 instance. On the Navigation menu, click Compute Engine > VM instances. Select the server-1 instance and click Stop. Wait for the instance to stop. Instances need to be stopped before you can make changes to their network interfaces. Click on the name of the server-1 instance to open the VM instance details page. Click Edit. For Network interfaces, click the Edit icon (Edit). Change External IP to None. Click Done. Click Save and wait for the instance details to update. Click Start. Click Start again to confirm that you want to start the VM instance. Return to the VM instances page and wait for the instance to start. Notice that External IP is set to None for the server-1 instance.","title":"Remove the external IP addresses"},{"location":"28-http-load-balancer/","text":"Configuring an HTTP Load Balancer with Autoscaling Google Cloud HTTP(S) load balancing is implemented at the edge of Google's network in Google's points of presence (POP) around the world. User traffic directed to an HTTP(S) load balancer enters the POP closest to the user and is then load-balanced over Google's global network to the closest backend that has sufficient available capacity. Objectives Create a health check firewall rule Create a NAT configuration using Cloud Router Create a custom image for a web server Create an instance template based on the custom image Create two managed instance groups Configure an HTTP load balancer with IPv4 and IPv6 Stress test an HTTP load balancer Task 1. Configure a health check firewall rule Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections. Create the health check rule Create a firewall rule to allow health checks. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > Firewall. Notice the existing ICMP, internal, RDP, and SSH firewall rules. Each Google Cloud project starts with the default network and these firewall rules. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-health-checks Network default Targets Specified target tags Target tags allow-health-checks Source filter IP Ranges Source IP ranges 130.211.0.0/22 and 35.191.0.0/16 Protocols and ports Specified protocols and ports Make sure to include the /22 and /16 in the Source IP ranges. Select tcp and specify port 80. Click Create. Task 2: Create a NAT configuration using Cloud Router The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses. Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer. Create the Cloud Router instance In the Cloud Console, on the Navigation menu (Navigation menu), click Network services > Cloud NAT. Click Get started. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Gateway name nat-config Network default Region us-central1 Click Cloud Router, and select Create new router. For Name, type nat-router-us-central1. Click Create. In Create a NAT gateway, click Create. Task 3: Create a custom image for a web server Create a custom web server image for the backend of the load balancer. Create a VM In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > VM instances. Click Create Instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name webserver Region us-central1 Zone us-central1-a Series N1 Machine type f1-micro (1 vCPU) Under Boot disk, select change. click Show advanced option. Under deletion rule, select keep boot disk Click Select Click Networking, disks, security, management, sole-tenancy. Click Networking. - For Network tags, type allow-health-checks. - Under Network interfaces , click default. - Under External IP dropdown, select None. Click Done. Click Create. Customize the VM For webserver, click SSH to launch a terminal and connect. If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry. To install Apache2, run the following commands: sudo apt-get update sudo apt-get install -y apache2 To start the Apache server, run the following command: sudo service apache2 start To test the default page for the Apache2 server, run the following command: curl localhost The default page for the Apache2 server should be displayed. Set the Apache service to start at boot The software installation was successful. However, when a new VM is created using this image, the freshly booted VM does not have the Apache web server running. Use the following command to set the Apache service to automatically start on boot. Then test it to make sure it works. In the webserver SSH terminal, set the service to start on boot: sudo update-rc.d apache2 enable In the Cloud Console, select webserver, and then click Reset. In the confirmation dialog, click Reset. Reset will stop and reboot the machine. It keeps the same IPs and the same persistent boot disk, but memory is wiped. Therefore, if the Apache service is available after the reset, the update-rc command was successful. Check the server by connecting via SSH to the VM and entering the following command: sudo service apache2 status NOTE: If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry . The result should show Started The Apache HTTP Server. Prepare the disk to create a custom image Verify that the boot disk will not be deleted when the instance is deleted. On the VM instances page, click webserver to view the VM instance details. Under Boot disk, verify that When deleting instance is set to Keep disk. Return to the VM instances page, click webserver, and click Delete. In the confirmation dialog, click Delete. In the left pane, click Disks and verify that the webserver disk exists. Create the custom image In the left pane, click Images. Click Create image. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name mywebserver Source Disk Source disk webserver Click Create. You have created a custom image that multiple identical webservers can be started from. At this point, you could delete the webserver disk. The next step is to use that image to define an instance template that can be used in the managed instance groups. Task 4. Configure an instance template and create instance groups A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the HTTP load balancer. Configure the instance template An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > Instance templates. Click Create instance template. For Name, type mywebserver-template. For Series, select N1. For Machine type, select f1-micro (1 vCPU). For Boot disk, click Change. Click Custom images. For Image, Select mywebserver. Click Select. Click Management, security, disks, networking, sole tenancy. Click Networking. - For Network tags, type allow-health-checks. - Under External IP dropdown, select None. Click Create. Create the managed instance groups Create a managed instance group in us-central1 and one in europe-west1. On the Navigation menu, click Compute Engine > Instance groups. Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name us-central1-mig Location Multiple zones Region us-central1 Instance template mywebserver-template Under Autoscaling metrics, click on the edit pencil icon. Under Metric type, select HTTP load balancing utilization. Enter Target HTTP load balancing utilization to 80. Click Done. Set Cool down period to 60 seconds. Enter Minimum number of instances 1 and Maximum number of instances 2. Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. For Health check, select Create a health check. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name http-health-check Protocol TCP Port 80 Managed instance group health checks proactively signal to delete and recreate instances that become unhealthy. Click Save and continue. For Initial delay, type 60. This is how long the Instance Group waits after initializing the boot-up of a VM before it tries a health check. You don't want to wait 5 minutes for this during the lab, so you set it to 1 minute. Click Create. Click OK. NOTE: If a warning window will appear stating that There is no backend service attached to the instance group. Ignore this; you will configure the load balancer with a backend service in the next section of the lab. Repeat the same procedure for europe-west1-mig in europe-west1: Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name europe-west1-mig Location Multiple zones Region europe-west1 Instance template mywebserver-template Autoscaling metrics > Metric Type HTTP load balancing utilization Target HTTP load balancing utilization 80 Minimum number of instances 1 Maximum number of instances 2 Cool down period 60 For Health check, select http-health-check (TCP). For Initial delay, type 60. Click Create. Click OK in the dialog window. Task 5. Configure the HTTP load balancer Configure the HTTP load balancer to balance traffic between the two backends (us-central1-mig in us-central1 and europe-west1-mig in europe-west1) Start the configuration On the Navigation menu, click Network Services > Load balancing. Click Create load balancer. Under HTTP(S) Load Balancing, click Start configuration. Select From Internet to my VMs, then click Continue. For Name, type http-lb. Configure the backend Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata. Click Backend configuration. For Backend services & backend buckets, click Create a backend service. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name http-backend Backend type Instance group Instance group us-central1-mig Port numbers 80 Balancing mode Rate Maximum RPS 50 Capacity 100 This configuration means that the load balancer attempts to keep each instance of us-central1-mig at or below 50 requests per second (RPS). Click Done. Click Add backend. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Instance group europe-west1-mig Port numbers 80 Balancing mode Utilization Maximum backend utilization 80 Capacity 100 This configuration means that the load balancer attempts to keep each instance of europe-west1-mig at or below 80% CPU utilization. Click Done. For Health Check, select http-health-check (TCP). Check the Enable logging checkbox. Specify Sample rate as 1. Click Create. Configure the frontend The host and path rules determine how your traffic will be directed. For example, you could direct video traffic to one backend and direct static traffic to another backend. However, you are not configuring the host and path rules in this lab. Click Frontend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Protocol HTTP IP version IPv4 IP address Ephemeral Port 80 Click Done. Click Add Frontend IP and port. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Protocol HTTP IP version IPv6 IP address Ephemeral Port 80 Click Done. HTTP(S) load balancing supports both IPv4 and IPv6 addresses for client traffic. Client IPv6 requests are terminated at the global load balancing layer and then proxied over IPv4 to your backends. Review and create the HTTP load balancer Click Review and finalize. Review the Backend services and Frontend. Click Create. Wait for the load balancer to be created. Click on the name of the load balancer (http-lb). Note the IPv4 and IPv6 addresses of the load balancer for the next task. They will be referred to as [LB_IP_v4] and [LB_IP_v6] , respectively. The IPv6 address is the one in hexadecimal format. Task 6. Stress test the HTTP load balancer Now that you have created the HTTP load balancer for your backends, it is time to verify that traffic is forwarded to the backend service. Access the HTTP load balancer Open a new tab in your browser and navigate to http://[LB_IP_v4] . Make sure to replace [LB_IP_v4] with the IPv4 address of the load balancer. Accessing the HTTP load balancer might take a couple of minutes. In the meantime, you might get a 404 or 502 error. Keep trying until you see the page of one of the backends. Stress test the HTTP load balancer Create a new VM to simulate a load on the HTTP load balancer. Then determine whether traffic is balanced across both backends when the load is high. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > VM instances. Click Create instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name stress-test Region us-west1 Zone us-west1-c Series N1 Machine type f1-micro (1 vCPU) Because us-west1 is closer to us-central1 than to europe-west1, traffic should be forwarded only to us-central1-mig (unless the load is too high). For Boot Disk, click Change. Click Custom images. For Image, select mywebserver. Click Select. Click Create. Wait for the stress-test instance to be created. For stress-test, click SSH to launch a terminal and connect. To create an environment variable for your load balancer IP address, run the following command: export LB_IP=<Enter your [LB_IP_v4] here> Verify it with echo: echo $LB_IP To place a load on the load balancer, run the following command: ab -n 500000 -c 1000 http://$LB_IP/ In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services > Load balancing. Click Backends. Click http-backend. Monitor the Frontend Location (Total inbound traffic) between North America and the two backends for a couple of minutes. At first, traffic should just be directed to us-central1-mig, but as the RPS increases, traffic is also directed to europe-west1-mig. This demonstrates that by default traffic is forwarded to the closest backend, but if the load is very high, traffic can be distributed across the backends. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > Instance groups. Click on us-central1-mig to open the instance group page. Click Observability to monitor the number of instances and LB capacity. Repeat the same for the europe-west1-mig instance group. Depending on the load, you might see the backends scale to accommodate the load.","title":"Configuring an HTTP Load Balancer with Autoscaling"},{"location":"28-http-load-balancer/#configuring-an-http-load-balancer-with-autoscaling","text":"Google Cloud HTTP(S) load balancing is implemented at the edge of Google's network in Google's points of presence (POP) around the world. User traffic directed to an HTTP(S) load balancer enters the POP closest to the user and is then load-balanced over Google's global network to the closest backend that has sufficient available capacity.","title":"Configuring an HTTP Load Balancer with Autoscaling"},{"location":"28-http-load-balancer/#objectives","text":"Create a health check firewall rule Create a NAT configuration using Cloud Router Create a custom image for a web server Create an instance template based on the custom image Create two managed instance groups Configure an HTTP load balancer with IPv4 and IPv6 Stress test an HTTP load balancer","title":"Objectives"},{"location":"28-http-load-balancer/#task-1-configure-a-health-check-firewall-rule","text":"Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.","title":"Task 1. Configure a health check firewall rule"},{"location":"28-http-load-balancer/#create-the-health-check-rule","text":"Create a firewall rule to allow health checks. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > Firewall. Notice the existing ICMP, internal, RDP, and SSH firewall rules. Each Google Cloud project starts with the default network and these firewall rules. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-health-checks Network default Targets Specified target tags Target tags allow-health-checks Source filter IP Ranges Source IP ranges 130.211.0.0/22 and 35.191.0.0/16 Protocols and ports Specified protocols and ports Make sure to include the /22 and /16 in the Source IP ranges. Select tcp and specify port 80. Click Create.","title":"Create the health check rule"},{"location":"28-http-load-balancer/#task-2-create-a-nat-configuration-using-cloud-router","text":"The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses. Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer.","title":"Task 2: Create a NAT configuration using Cloud Router"},{"location":"28-http-load-balancer/#create-the-cloud-router-instance","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click Network services > Cloud NAT. Click Get started. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Gateway name nat-config Network default Region us-central1 Click Cloud Router, and select Create new router. For Name, type nat-router-us-central1. Click Create. In Create a NAT gateway, click Create.","title":"Create the Cloud Router instance"},{"location":"28-http-load-balancer/#task-3-create-a-custom-image-for-a-web-server","text":"Create a custom web server image for the backend of the load balancer.","title":"Task 3: Create a custom image for a web server"},{"location":"28-http-load-balancer/#create-a-vm","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > VM instances. Click Create Instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name webserver Region us-central1 Zone us-central1-a Series N1 Machine type f1-micro (1 vCPU) Under Boot disk, select change. click Show advanced option. Under deletion rule, select keep boot disk Click Select Click Networking, disks, security, management, sole-tenancy. Click Networking. - For Network tags, type allow-health-checks. - Under Network interfaces , click default. - Under External IP dropdown, select None. Click Done. Click Create.","title":"Create a VM"},{"location":"28-http-load-balancer/#customize-the-vm","text":"For webserver, click SSH to launch a terminal and connect. If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry. To install Apache2, run the following commands: sudo apt-get update sudo apt-get install -y apache2 To start the Apache server, run the following command: sudo service apache2 start To test the default page for the Apache2 server, run the following command: curl localhost The default page for the Apache2 server should be displayed.","title":"Customize the VM"},{"location":"28-http-load-balancer/#set-the-apache-service-to-start-at-boot","text":"The software installation was successful. However, when a new VM is created using this image, the freshly booted VM does not have the Apache web server running. Use the following command to set the Apache service to automatically start on boot. Then test it to make sure it works. In the webserver SSH terminal, set the service to start on boot: sudo update-rc.d apache2 enable In the Cloud Console, select webserver, and then click Reset. In the confirmation dialog, click Reset. Reset will stop and reboot the machine. It keeps the same IPs and the same persistent boot disk, but memory is wiped. Therefore, if the Apache service is available after the reset, the update-rc command was successful. Check the server by connecting via SSH to the VM and entering the following command: sudo service apache2 status NOTE: If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry . The result should show Started The Apache HTTP Server.","title":"Set the Apache service to start at boot"},{"location":"28-http-load-balancer/#prepare-the-disk-to-create-a-custom-image","text":"Verify that the boot disk will not be deleted when the instance is deleted. On the VM instances page, click webserver to view the VM instance details. Under Boot disk, verify that When deleting instance is set to Keep disk. Return to the VM instances page, click webserver, and click Delete. In the confirmation dialog, click Delete. In the left pane, click Disks and verify that the webserver disk exists.","title":"Prepare the disk to create a custom image"},{"location":"28-http-load-balancer/#create-the-custom-image","text":"In the left pane, click Images. Click Create image. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name mywebserver Source Disk Source disk webserver Click Create. You have created a custom image that multiple identical webservers can be started from. At this point, you could delete the webserver disk. The next step is to use that image to define an instance template that can be used in the managed instance groups.","title":"Create the custom image"},{"location":"28-http-load-balancer/#task-4-configure-an-instance-template-and-create-instance-groups","text":"A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the HTTP load balancer.","title":"Task 4. Configure an instance template and create instance groups"},{"location":"28-http-load-balancer/#configure-the-instance-template","text":"An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > Instance templates. Click Create instance template. For Name, type mywebserver-template. For Series, select N1. For Machine type, select f1-micro (1 vCPU). For Boot disk, click Change. Click Custom images. For Image, Select mywebserver. Click Select. Click Management, security, disks, networking, sole tenancy. Click Networking. - For Network tags, type allow-health-checks. - Under External IP dropdown, select None. Click Create.","title":"Configure the instance template"},{"location":"28-http-load-balancer/#create-the-managed-instance-groups","text":"Create a managed instance group in us-central1 and one in europe-west1. On the Navigation menu, click Compute Engine > Instance groups. Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name us-central1-mig Location Multiple zones Region us-central1 Instance template mywebserver-template Under Autoscaling metrics, click on the edit pencil icon. Under Metric type, select HTTP load balancing utilization. Enter Target HTTP load balancing utilization to 80. Click Done. Set Cool down period to 60 seconds. Enter Minimum number of instances 1 and Maximum number of instances 2. Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. For Health check, select Create a health check. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name http-health-check Protocol TCP Port 80 Managed instance group health checks proactively signal to delete and recreate instances that become unhealthy. Click Save and continue. For Initial delay, type 60. This is how long the Instance Group waits after initializing the boot-up of a VM before it tries a health check. You don't want to wait 5 minutes for this during the lab, so you set it to 1 minute. Click Create. Click OK. NOTE: If a warning window will appear stating that There is no backend service attached to the instance group. Ignore this; you will configure the load balancer with a backend service in the next section of the lab. Repeat the same procedure for europe-west1-mig in europe-west1: Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name europe-west1-mig Location Multiple zones Region europe-west1 Instance template mywebserver-template Autoscaling metrics > Metric Type HTTP load balancing utilization Target HTTP load balancing utilization 80 Minimum number of instances 1 Maximum number of instances 2 Cool down period 60 For Health check, select http-health-check (TCP). For Initial delay, type 60. Click Create. Click OK in the dialog window.","title":"Create the managed instance groups"},{"location":"28-http-load-balancer/#task-5-configure-the-http-load-balancer","text":"Configure the HTTP load balancer to balance traffic between the two backends (us-central1-mig in us-central1 and europe-west1-mig in europe-west1)","title":"Task 5. Configure the HTTP load balancer"},{"location":"28-http-load-balancer/#start-the-configuration","text":"On the Navigation menu, click Network Services > Load balancing. Click Create load balancer. Under HTTP(S) Load Balancing, click Start configuration. Select From Internet to my VMs, then click Continue. For Name, type http-lb.","title":"Start the configuration"},{"location":"28-http-load-balancer/#configure-the-backend","text":"Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata. Click Backend configuration. For Backend services & backend buckets, click Create a backend service. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name http-backend Backend type Instance group Instance group us-central1-mig Port numbers 80 Balancing mode Rate Maximum RPS 50 Capacity 100 This configuration means that the load balancer attempts to keep each instance of us-central1-mig at or below 50 requests per second (RPS). Click Done. Click Add backend. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Instance group europe-west1-mig Port numbers 80 Balancing mode Utilization Maximum backend utilization 80 Capacity 100 This configuration means that the load balancer attempts to keep each instance of europe-west1-mig at or below 80% CPU utilization. Click Done. For Health Check, select http-health-check (TCP). Check the Enable logging checkbox. Specify Sample rate as 1. Click Create.","title":"Configure the backend"},{"location":"28-http-load-balancer/#configure-the-frontend","text":"The host and path rules determine how your traffic will be directed. For example, you could direct video traffic to one backend and direct static traffic to another backend. However, you are not configuring the host and path rules in this lab. Click Frontend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Protocol HTTP IP version IPv4 IP address Ephemeral Port 80 Click Done. Click Add Frontend IP and port. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Protocol HTTP IP version IPv6 IP address Ephemeral Port 80 Click Done. HTTP(S) load balancing supports both IPv4 and IPv6 addresses for client traffic. Client IPv6 requests are terminated at the global load balancing layer and then proxied over IPv4 to your backends.","title":"Configure the frontend"},{"location":"28-http-load-balancer/#review-and-create-the-http-load-balancer","text":"Click Review and finalize. Review the Backend services and Frontend. Click Create. Wait for the load balancer to be created. Click on the name of the load balancer (http-lb). Note the IPv4 and IPv6 addresses of the load balancer for the next task. They will be referred to as [LB_IP_v4] and [LB_IP_v6] , respectively. The IPv6 address is the one in hexadecimal format.","title":"Review and create the HTTP load balancer"},{"location":"28-http-load-balancer/#task-6-stress-test-the-http-load-balancer","text":"Now that you have created the HTTP load balancer for your backends, it is time to verify that traffic is forwarded to the backend service.","title":"Task 6. Stress test the HTTP load balancer"},{"location":"28-http-load-balancer/#access-the-http-load-balancer","text":"Open a new tab in your browser and navigate to http://[LB_IP_v4] . Make sure to replace [LB_IP_v4] with the IPv4 address of the load balancer. Accessing the HTTP load balancer might take a couple of minutes. In the meantime, you might get a 404 or 502 error. Keep trying until you see the page of one of the backends.","title":"Access the HTTP load balancer"},{"location":"28-http-load-balancer/#stress-test-the-http-load-balancer","text":"Create a new VM to simulate a load on the HTTP load balancer. Then determine whether traffic is balanced across both backends when the load is high. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > VM instances. Click Create instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name stress-test Region us-west1 Zone us-west1-c Series N1 Machine type f1-micro (1 vCPU) Because us-west1 is closer to us-central1 than to europe-west1, traffic should be forwarded only to us-central1-mig (unless the load is too high). For Boot Disk, click Change. Click Custom images. For Image, select mywebserver. Click Select. Click Create. Wait for the stress-test instance to be created. For stress-test, click SSH to launch a terminal and connect. To create an environment variable for your load balancer IP address, run the following command: export LB_IP=<Enter your [LB_IP_v4] here> Verify it with echo: echo $LB_IP To place a load on the load balancer, run the following command: ab -n 500000 -c 1000 http://$LB_IP/ In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services > Load balancing. Click Backends. Click http-backend. Monitor the Frontend Location (Total inbound traffic) between North America and the two backends for a couple of minutes. At first, traffic should just be directed to us-central1-mig, but as the RPS increases, traffic is also directed to europe-west1-mig. This demonstrates that by default traffic is forwarded to the closest backend, but if the load is very high, traffic can be distributed across the backends. In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine > Instance groups. Click on us-central1-mig to open the instance group page. Click Observability to monitor the number of instances and LB capacity. Repeat the same for the europe-west1-mig instance group. Depending on the load, you might see the backends scale to accommodate the load.","title":"Stress test the HTTP load balancer"},{"location":"29-internal-load-balancer/","text":"Configuring an Internal Load Balancer Task 1. Configure internal traffic and health check firewall rules. Configure firewall rules to allow internal traffic connectivity from sources in the 10.10.0.0/16 range. This rule allows incoming traffic from any client located in the subnet. Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections. Explore the my-internal-app network The network my-internal-app with subnet-a and subnet-b and firewall rules for RDP, SSH, and ICMP traffic have been configured for you. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > VPC networks. Notice the my-internal-app network with its subnets: subnet-a and subnet-b. Each Google Cloud project starts with the default network. In addition, the my-internal-app network has been created for you as part of your network diagram. You will create the managed instance groups in subnet-a and subnet-b. Both subnets are in the us-central1 region because an internal load balancer is a regional service. The managed instance groups will be in different zones, making your service immune to zonal failures. Create the firewall rule to allow traffic from any sources in the 10.10.0.0/16 range Create a firewall rule to allow traffic in the 10.10.0.0/16 subnet. On the Navigation menu (Navigation menu), click VPC network > Firewall. Notice the app-allow-icmp and app-allow-ssh-rdp firewall rules. These firewall rules have been created for you. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-lb-access Network my-internal-app Targets Specified target tags Target tags backend-service Source filter IP ranges Source IP ranges 10.10.0.0/16 Protocols and ports Allow all Click create. Create the health check rule Create a firewall rule to allow health checks. On the Navigation menu (Navigation menu), click VPC network > Firewall. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-health-checks Network my-internal-app Targets Specified target tags Target tags backend-service Source filter IP Ranges Source IP ranges 130.211.0.0/22 and 35.191.0.0/16 Protocols and ports Specified protocols and ports Make sure to include the /22 and /16 in the Source IP ranges. For tcp, specify port 80. Click Create. Task 2: Create a NAT configuration using Cloud Router The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses. Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer. Create the Cloud Router instance In the Cloud Console, on the Navigation menu (Navigation menu), click Network services > Cloud NAT. Click Get started. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Gateway name nat-config VPC network my-internal-app Region us-central1 Click Cloud Router, and select Create new router. For Name, type nat-router-us-central1. Click Create. In Create a NAT gateway, click Create. Wait until the NAT Gateway Status changes to Running before moving onto the next task. Task 3. Configure instance templates and create instance groups A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the internal load balancer. Configure the instance templates An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. Create an instance template for both subnets of the my-internal-app network. On the Navigation menu (Navigation menu), click Compute Engine > Instance templates. Click Create instance template. For Name, type instance-template-1 Under Machine configuration, For Series, Select N1. Machine type f1-micro(1 vCPU). Click Management, security, disks, networking, sole tenancy. Click Management. Under Metadata, specify the following: Key Value startup-script-url gs://cloud-training/gcpnet/ilb/startup.sh The startup-script-url specifies a script that is executed when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region, and zone of the VM instance. You can explore this script here. #! /bin/bash apt-get update apt-get install -y apache2 php apt-get install -y wget cd /var/www/html rm index.html -f rm index.php -f wget https://storage.googleapis.com/cloud-training/gcpnet/ilb/index.php META_REGION_STRING=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/zone\" -H \"Metadata-Flavor: Google\") REGION=`echo \"$META_REGION_STRING\" | awk -F/ '{print $4}'` sed -i \"s|region-here|$REGION|\" index.php Click Networking. For Network interfaces, specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Network my-internal-app Subnet subnet-a Network tags backend-service External IP None The network tag backend-service ensures that the firewall rule to allow traffic from any sources in the 10.10.0.0/16 subnet and the Health Check firewall rule applies to these instances. Click Create. Wait for the instance template to be created. Create another instance template for subnet-b by copying instance-template-1: Select the instance-template-1 and click Copy. Click Management, security, disks, networking, sole tenancy. Click Networking. For Network interfaces, select subnet-b as the Subnet. Click Create. Create the managed instance groups Create a managed instance group in subnet-a (us-central1-a) and subnet-b (us-central1-b). On the Navigation menu (Navigation menu), click Compute Engine > Instance groups. Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name instance-group-1 Location Single zone Region us-central1 Zone us-central1-a Instance template instance-template-1 Autoscaling > metrics type (Click the pencil edit icon) CPU utilization Target CPU utilization 80, click Done. Cool-down period 45 Minimum number of instances 1 Maximum number of instances 5 Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. Just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. Click Create. Repeat the same procedure for instance-group-2 in us-central1-b: Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name instance-group-2 Location Single zone Region us-central1 Zone us-central1-b Instance template instance-template-2 Autoscaling > metric type (Click the pencil edit icon) CPU utilization Target CPU utilization 80, click Done. Cool-down period 45 Minimum number of instances 1 Maximum number of instances 5 Click Create. Verify the backends Verify that VM instances are being created in both subnets and create a utility VM to access the backends' HTTP sites. On the Navigation menu, click Compute Engine > VM instances. Notice two instances that start with instance-group-1 and instance-group-2. These instances are in separate zones, and their internal IP addresses are part of the subnet-a and subnet-b CIDR blocks. Click Create Instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name utility-vm Region us-central1 Zone us-central1-f Series N1 Machine type f1-micro (1 vCPU) Boot disk Debian GNU/Linux 10 (buster) Click Management, security, disks, networking, sole tenancy. Click Networking. For Network interfaces, click the pencil icon to edit. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Network my-internal-app Subnetwork subnet-a Primary internal IP Ephemeral (Custom) Custom ephemeral IP address 10.10.20.50 External IP None Click Done. Click Create. Note that the internal IP addresses for the backends are 10.10.20.2 and 10.10.30.2. If these IP addresses are different, replace them in the two curl commands below. For utility-vm, click SSH to launch a terminal and connect. If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry. To verify the welcome page for instance-group-1-xxxx, run the following command: curl 10.10.20.2 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-1-1zn8<h2>Server Location</h2>Region and Zone: us-central1-a To verify the welcome page for instance-group-2-xxxx, run the following command: curl 10.10.30.2 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-2-q5wp<h2>Server Location</h2>Region and Zone: us-central1-b This will be useful when verifying that the internal load balancer sends traffic to both backends. Close the SSH terminal to utility-vm Task 4. Configure the internal load balancer Configure the internal load balancer to balance traffic between the two backends (instance-group-1 in us-central1-a and instance-group-2 in us-central1-b) Start the configuration In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services > Load balancing. Click Create load balancer. Under TCP Load Balancing, click Start configuration. For Internet facing or internal only, select Only between my VMs. Choosing Only between my VMs makes this load balancer internal. This choice requires the backends to be in a single region (us-central1) and does not allow offloading TCP processing to the load balancer. Click Continue. For Name, type my-ilb. Configure the regional backend service The backend service monitors instance groups and prevents them from exceeding configured usage. Click Backend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Region us-central1 Network my-internal-app Instance group instance-group-1 (us-central1-a) Click Done. Click Add backend. For Instance group, select instance-group-2 (us-central1-b). Click Done. For Health Check, select Create a health check. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name my-ilb-health-check Protocol TCP Port 80 Check interval 10 sec Timeout 5 sec Healthy threshold 2 Unhealthy threshold 3 Health checks determine which instances can receive new connections. This HTTP health check polls instances every 10 seconds, waits up to 5 seconds for a response, and treats 2 successful or 3 failed attempts as healthy threshold or unhealthy threshold, respectively. Click Save and continue. Verify that there is a blue check mark next to Backend configuration in the Cloud Console. If there isn't, double-check that you have completed all the steps above. Configure the frontend The frontend forwards traffic to the backend. Click Frontend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Subnetwork subnet-b Internal IP > IP address Reserve static internal IP address Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name my-ilb-ip Static IP address Let me choose Custom IP address 10.10.30.5 Click Reserve. For Ports, type 80. Click Done. Review and create the internal load balancer Click Review and finalize. Review the Backend and Frontend. Click Create. Wait for the load balancer to be created before moving to the next task. Task 5. Test the internal load balancer Verify that the my-ilb IP address forwards traffic to instance-group-1 in us-central1-a and instance-group-2 in us-central1-b. Access the internal load balancer On the Navigation menu, click Compute Engine > VM instances. For utility-vm, click SSH to launch a terminal and connect. To verify that the internal load balancer forwards traffic, run the following command: curl 10.10.30.5 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-2-1zn8<h2>Server Location</h2>Region and Zone: us-central1-b As expected, traffic is forwarded from the internal load balancer (10.10.30.5) to the backend. Run the same command a couple of times","title":"Configuring an Internal Load Balancer"},{"location":"29-internal-load-balancer/#configuring-an-internal-load-balancer","text":"","title":"Configuring an Internal Load Balancer"},{"location":"29-internal-load-balancer/#task-1-configure-internal-traffic-and-health-check-firewall-rules","text":"Configure firewall rules to allow internal traffic connectivity from sources in the 10.10.0.0/16 range. This rule allows incoming traffic from any client located in the subnet. Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.","title":"Task 1. Configure internal traffic and health check firewall rules."},{"location":"29-internal-load-balancer/#explore-the-my-internal-app-network","text":"The network my-internal-app with subnet-a and subnet-b and firewall rules for RDP, SSH, and ICMP traffic have been configured for you. In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > VPC networks. Notice the my-internal-app network with its subnets: subnet-a and subnet-b. Each Google Cloud project starts with the default network. In addition, the my-internal-app network has been created for you as part of your network diagram. You will create the managed instance groups in subnet-a and subnet-b. Both subnets are in the us-central1 region because an internal load balancer is a regional service. The managed instance groups will be in different zones, making your service immune to zonal failures.","title":"Explore the my-internal-app network"},{"location":"29-internal-load-balancer/#create-the-firewall-rule-to-allow-traffic-from-any-sources-in-the-10100016-range","text":"Create a firewall rule to allow traffic in the 10.10.0.0/16 subnet. On the Navigation menu (Navigation menu), click VPC network > Firewall. Notice the app-allow-icmp and app-allow-ssh-rdp firewall rules. These firewall rules have been created for you. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-lb-access Network my-internal-app Targets Specified target tags Target tags backend-service Source filter IP ranges Source IP ranges 10.10.0.0/16 Protocols and ports Allow all Click create.","title":"Create the firewall rule to allow traffic from any sources in the 10.10.0.0/16 range"},{"location":"29-internal-load-balancer/#create-the-health-check-rule","text":"Create a firewall rule to allow health checks. On the Navigation menu (Navigation menu), click VPC network > Firewall. Click Create Firewall Rule. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name fw-allow-health-checks Network my-internal-app Targets Specified target tags Target tags backend-service Source filter IP Ranges Source IP ranges 130.211.0.0/22 and 35.191.0.0/16 Protocols and ports Specified protocols and ports Make sure to include the /22 and /16 in the Source IP ranges. For tcp, specify port 80. Click Create.","title":"Create the health check rule"},{"location":"29-internal-load-balancer/#task-2-create-a-nat-configuration-using-cloud-router","text":"The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses. Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer.","title":"Task 2: Create a NAT configuration using Cloud Router"},{"location":"29-internal-load-balancer/#create-the-cloud-router-instance","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click Network services > Cloud NAT. Click Get started. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Gateway name nat-config VPC network my-internal-app Region us-central1 Click Cloud Router, and select Create new router. For Name, type nat-router-us-central1. Click Create. In Create a NAT gateway, click Create. Wait until the NAT Gateway Status changes to Running before moving onto the next task.","title":"Create the Cloud Router instance"},{"location":"29-internal-load-balancer/#task-3-configure-instance-templates-and-create-instance-groups","text":"A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the internal load balancer.","title":"Task 3. Configure instance templates and create instance groups"},{"location":"29-internal-load-balancer/#configure-the-instance-templates","text":"An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. Create an instance template for both subnets of the my-internal-app network. On the Navigation menu (Navigation menu), click Compute Engine > Instance templates. Click Create instance template. For Name, type instance-template-1 Under Machine configuration, For Series, Select N1. Machine type f1-micro(1 vCPU). Click Management, security, disks, networking, sole tenancy. Click Management. Under Metadata, specify the following: Key Value startup-script-url gs://cloud-training/gcpnet/ilb/startup.sh The startup-script-url specifies a script that is executed when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region, and zone of the VM instance. You can explore this script here. #! /bin/bash apt-get update apt-get install -y apache2 php apt-get install -y wget cd /var/www/html rm index.html -f rm index.php -f wget https://storage.googleapis.com/cloud-training/gcpnet/ilb/index.php META_REGION_STRING=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/zone\" -H \"Metadata-Flavor: Google\") REGION=`echo \"$META_REGION_STRING\" | awk -F/ '{print $4}'` sed -i \"s|region-here|$REGION|\" index.php Click Networking. For Network interfaces, specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Network my-internal-app Subnet subnet-a Network tags backend-service External IP None The network tag backend-service ensures that the firewall rule to allow traffic from any sources in the 10.10.0.0/16 subnet and the Health Check firewall rule applies to these instances. Click Create. Wait for the instance template to be created. Create another instance template for subnet-b by copying instance-template-1: Select the instance-template-1 and click Copy. Click Management, security, disks, networking, sole tenancy. Click Networking. For Network interfaces, select subnet-b as the Subnet. Click Create.","title":"Configure the instance templates"},{"location":"29-internal-load-balancer/#create-the-managed-instance-groups","text":"Create a managed instance group in subnet-a (us-central1-a) and subnet-b (us-central1-b). On the Navigation menu (Navigation menu), click Compute Engine > Instance groups. Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name instance-group-1 Location Single zone Region us-central1 Zone us-central1-a Instance template instance-template-1 Autoscaling > metrics type (Click the pencil edit icon) CPU utilization Target CPU utilization 80, click Done. Cool-down period 45 Minimum number of instances 1 Maximum number of instances 5 Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. Just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. Click Create. Repeat the same procedure for instance-group-2 in us-central1-b: Click Create Instance group. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name instance-group-2 Location Single zone Region us-central1 Zone us-central1-b Instance template instance-template-2 Autoscaling > metric type (Click the pencil edit icon) CPU utilization Target CPU utilization 80, click Done. Cool-down period 45 Minimum number of instances 1 Maximum number of instances 5 Click Create.","title":"Create the managed instance groups"},{"location":"29-internal-load-balancer/#verify-the-backends","text":"Verify that VM instances are being created in both subnets and create a utility VM to access the backends' HTTP sites. On the Navigation menu, click Compute Engine > VM instances. Notice two instances that start with instance-group-1 and instance-group-2. These instances are in separate zones, and their internal IP addresses are part of the subnet-a and subnet-b CIDR blocks. Click Create Instance. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name utility-vm Region us-central1 Zone us-central1-f Series N1 Machine type f1-micro (1 vCPU) Boot disk Debian GNU/Linux 10 (buster) Click Management, security, disks, networking, sole tenancy. Click Networking. For Network interfaces, click the pencil icon to edit. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Network my-internal-app Subnetwork subnet-a Primary internal IP Ephemeral (Custom) Custom ephemeral IP address 10.10.20.50 External IP None Click Done. Click Create. Note that the internal IP addresses for the backends are 10.10.20.2 and 10.10.30.2. If these IP addresses are different, replace them in the two curl commands below. For utility-vm, click SSH to launch a terminal and connect. If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry. To verify the welcome page for instance-group-1-xxxx, run the following command: curl 10.10.20.2 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-1-1zn8<h2>Server Location</h2>Region and Zone: us-central1-a To verify the welcome page for instance-group-2-xxxx, run the following command: curl 10.10.30.2 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-2-q5wp<h2>Server Location</h2>Region and Zone: us-central1-b This will be useful when verifying that the internal load balancer sends traffic to both backends. Close the SSH terminal to utility-vm","title":"Verify the backends"},{"location":"29-internal-load-balancer/#task-4-configure-the-internal-load-balancer","text":"Configure the internal load balancer to balance traffic between the two backends (instance-group-1 in us-central1-a and instance-group-2 in us-central1-b)","title":"Task 4. Configure the internal load balancer"},{"location":"29-internal-load-balancer/#start-the-configuration","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services > Load balancing. Click Create load balancer. Under TCP Load Balancing, click Start configuration. For Internet facing or internal only, select Only between my VMs. Choosing Only between my VMs makes this load balancer internal. This choice requires the backends to be in a single region (us-central1) and does not allow offloading TCP processing to the load balancer. Click Continue. For Name, type my-ilb.","title":"Start the configuration"},{"location":"29-internal-load-balancer/#configure-the-regional-backend-service","text":"The backend service monitors instance groups and prevents them from exceeding configured usage. Click Backend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Region us-central1 Network my-internal-app Instance group instance-group-1 (us-central1-a) Click Done. Click Add backend. For Instance group, select instance-group-2 (us-central1-b). Click Done. For Health Check, select Create a health check. Specify the following, and leave the remaining settings as their defaults: Property Value (select option as specified) Name my-ilb-health-check Protocol TCP Port 80 Check interval 10 sec Timeout 5 sec Healthy threshold 2 Unhealthy threshold 3 Health checks determine which instances can receive new connections. This HTTP health check polls instances every 10 seconds, waits up to 5 seconds for a response, and treats 2 successful or 3 failed attempts as healthy threshold or unhealthy threshold, respectively. Click Save and continue. Verify that there is a blue check mark next to Backend configuration in the Cloud Console. If there isn't, double-check that you have completed all the steps above.","title":"Configure the regional backend service"},{"location":"29-internal-load-balancer/#configure-the-frontend","text":"The frontend forwards traffic to the backend. Click Frontend configuration. Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Subnetwork subnet-b Internal IP > IP address Reserve static internal IP address Specify the following, and leave the remaining settings as their defaults: Property Value (type value or select option as specified) Name my-ilb-ip Static IP address Let me choose Custom IP address 10.10.30.5 Click Reserve. For Ports, type 80. Click Done.","title":"Configure the frontend"},{"location":"29-internal-load-balancer/#review-and-create-the-internal-load-balancer","text":"Click Review and finalize. Review the Backend and Frontend. Click Create. Wait for the load balancer to be created before moving to the next task.","title":"Review and create the internal load balancer"},{"location":"29-internal-load-balancer/#task-5-test-the-internal-load-balancer","text":"Verify that the my-ilb IP address forwards traffic to instance-group-1 in us-central1-a and instance-group-2 in us-central1-b. Access the internal load balancer On the Navigation menu, click Compute Engine > VM instances. For utility-vm, click SSH to launch a terminal and connect. To verify that the internal load balancer forwards traffic, run the following command: curl 10.10.30.5 The output should look like this (do not copy; this is example output): <h1>Internal Load Balancing Lab</h1><h2>Client IP</h2>Your IP address : 10.10.20.50<h2>Hostname</h2>Server Hostname: instance-group-2-1zn8<h2>Server Location</h2>Region and Zone: us-central1-b As expected, traffic is forwarded from the internal load balancer (10.10.30.5) to the backend. Run the same command a couple of times","title":"Task 5. Test the internal load balancer"},{"location":"30-deployment-manager/","text":"Automating the Deployment of Infrastructure Using Deployment Manager Overview Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Cloud Platform services, such as Cloud Storage, Compute Engine, and Cloud SQL, configured to work together. In this lab, you create a Deployment Manager configuration with a template to automate the deployment of Google Cloud infrastructure. Specifically, you deploy one auto mode network with a firewall rule and two VM instances Objectives Create a configuration for an auto mode network Create a configuration for a firewall rule Create a template for VM instances Create and deploy a configuration Verify the deployment of a configuration Task 1. Configure the network A configuration describes all the resources you want for a single deployment. Verify that the Deployment Manager API is enabled In the Cloud Console, on the Navigation menu (Navigation menu), click APIs & services > Library. In the search bar, type Deployment Manager, and click the result for Cloud Deployment Manager V2 API. Start the Cloud Shell Editor To write the configuration and the template, you use the Cloud Shell Editor. In the Cloud Console, click Activate Cloud Shell (Cloud Shell). If prompted, click Continue. Run the following commands: mkdir dminfra cd dminfra In Cloud Shell, click Open Editor (Cloud Shell Editor). In the left pane of the code editor, expand the dminfra folder. Create the auto mode network configuration A configuration is a file written in YAML syntax that lists each of the resources you want to create and their respective resource properties. A configuration must contain a resources: section followed by the list of resources to create. Start the configuration with the mynetwork resource. To create a new file, click File > New File. Name the new file config.yaml, and then open it. Copy the following base code into config.yaml: resources: # Create the auto-mode network - name: [RESOURCE_NAME] type: [RESOURCE_TYPE] properties: #RESOURCE properties go here In config.yaml, replace [RESOURCE_NAME] with mynetwork To get a list of all available network resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep network The output should look like this (do not copy; this is example output): compute.beta.subnetwork compute.alpha.subnetwork compute.v1.subnetwork compute.beta.network compute.v1.network compute.alpha.network Locate compute.v1.network, which is the type needed to create a VPC network using Deployment Manager. By definition, an auto mode network automatically creates a subnetwork in each region. Task 2. Configure the firewall rule In order to allow ingress traffic instances in mynetwork, you need to create a firewall rule. Add the firewall rule to the configuration Add a firewall rule that allows HTTP, SSH, RDP, and ICMP traffic on mynetwork. To get a list of all available firewall rule resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep firewall The output should look like this (do not copy; this is example output): compute.v1.firewall compute.alpha.firewall compute.beta.firewall Locate compute.v1.firewall, which is the type needed to create a firewall rule using Deployment Manager. The final config.yaml imports: - path: instance-template.jinja resources: # Create the auto-mode network - name: mynetwork type: compute.v1.network properties: autoCreateSubnetworks: true # Create the firewall rule - name: mynetwork-allow-http-ssh-rdp-icmp type: compute.v1.firewall properties: network: $(ref.mynetwork.selfLink) sourceRanges: [\"0.0.0.0/0\"] allowed: - IPProtocol: TCP ports: [22, 80, 3389] - IPProtocol: ICMP # Create the mynet-us-vm instance - name: mynet-us-vm type: instance-template.jinja properties: zone: us-central1-a machineType: n1-standard-1 network: $(ref.mynetwork.selfLink) subnetwork: regions/us-central1/subnetworks/mynetwork # Create the mynet-eu-vm instance - name: mynet-eu-vm type: instance-template.jinja properties: zone: europe-west1-d machineType: n1-standard-1 network: $(ref.mynetwork.selfLink) subnetwork: regions/europe-west1/subnetworks/mynetwork Task 3. Create a template for VM instances Deployment Manager allows you to use Python or Jinja2 templates to parameterize your configuration. This allows you to reuse common deployment paradigms such as networks, firewall rules, and VM instances. Create the VM instance template Because you will be creating two similar VM instances, create a VM instance template. To create a new file, click File > New File. Name the new file instance-template.jinja, and then open it. Properties to define: machineType: Machine type and zone zone: Instance zone networkInterfaces: Network and subnetwork that VM is attached to accessConfigs: Required to give the instance a public IP address (required in this lab). To create instances with only an internal IP address, remove the accessConfigs section. disks: The boot disk, its name and image Most properties are defined as template properties, which you will provide values for from the top-level configuration (config.yaml). To get a list of all available instance resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep instance Locate compute.v1.instance, which is the type needed to create a VM instance using Deployment Manager. Final instance-template.jinja : resources: - name: {{ env[\"name\"] }} type: compute.v1.instance properties: machineType: zones/{{ properties[\"zone\"] }}/machineTypes/{{ properties[\"machineType\"] }} zone: {{ properties[\"zone\"] }} networkInterfaces: - network: {{ properties[\"network\"] }} subnetwork: {{ properties[\"subnetwork\"] }} accessConfigs: - name: External NAT type: ONE_TO_ONE_NAT disks: - deviceName: {{ env[\"name\"] }} type: PERSISTENT boot: true autoDelete: true initializeParams: sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/family/debian-9 Deploy the configuration gcloud deployment-manager deployments create dminfra --config=config.yaml --preview gcloud deployment-manager deployments update dminfra Or, directly deploy: gcloud deployment-manager deployments create dminfra --config=config.yaml Delete using: gcloud deployment-manager deployments delete dminfra Task 5. Verify your deployment Verify your network in the Cloud Console In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > VPC networks. View the mynetwork VPC network with a subnetwork in every region. On the Navigation menu, click VPC network > Firewall. Sort the firewall rules by Network. View the mynetwork-allow-http-ssh-rdp-icmp firewall rule for mynetwork. Verify your VM instances in the Cloud Console On the Navigation menu (Navigation menu), click Compute Engine > VM instances. View the mynet-us-vm and mynet-eu-vm instances. Note the internal IP address for mynet-eu-vm. For mynet-us-vm, click SSH to launch a terminal and connect. To test connectivity to mynet-eu-vm's internal IP address, run the following command in the SSH terminal (replacing mynet-eu-vm's internal IP address with the value noted earlier): ping -c 3 <Enter mynet-eu-vm's internal IP here> This should work because both VM instances are on the same network and the firewall rule allows ICMP traffic!","title":"Automating the Deployment of Infrastructure Using Deployment Manager"},{"location":"30-deployment-manager/#automating-the-deployment-of-infrastructure-using-deployment-manager","text":"","title":"Automating the Deployment of Infrastructure Using Deployment Manager"},{"location":"30-deployment-manager/#overview","text":"Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Cloud Platform services, such as Cloud Storage, Compute Engine, and Cloud SQL, configured to work together. In this lab, you create a Deployment Manager configuration with a template to automate the deployment of Google Cloud infrastructure. Specifically, you deploy one auto mode network with a firewall rule and two VM instances","title":"Overview"},{"location":"30-deployment-manager/#objectives","text":"Create a configuration for an auto mode network Create a configuration for a firewall rule Create a template for VM instances Create and deploy a configuration Verify the deployment of a configuration","title":"Objectives"},{"location":"30-deployment-manager/#task-1-configure-the-network","text":"A configuration describes all the resources you want for a single deployment.","title":"Task 1. Configure the network"},{"location":"30-deployment-manager/#verify-that-the-deployment-manager-api-is-enabled","text":"In the Cloud Console, on the Navigation menu (Navigation menu), click APIs & services > Library. In the search bar, type Deployment Manager, and click the result for Cloud Deployment Manager V2 API.","title":"Verify that the Deployment Manager API is enabled"},{"location":"30-deployment-manager/#start-the-cloud-shell-editor","text":"To write the configuration and the template, you use the Cloud Shell Editor. In the Cloud Console, click Activate Cloud Shell (Cloud Shell). If prompted, click Continue. Run the following commands: mkdir dminfra cd dminfra In Cloud Shell, click Open Editor (Cloud Shell Editor). In the left pane of the code editor, expand the dminfra folder.","title":"Start the Cloud Shell Editor"},{"location":"30-deployment-manager/#create-the-auto-mode-network-configuration","text":"A configuration is a file written in YAML syntax that lists each of the resources you want to create and their respective resource properties. A configuration must contain a resources: section followed by the list of resources to create. Start the configuration with the mynetwork resource. To create a new file, click File > New File. Name the new file config.yaml, and then open it. Copy the following base code into config.yaml: resources: # Create the auto-mode network - name: [RESOURCE_NAME] type: [RESOURCE_TYPE] properties: #RESOURCE properties go here In config.yaml, replace [RESOURCE_NAME] with mynetwork To get a list of all available network resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep network The output should look like this (do not copy; this is example output): compute.beta.subnetwork compute.alpha.subnetwork compute.v1.subnetwork compute.beta.network compute.v1.network compute.alpha.network Locate compute.v1.network, which is the type needed to create a VPC network using Deployment Manager. By definition, an auto mode network automatically creates a subnetwork in each region.","title":"Create the auto mode network configuration"},{"location":"30-deployment-manager/#task-2-configure-the-firewall-rule","text":"In order to allow ingress traffic instances in mynetwork, you need to create a firewall rule.","title":"Task 2. Configure the firewall rule"},{"location":"30-deployment-manager/#add-the-firewall-rule-to-the-configuration","text":"Add a firewall rule that allows HTTP, SSH, RDP, and ICMP traffic on mynetwork. To get a list of all available firewall rule resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep firewall The output should look like this (do not copy; this is example output): compute.v1.firewall compute.alpha.firewall compute.beta.firewall Locate compute.v1.firewall, which is the type needed to create a firewall rule using Deployment Manager. The final config.yaml imports: - path: instance-template.jinja resources: # Create the auto-mode network - name: mynetwork type: compute.v1.network properties: autoCreateSubnetworks: true # Create the firewall rule - name: mynetwork-allow-http-ssh-rdp-icmp type: compute.v1.firewall properties: network: $(ref.mynetwork.selfLink) sourceRanges: [\"0.0.0.0/0\"] allowed: - IPProtocol: TCP ports: [22, 80, 3389] - IPProtocol: ICMP # Create the mynet-us-vm instance - name: mynet-us-vm type: instance-template.jinja properties: zone: us-central1-a machineType: n1-standard-1 network: $(ref.mynetwork.selfLink) subnetwork: regions/us-central1/subnetworks/mynetwork # Create the mynet-eu-vm instance - name: mynet-eu-vm type: instance-template.jinja properties: zone: europe-west1-d machineType: n1-standard-1 network: $(ref.mynetwork.selfLink) subnetwork: regions/europe-west1/subnetworks/mynetwork","title":"Add the firewall rule to the configuration"},{"location":"30-deployment-manager/#task-3-create-a-template-for-vm-instances","text":"Deployment Manager allows you to use Python or Jinja2 templates to parameterize your configuration. This allows you to reuse common deployment paradigms such as networks, firewall rules, and VM instances.","title":"Task 3. Create a template for VM instances"},{"location":"30-deployment-manager/#create-the-vm-instance-template","text":"Because you will be creating two similar VM instances, create a VM instance template. To create a new file, click File > New File. Name the new file instance-template.jinja, and then open it. Properties to define: machineType: Machine type and zone zone: Instance zone networkInterfaces: Network and subnetwork that VM is attached to accessConfigs: Required to give the instance a public IP address (required in this lab). To create instances with only an internal IP address, remove the accessConfigs section. disks: The boot disk, its name and image Most properties are defined as template properties, which you will provide values for from the top-level configuration (config.yaml). To get a list of all available instance resource types in Google Cloud, run the following command in Cloud Shell: gcloud deployment-manager types list | grep instance Locate compute.v1.instance, which is the type needed to create a VM instance using Deployment Manager. Final instance-template.jinja : resources: - name: {{ env[\"name\"] }} type: compute.v1.instance properties: machineType: zones/{{ properties[\"zone\"] }}/machineTypes/{{ properties[\"machineType\"] }} zone: {{ properties[\"zone\"] }} networkInterfaces: - network: {{ properties[\"network\"] }} subnetwork: {{ properties[\"subnetwork\"] }} accessConfigs: - name: External NAT type: ONE_TO_ONE_NAT disks: - deviceName: {{ env[\"name\"] }} type: PERSISTENT boot: true autoDelete: true initializeParams: sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/family/debian-9","title":"Create the VM instance template"},{"location":"30-deployment-manager/#deploy-the-configuration","text":"gcloud deployment-manager deployments create dminfra --config=config.yaml --preview gcloud deployment-manager deployments update dminfra Or, directly deploy: gcloud deployment-manager deployments create dminfra --config=config.yaml Delete using: gcloud deployment-manager deployments delete dminfra","title":"Deploy the configuration"},{"location":"30-deployment-manager/#task-5-verify-your-deployment","text":"Verify your network in the Cloud Console In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network > VPC networks. View the mynetwork VPC network with a subnetwork in every region. On the Navigation menu, click VPC network > Firewall. Sort the firewall rules by Network. View the mynetwork-allow-http-ssh-rdp-icmp firewall rule for mynetwork. Verify your VM instances in the Cloud Console On the Navigation menu (Navigation menu), click Compute Engine > VM instances. View the mynet-us-vm and mynet-eu-vm instances. Note the internal IP address for mynet-eu-vm. For mynet-us-vm, click SSH to launch a terminal and connect. To test connectivity to mynet-eu-vm's internal IP address, run the following command in the SSH terminal (replacing mynet-eu-vm's internal IP address with the value noted earlier): ping -c 3 <Enter mynet-eu-vm's internal IP here> This should work because both VM instances are on the same network and the firewall rule allows ICMP traffic!","title":"Task 5. Verify your deployment"},{"location":"31-aws-session/","text":"Date: April 1. 2022 Module 1 Client server model: We are making a request and some server in the backend is fulfilling the request. Fundamental principles of cloud computing: 1. Services on demand 1. Avoid large upfront investment/capex 1. Provision computing resources as needed - elasticity 1. Pay for what we use Baked in vs Bolt on AWS has 200+ services ready to go AWS core services categories 1. compute 1. networking an content delivery 1. storage 1. database 1. security, identity and compliance 1. monitoring and analytics Module 2: compute Instance types : 1. General purpose : t series 1. Compute optimized : 1. Accelerated computing : 1. Storage optimized : high IOPS 1. Memory optimized ASG = auto scaling group = add or removing instance ELB = Elastic load balancer = distributes workload across several amazon EC2 instances, provide a single point of contact for the asg Messaging services Monolithic appliation vs micro services SQS = Simple Queue service SNS = Simple notification service = sending messgaes for the specific topics they have subscribed to AWS Lambda AWS Containers = ECS vs EKS AWS Fargate = serverless containers where orchestration is taken care of. higher level of abstraction than EKS. Module 3: Global infrastructure and reliability Regions consists of 2+ availability zones. Within the availability zones there will be multiple data centers. Cloudfront is CDN Module 4: Network Network access Control List ? = stateless security groups are stateful and deny all inbound traffic by default internet gateway is used to [?] Route 53 - DNS port number is 53 Missed this session :( Module 5: Database and storage Storage * block storage * ec2 instances has local storage called local storage. Ephemeral storage * ebs snapshots - stores delta in data for backup * s3 object storage with 6 classes * file storage - multiple clients can access same data with a shared file structure Databases * relational database service - a managed service for the data service * amazon aurora - * 6 copies in 3 availability zones * for high data durability * cut down on IOPS and reduce costs thereby * dynamoDB - serverless key-value database * aws database migration service * redshift - data warehouse for etl jobs * documentDB - for mongoDB * neptune - highly connected dataset * qldb - blockchain * managed blockchain for ledger database * elasticache - caching layer to improve performance * dynamoBD accelerator - improve performance Module 6: Security Shared responsibility model DDoS attack prevention : AWS Shield AWS Key Management Service - create and manage crypto keys Missed this section :( Module 7: Monitoring cloudtrail- log service, track user activity and api usage cloudwatch - monitor architecture, aws resources and applications, metrics from a dashbaord aws trusted advisor - reduce costs, improve performance, improve security. dashboard shows cost optimization, performance, security, fault tolerance, service limits - no problem green, recommendations yellow, actions red Module 8: Pricing and support Pricing: pay as you go, commitment for 1-3 years,volume discount. Module 9: Migration and Innovation AWS cloud adoption framework Six areas of focus , in 2 key areas 1. Business capabilities * Business * People * Governance 1. Technical capabilities * Platform * Security * Operations Six R's - migration strategies * Rehost - lift & shift * Replatform * Refactor / rearchitect * Repurchase * Retain * Retire AWS Snow Family * Copy data using a device Five pillars of well architected framework Module 10: Preparing for the certified cloud practitioner 4 domains : 1. cloud concepts 2. security and compliance 3. technology 4. billing and pricing","title":"31 aws session"},{"location":"31-aws-session/#date-april-1-2022","text":"","title":"Date: April 1. 2022"},{"location":"31-aws-session/#module-1","text":"Client server model: We are making a request and some server in the backend is fulfilling the request. Fundamental principles of cloud computing: 1. Services on demand 1. Avoid large upfront investment/capex 1. Provision computing resources as needed - elasticity 1. Pay for what we use Baked in vs Bolt on AWS has 200+ services ready to go AWS core services categories 1. compute 1. networking an content delivery 1. storage 1. database 1. security, identity and compliance 1. monitoring and analytics","title":"Module 1"},{"location":"31-aws-session/#module-2-compute","text":"Instance types : 1. General purpose : t series 1. Compute optimized : 1. Accelerated computing : 1. Storage optimized : high IOPS 1. Memory optimized ASG = auto scaling group = add or removing instance ELB = Elastic load balancer = distributes workload across several amazon EC2 instances, provide a single point of contact for the asg Messaging services Monolithic appliation vs micro services SQS = Simple Queue service SNS = Simple notification service = sending messgaes for the specific topics they have subscribed to AWS Lambda AWS Containers = ECS vs EKS AWS Fargate = serverless containers where orchestration is taken care of. higher level of abstraction than EKS.","title":"Module 2: compute"},{"location":"31-aws-session/#module-3-global-infrastructure-and-reliability","text":"Regions consists of 2+ availability zones. Within the availability zones there will be multiple data centers. Cloudfront is CDN","title":"Module 3: Global infrastructure and reliability"},{"location":"31-aws-session/#module-4-network","text":"Network access Control List ? = stateless security groups are stateful and deny all inbound traffic by default internet gateway is used to [?] Route 53 - DNS port number is 53 Missed this session :(","title":"Module 4: Network"},{"location":"31-aws-session/#module-5-database-and-storage","text":"Storage * block storage * ec2 instances has local storage called local storage. Ephemeral storage * ebs snapshots - stores delta in data for backup * s3 object storage with 6 classes * file storage - multiple clients can access same data with a shared file structure Databases * relational database service - a managed service for the data service * amazon aurora - * 6 copies in 3 availability zones * for high data durability * cut down on IOPS and reduce costs thereby * dynamoDB - serverless key-value database * aws database migration service * redshift - data warehouse for etl jobs * documentDB - for mongoDB * neptune - highly connected dataset * qldb - blockchain * managed blockchain for ledger database * elasticache - caching layer to improve performance * dynamoBD accelerator - improve performance","title":"Module 5: Database and storage"},{"location":"31-aws-session/#module-6-security","text":"Shared responsibility model DDoS attack prevention : AWS Shield AWS Key Management Service - create and manage crypto keys Missed this section :(","title":"Module 6: Security"},{"location":"31-aws-session/#module-7-monitoring","text":"cloudtrail- log service, track user activity and api usage cloudwatch - monitor architecture, aws resources and applications, metrics from a dashbaord aws trusted advisor - reduce costs, improve performance, improve security. dashboard shows cost optimization, performance, security, fault tolerance, service limits - no problem green, recommendations yellow, actions red","title":"Module 7: Monitoring"},{"location":"31-aws-session/#module-8-pricing-and-support","text":"Pricing: pay as you go, commitment for 1-3 years,volume discount.","title":"Module 8: Pricing and support"},{"location":"31-aws-session/#module-9-migration-and-innovation","text":"AWS cloud adoption framework Six areas of focus , in 2 key areas 1. Business capabilities * Business * People * Governance 1. Technical capabilities * Platform * Security * Operations Six R's - migration strategies * Rehost - lift & shift * Replatform * Refactor / rearchitect * Repurchase * Retain * Retire AWS Snow Family * Copy data using a device Five pillars of well architected framework","title":"Module 9: Migration and Innovation"},{"location":"31-aws-session/#module-10-preparing-for-the-certified-cloud-practitioner","text":"4 domains : 1. cloud concepts 2. security and compliance 3. technology 4. billing and pricing","title":"Module 10: Preparing for the certified cloud practitioner"},{"location":"32-gcp-cloud-build/","text":"Enable APIs Cloud Build API Container Registry API Building Containers with DockerFile and Cloud Build You can write build configuration files to provide instructions to Cloud Build as to which tasks to perform when building a container. These build files can fetch dependencies, run unit tests, analyses and more. In this task, you'll create a DockerFile and use it as a build configuration script with Cloud Build. You will also create a simple shell script (quickstart.sh) which will represent an application inside the container. On the Google Cloud Console title bar, click Activate Cloud Shell. When prompted, click Continue. Cloud Shell opens at the bottom of the Google Cloud Console window. Create an empty quickstart.sh Add: #!/bin/sh echo \"Hello, world! The time is $(date).\" Create an Dockerfile FROM alpine COPY quickstart.sh / CMD [\"/quickstart.sh\"] In Cloud Shell, run the following command to make the quickstart.sh script executable. chmod +x quickstart.sh In Cloud Shell, run the following command to build the Docker container image in Cloud Build. gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/quickstart-image . Docker image is built and pushed to Container Registry. Building Containers with a build configuration file and Cloud Build Cloud Build also supports custom build configuration files. In this task you will incorporate an existing Docker container using a custom YAML-formatted build file with Cloud Build. In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Cloud_Build/a A sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory as well as copies of the Dockerfile and the quickstart.sh script you created in the first task. View the could build configuration file. cat cloudbuild.yaml steps: - name: 'gcr.io/cloud-builders/docker' args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ] images: - 'gcr.io/$PROJECT_ID/quickstart-image' This file instructs Cloud Build to use Docker to build an image using the Dockerfile specification in the current local directory, tag it with gcr.io/$PROJECT_ID/quickstart-image ( $PROJECT_ID is a substitution variable automatically populated by Cloud Build with the project ID of the associated project) and then push that image to Container Registry. In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file: gcloud builds submit --config cloudbuild.yaml . The build output to Cloud Shell should be the same as before. When the build completes, a new version of the same image is pushed to Container Registry. Building and Testing Containers with a build configuration file and Cloud Build The true power of custom build configuration files is their ability to perform other actions, in parallel or in sequence, in addition to simply building containers: running tests on your newly built containers, pushing them to various destinations, and even deploying them to Kubernetes Engine. In this lab, we will see a simple example: a build configuration file that tests the container it built and reports the result to its calling environment. In Cloud Shell, change to the directory that contains the sample files for this lab. cd ~/ak8s/Cloud_Build/b Three files are necessary for this step. quickstart.sh #!/bin/sh if [ -z \"$1\" ] then echo \"Hello, world! The time is $(date).\" exit 0 else exit 1 fi cloudbuild.yaml steps: - name: 'gcr.io/cloud-builders/docker' args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ] - name: 'gcr.io/$PROJECT_ID/quickstart-image' args: ['fail'] images: - 'gcr.io/$PROJECT_ID/quickstart-image Dockerfile FROM alpine COPY quickstart.sh / CMD [\"/quickstart.sh\"] In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file: gcloud builds submit --config cloudbuild.yaml . Output from the command that ends with FAILURE Confirm that your command shell knows that the build failed echo $?","title":"32 gcp cloud build"},{"location":"32-gcp-cloud-build/#enable-apis","text":"Cloud Build API Container Registry API","title":"Enable APIs"},{"location":"32-gcp-cloud-build/#building-containers-with-dockerfile-and-cloud-build","text":"You can write build configuration files to provide instructions to Cloud Build as to which tasks to perform when building a container. These build files can fetch dependencies, run unit tests, analyses and more. In this task, you'll create a DockerFile and use it as a build configuration script with Cloud Build. You will also create a simple shell script (quickstart.sh) which will represent an application inside the container. On the Google Cloud Console title bar, click Activate Cloud Shell. When prompted, click Continue. Cloud Shell opens at the bottom of the Google Cloud Console window. Create an empty quickstart.sh Add: #!/bin/sh echo \"Hello, world! The time is $(date).\" Create an Dockerfile FROM alpine COPY quickstart.sh / CMD [\"/quickstart.sh\"] In Cloud Shell, run the following command to make the quickstart.sh script executable. chmod +x quickstart.sh In Cloud Shell, run the following command to build the Docker container image in Cloud Build. gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/quickstart-image . Docker image is built and pushed to Container Registry.","title":"Building Containers with DockerFile and Cloud Build"},{"location":"32-gcp-cloud-build/#building-containers-with-a-build-configuration-file-and-cloud-build","text":"Cloud Build also supports custom build configuration files. In this task you will incorporate an existing Docker container using a custom YAML-formatted build file with Cloud Build. In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Cloud_Build/a A sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory as well as copies of the Dockerfile and the quickstart.sh script you created in the first task. View the could build configuration file. cat cloudbuild.yaml steps: - name: 'gcr.io/cloud-builders/docker' args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ] images: - 'gcr.io/$PROJECT_ID/quickstart-image' This file instructs Cloud Build to use Docker to build an image using the Dockerfile specification in the current local directory, tag it with gcr.io/$PROJECT_ID/quickstart-image ( $PROJECT_ID is a substitution variable automatically populated by Cloud Build with the project ID of the associated project) and then push that image to Container Registry. In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file: gcloud builds submit --config cloudbuild.yaml . The build output to Cloud Shell should be the same as before. When the build completes, a new version of the same image is pushed to Container Registry.","title":"Building Containers with a build configuration file and Cloud Build"},{"location":"32-gcp-cloud-build/#building-and-testing-containers-with-a-build-configuration-file-and-cloud-build","text":"The true power of custom build configuration files is their ability to perform other actions, in parallel or in sequence, in addition to simply building containers: running tests on your newly built containers, pushing them to various destinations, and even deploying them to Kubernetes Engine. In this lab, we will see a simple example: a build configuration file that tests the container it built and reports the result to its calling environment. In Cloud Shell, change to the directory that contains the sample files for this lab. cd ~/ak8s/Cloud_Build/b Three files are necessary for this step. quickstart.sh #!/bin/sh if [ -z \"$1\" ] then echo \"Hello, world! The time is $(date).\" exit 0 else exit 1 fi cloudbuild.yaml steps: - name: 'gcr.io/cloud-builders/docker' args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ] - name: 'gcr.io/$PROJECT_ID/quickstart-image' args: ['fail'] images: - 'gcr.io/$PROJECT_ID/quickstart-image Dockerfile FROM alpine COPY quickstart.sh / CMD [\"/quickstart.sh\"] In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file: gcloud builds submit --config cloudbuild.yaml . Output from the command that ends with FAILURE Confirm that your command shell knows that the build failed echo $?","title":"Building and Testing Containers with a build configuration file and Cloud Build"},{"location":"33-gcp-deploying-kubernetes/","text":"Objectives Use the Google Cloud Console to build and manipulate GKE clusters Use the Google Cloud Console to deploy a Pod Use the Google Cloud Console to examine the cluster and Pods Task 1. Deploy GKE clusters Using cloud console: On navigation menu, click on Kubernetes Engine > Clusters Click Create to begin creating a GKE cluster. In the GKE Standard cluster option select configure in the next screen. Examine the console UI and the controls to change the cluster name, the cluster location, Kubernetes version, the number of nodes, and the node resources such as the machine type in the default node pool. Clusters can be created across a region or in a single zone. A single zone is the default. When you deploy across a region the nodes are deployed to three separate zones and the total number of nodes deployed will be three times higher. Change the cluster name to standard-cluster-1 and zone to us-central1-a. Leave all the values at their defaults and click Create. The cluster begins provisioning. Note: You need to wait a few minutes for the cluster deployment to complete. Click the cluster name standard-cluster-1 to view the cluster details You can scroll down the page to view more details. Click the Storage and Nodes tabs under the cluster name (standard-cluster-1) at the top to view more of the cluster details. Task 2. Modify GKE clusters It is easy to modify many of the parameters of existing clusters using either the Google Cloud Console or Cloud Shell. In this task, you use the Google Cloud Console to modify the size of GKE clusters. In the Google Cloud Console, click NODES at the top of the details page for standard-cluster-1. In Node Pools section, click default-pool. In the Google Cloud Console, click RESIZE at the top of the Node Pool Details page. Change the number of nodes from 3 to 4 and click RESIZE. Task 3. Deploy a sample workload In this task, using the Google Cloud console you will deploy a Pod running the nginx web server as a sample workload. In the Google Cloud Console, on the Navigation menu( Navigation menu), click Kubernetes Engine > Workloads. Click Deploy to show the Create a deployment wizard. Click Continue to accept the default container image, nginx:latest, which deploys 3 Pods each with a single container running the latest version of nginx. Scroll to the bottom of the window and click the Deploy button leaving the Configuration details at the defaults. When the deployment completes your screen will refresh to show the details of your new nginx deployment. Task 4. View details about workloads in the Google Cloud Console In this task, you view details of your GKE workloads directly in the Google Cloud Console. In the Google Cloud Console, on the Navigation menu (Navigation menu), click Kubernetes Engine > Workloads. In the Google Cloud Console, on the Kubernetes Engine > Workloads page, click nginx-1. You may see Pods (3/3) as the default deployment will start with three pods but will scale back to 1 after a few minutes. You can continue with the lab. This displays the overview information for the workload showing details like resource utilization charts, links to logs, and details of the Pods associated with this workload. In the Google Cloud Console, click the Details tab for the nginx-1 workload. The Details tab shows more details about the workload including the Pod specification, number and status of Pod replicas and details about the horizontal Pod autoscaler. Click the Revision History tab. This displays a list of the revisions that have been made to this workload. Click the Events tab. This tab lists events associated with this workload. And then the YAML tab. This tab provides the complete YAML file that defines this components and full configuration of this sample workload. Still in the Google Cloud Console's Details tab for the nginx-1 workload, click the Overview tab, scroll down to the Managed Pods section and click the name of one of the Pods to view the details page for that Pod. Note: The default deployment will start with three pods but will scale back to 1 after a few minutes so you may need to refresh the Overview page to make sure you have a valid Pod to inspect. The Pod Details page provides information on the Pod configuration and resource utilization and the node where the Pod is running. In the Pod details page, you can click the Events and Logs tabs to view event details and links to container logs in Cloud Operations. Click the YAML tab to view the detailed YAML file for the Pod configuration.","title":"33 gcp deploying kubernetes"},{"location":"33-gcp-deploying-kubernetes/#objectives","text":"Use the Google Cloud Console to build and manipulate GKE clusters Use the Google Cloud Console to deploy a Pod Use the Google Cloud Console to examine the cluster and Pods","title":"Objectives"},{"location":"33-gcp-deploying-kubernetes/#task-1-deploy-gke-clusters","text":"Using cloud console: On navigation menu, click on Kubernetes Engine > Clusters Click Create to begin creating a GKE cluster. In the GKE Standard cluster option select configure in the next screen. Examine the console UI and the controls to change the cluster name, the cluster location, Kubernetes version, the number of nodes, and the node resources such as the machine type in the default node pool. Clusters can be created across a region or in a single zone. A single zone is the default. When you deploy across a region the nodes are deployed to three separate zones and the total number of nodes deployed will be three times higher. Change the cluster name to standard-cluster-1 and zone to us-central1-a. Leave all the values at their defaults and click Create. The cluster begins provisioning. Note: You need to wait a few minutes for the cluster deployment to complete. Click the cluster name standard-cluster-1 to view the cluster details You can scroll down the page to view more details. Click the Storage and Nodes tabs under the cluster name (standard-cluster-1) at the top to view more of the cluster details.","title":"Task 1. Deploy GKE clusters"},{"location":"33-gcp-deploying-kubernetes/#task-2-modify-gke-clusters","text":"It is easy to modify many of the parameters of existing clusters using either the Google Cloud Console or Cloud Shell. In this task, you use the Google Cloud Console to modify the size of GKE clusters. In the Google Cloud Console, click NODES at the top of the details page for standard-cluster-1. In Node Pools section, click default-pool. In the Google Cloud Console, click RESIZE at the top of the Node Pool Details page. Change the number of nodes from 3 to 4 and click RESIZE.","title":"Task 2. Modify GKE clusters"},{"location":"33-gcp-deploying-kubernetes/#task-3-deploy-a-sample-workload","text":"In this task, using the Google Cloud console you will deploy a Pod running the nginx web server as a sample workload. In the Google Cloud Console, on the Navigation menu( Navigation menu), click Kubernetes Engine > Workloads. Click Deploy to show the Create a deployment wizard. Click Continue to accept the default container image, nginx:latest, which deploys 3 Pods each with a single container running the latest version of nginx. Scroll to the bottom of the window and click the Deploy button leaving the Configuration details at the defaults. When the deployment completes your screen will refresh to show the details of your new nginx deployment.","title":"Task 3. Deploy a sample workload"},{"location":"33-gcp-deploying-kubernetes/#task-4-view-details-about-workloads-in-the-google-cloud-console","text":"In this task, you view details of your GKE workloads directly in the Google Cloud Console. In the Google Cloud Console, on the Navigation menu (Navigation menu), click Kubernetes Engine > Workloads. In the Google Cloud Console, on the Kubernetes Engine > Workloads page, click nginx-1. You may see Pods (3/3) as the default deployment will start with three pods but will scale back to 1 after a few minutes. You can continue with the lab. This displays the overview information for the workload showing details like resource utilization charts, links to logs, and details of the Pods associated with this workload. In the Google Cloud Console, click the Details tab for the nginx-1 workload. The Details tab shows more details about the workload including the Pod specification, number and status of Pod replicas and details about the horizontal Pod autoscaler. Click the Revision History tab. This displays a list of the revisions that have been made to this workload. Click the Events tab. This tab lists events associated with this workload. And then the YAML tab. This tab provides the complete YAML file that defines this components and full configuration of this sample workload. Still in the Google Cloud Console's Details tab for the nginx-1 workload, click the Overview tab, scroll down to the Managed Pods section and click the name of one of the Pods to view the details page for that Pod. Note: The default deployment will start with three pods but will scale back to 1 after a few minutes so you may need to refresh the Overview page to make sure you have a valid Pod to inspect. The Pod Details page provides information on the Pod configuration and resource utilization and the node where the Pod is running. In the Pod details page, you can click the Events and Logs tabs to view event details and links to container logs in Cloud Operations. Click the YAML tab to view the detailed YAML file for the Pod configuration.","title":"Task 4. View details about workloads in the Google Cloud Console"},{"location":"34-gcp-creating-kubernetes-engine-deployments/","text":"Creating Google Kubernetes Engine Deployments Objectives Create deployment manifests, deploy to cluster, and verify Pod rescheduling as nodes are disabled Trigger manual scaling up and down of Pods in deployments Trigger deployment rollout (rolling update to new version) and rollbacks Perform a Canary deployment Activate cloud shell Google Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Google Cloud Shell provides command-line access to your GCP resources. This can be activated from the Cloud shell button on the GCP top right toolbar. gcloud is the command-line tool for Google Cloud Platform. It comes pre-installed on Cloud Shell and supports tab-completion. list the active account name with this command: gcloud auth list list the project ID with this command: gcloud config list project Task 1. Create deployment manifests and deploy to the cluster In this task, you create a deployment manifest for a Pod inside the cluster. Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure kubectl tab completion in Cloud Shell. source <(kubectl completion bash) In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Deployments/ Create a deployment manifest You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy your manifest, execute the following command: kubectl apply -f ./nginx-deployment.yaml To view a list of deployments, execute the following command: kubectl get deployments The output should look like this example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 3 0 3s Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 42s Task 2. Manually scale up and down the number of Pods in deployments Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero. In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell. Scale Pods up and down in the console Switch to the Google Cloud Console tab. On the Navigation menu, click Kubernetes Engine > Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS > Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh. Scale Pods up and down in the shell Switch back to the Cloud Shell browser tab. In the Cloud Shell, to view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 3m To scale the Pod back up to three replicas, execute the following command: kubectl scale --replicas=3 deployment nginx-deployment To view a list of Pods in the deployments, execute the following command: kubectl get deployments Task 3. Trigger a deployment rollout and a deployment rollback A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout. In this task, you trigger deployment rollout, and then you trigger deployment rollback. Trigger a deployment rollout To update the version of nginx in the deployment, execute the following command: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record This updates the container image in your Deployment to nginx v1.9.1. The record flag is being deprecated. To view the rollout status, execute the following command: kubectl rollout status deployment.v1.apps/nginx-deployment The output should look like the example. Output (do not copy) Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 1 old replicas pending termination... Waiting for rollout to finish: 1 old replicas pending termination... deployment \"nginx-deployment\" successfully rolled out To verify the change, get the list of deployments. kubectl get deployments The output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 6m View the rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true Trigger a deployment rollback To roll back an object's rollout, you can use the kubectl rollout undo command. To roll back to the previous version of the nginx deployment, execute the following command: kubectl rollout undo deployments nginx-deployment View the updated rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 <none> View the details of the latest deployment revision kubectl rollout history deployment/nginx-deployment --revision=3 The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9. Output (do not copy) deployments \"nginx-deployment\" with revision #3 Pod Template: Labels: app=nginx pod-template-hash=3123191453 Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Define service types in the manifest A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx. apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80 In the Cloud Shell, to deploy your manifest, execute the following command: kubectl apply -f ./service-nginx.yaml This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service. Verify the LoadBalancer creation To view the details of the nginx service, execute the following command: kubectl get service nginx The output should look like the example. Output (do not copy) NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE nginx 10.X.X.X X.X.X.X 60000/TCP run=nginx 1m When the external IP appears, open http://[EXTERNAL_IP]:60000/ in a new browser tab to see the server being served through network load balancing. It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated. Task 5. Perform a canary deployment A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-canary labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx track: canary Version: 1.9.1 spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment. Create the canary deployment based on the configuration file. kubectl apply -f nginx-canary.yaml When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present. kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page. Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas. kubectl scale --replicas=0 deployment nginx-deployment Verify that the only running replica is now the Canary deployment: kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment. Session affinity The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections. For example: apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer sessionAffinity: ClientIP selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80","title":"Creating Google Kubernetes Engine Deployments"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#creating-google-kubernetes-engine-deployments","text":"","title":"Creating Google Kubernetes Engine Deployments"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#objectives","text":"Create deployment manifests, deploy to cluster, and verify Pod rescheduling as nodes are disabled Trigger manual scaling up and down of Pods in deployments Trigger deployment rollout (rolling update to new version) and rollbacks Perform a Canary deployment","title":"Objectives"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#activate-cloud-shell","text":"Google Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Google Cloud Shell provides command-line access to your GCP resources. This can be activated from the Cloud shell button on the GCP top right toolbar. gcloud is the command-line tool for Google Cloud Platform. It comes pre-installed on Cloud Shell and supports tab-completion. list the active account name with this command: gcloud auth list list the project ID with this command: gcloud config list project","title":"Activate cloud shell"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-1-create-deployment-manifests-and-deploy-to-the-cluster","text":"In this task, you create a deployment manifest for a Pod inside the cluster.","title":"Task 1. Create deployment manifests and deploy to the cluster"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#connect-to-the-lab-gke-cluster","text":"In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure kubectl tab completion in Cloud Shell. source <(kubectl completion bash) In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Deployments/","title":"Connect to the lab GKE cluster"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#create-a-deployment-manifest","text":"You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy your manifest, execute the following command: kubectl apply -f ./nginx-deployment.yaml To view a list of deployments, execute the following command: kubectl get deployments The output should look like this example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 3 0 3s Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 42s","title":"Create a deployment manifest"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-2-manually-scale-up-and-down-the-number-of-pods-in-deployments","text":"Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero. In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell.","title":"Task 2. Manually scale up and down the number of Pods in deployments"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#scale-pods-up-and-down-in-the-console","text":"Switch to the Google Cloud Console tab. On the Navigation menu, click Kubernetes Engine > Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS > Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh.","title":"Scale Pods up and down in the console"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#scale-pods-up-and-down-in-the-shell","text":"Switch back to the Cloud Shell browser tab. In the Cloud Shell, to view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 3m To scale the Pod back up to three replicas, execute the following command: kubectl scale --replicas=3 deployment nginx-deployment To view a list of Pods in the deployments, execute the following command: kubectl get deployments","title":"Scale Pods up and down in the shell"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-3-trigger-a-deployment-rollout-and-a-deployment-rollback","text":"A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout. In this task, you trigger deployment rollout, and then you trigger deployment rollback.","title":"Task 3. Trigger a deployment rollout and a deployment rollback"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#trigger-a-deployment-rollout","text":"To update the version of nginx in the deployment, execute the following command: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record This updates the container image in your Deployment to nginx v1.9.1. The record flag is being deprecated. To view the rollout status, execute the following command: kubectl rollout status deployment.v1.apps/nginx-deployment The output should look like the example. Output (do not copy) Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 1 old replicas pending termination... Waiting for rollout to finish: 1 old replicas pending termination... deployment \"nginx-deployment\" successfully rolled out To verify the change, get the list of deployments. kubectl get deployments The output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 6m","title":"Trigger a deployment rollout"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#view-the-rollout-history-of-the-deployment","text":"kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true","title":"View the rollout history of the deployment."},{"location":"34-gcp-creating-kubernetes-engine-deployments/#trigger-a-deployment-rollback","text":"To roll back an object's rollout, you can use the kubectl rollout undo command. To roll back to the previous version of the nginx deployment, execute the following command: kubectl rollout undo deployments nginx-deployment View the updated rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 <none> View the details of the latest deployment revision kubectl rollout history deployment/nginx-deployment --revision=3 The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9. Output (do not copy) deployments \"nginx-deployment\" with revision #3 Pod Template: Labels: app=nginx pod-template-hash=3123191453 Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"Trigger a deployment rollback"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#define-service-types-in-the-manifest","text":"A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx. apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80 In the Cloud Shell, to deploy your manifest, execute the following command: kubectl apply -f ./service-nginx.yaml This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service.","title":"Define service types in the manifest"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#verify-the-loadbalancer-creation","text":"To view the details of the nginx service, execute the following command: kubectl get service nginx The output should look like the example. Output (do not copy) NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE nginx 10.X.X.X X.X.X.X 60000/TCP run=nginx 1m When the external IP appears, open http://[EXTERNAL_IP]:60000/ in a new browser tab to see the server being served through network load balancing. It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated.","title":"Verify the LoadBalancer creation"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-5-perform-a-canary-deployment","text":"A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-canary labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx track: canary Version: 1.9.1 spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment. Create the canary deployment based on the configuration file. kubectl apply -f nginx-canary.yaml When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present. kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page. Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas. kubectl scale --replicas=0 deployment nginx-deployment Verify that the only running replica is now the Canary deployment: kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment.","title":"Task 5. Perform a canary deployment"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#session-affinity","text":"The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections. For example: apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer sessionAffinity: ClientIP selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80","title":"Session affinity"},{"location":"35-gcp-persistent-volume-for-gke/","text":"Configuring persistent volume for Google Kubernetes Engine Objectives Create manifests for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) for Google Cloud persistent disks (dynamically created or existing) Mount Google Cloud persistent disk PVCs as volumes in Pods Use manifests to create StatefulSets Mount Google Cloud persistent disk PVCs as volumes in StatefulSets Verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted Task 1. Create PVs and PVCs In this task, you create a PVC, which triggers Kubernetes to automatically create a PV. Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure tab completion for the kubectl command-line tool. source <(kubectl completion bash) Configure access to your cluster for kubectl: gcloud container clusters get-credentials $my_cluster --zone $my_zone Create and apply a manifest with a PVC Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you. You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Storage/ To show that you currently have no PVCs, execute the following command: kubectl get persistentvolumeclaim Output : No resources found in default namespace. To create the PVC, execute the following command: kubectl apply -f pvc-demo.yaml To show your newly created PVC, execute the following command: kubectl get persistentvolumeclaim Partial output NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 5s Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod. Mount the PVC to a Pod The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated. kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk To create the Pod with the volume, execute the following command: kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE pvc-demo-pod 0/1 ContainerCreating 0 18s If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\". To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output : Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Test the persistence of the PV You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact. Delete the pvc-demo-pod. kubectl delete pod pvc-demo-pod List the Pods in the cluster. kubectl get pods Output No resources found in default namespace. There should be no Pods on the cluster. To show your PVC, execute the following command: kubectl get persistentvolumeclaim Partial output NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 22m Your PVC still exists, and was not deleted when the Pod was deleted. Redeploy the pvc-demo-pod. kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 15s The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created. To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To verify that the text file still contains your message execute the following command: cat /var/www/html/index.html Output Test webpage in a persistent volume! The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated. Enter the following command to leave the interactive shell on the nginx container. exit Task 3. Create StatefulSets with PVCs In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers. Release the PVC Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod: kubectl delete pod pvc-demo-pod Confirm the Pod has been removed. kubectl get pods Output No resources found in default namespace. Create a StatefulSet The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task. kind: Service apiVersion: v1 metadata: name: statefulset-demo-service spec: ports: - protocol: TCP port: 80 targetPort: 9376 type: LoadBalancer --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-demo spec: selector: matchLabels: app: MyApp serviceName: statefulset-demo-service replicas: 3 updateStrategy: type: RollingUpdate template: metadata: labels: app: MyApp spec: containers: - name: stateful-set-container image: nginx ports: - containerPort: 80 name: http volumeMounts: - name: hello-web-disk mountPath: \"/var/www/html\" volumeClaimTemplates: - metadata: name: hello-web-disk spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 30Gi To create the StatefulSet with the volume, execute the following command: kubectl apply -f statefulset-demo.yaml Output service \"statefulset-demo-service\" created statefulset.apps \"statefulset-demo\" created You now have a statefulset running behind a service named statefulset-demo-service. Verify the connection of Pods in StatefulSets Use \"kubectl describe\" to view the details of the StatefulSet: kubectl describe statefulset statefulset-demo Note the event status at the end of the output. The service and statefulset created successfully. Normal SuccessfulCreate 10s statefulset-controller Message: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success Normal SuccessfulCreate 10s statefulset-controller Message: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE statefulset-demo-0 1/1 Running 0 6m statefulset-demo-1 1/1 Running 0 3m statefulset-demo-2 1/1 Running 0 2m To list the PVCs, execute the following command: kubectl get pvc Output NAME STATUS VOLUME CAPACITY ACCESS hello-web-disk Bound pvc-86117ea6-...34 30Gi RWO hello-web-disk-st...-demo-0 Bound pvc-92d21d0f-...34 30Gi RWO hello-web-disk-st...-demo-1 Bound pvc-9bc84d92-...34 30Gi RWO hello-web-disk-st...-demo-2 Bound pvc-a526ecdf-...34 30Gi RWO The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod. Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet: kubectl describe pvc hello-web-disk-statefulset-demo-0 Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted. To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it statefulset-demo-0 -- sh Verify that there is no index.html text file in the /var/www/html directory. cat /var/www/html/index.html To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Delete the Pod where you updated the file on the PVC. kubectl delete pod statefulset-demo-0 List the Pods in the cluster. kubectl get pods You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod. Note: You need to wait until the Pod status shows that it is running again. Connect to the shell on the new statefulset-demo-0 Pod. kubectl exec -it statefulset-demo-0 -- sh Verify that the text file still contains your message. cat /var/www/html/index.html Output Test webpage in a persistent volume! The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Configuring persistent volume for Google Kubernetes Engine"},{"location":"35-gcp-persistent-volume-for-gke/#configuring-persistent-volume-for-google-kubernetes-engine","text":"","title":"Configuring persistent volume for Google Kubernetes Engine"},{"location":"35-gcp-persistent-volume-for-gke/#objectives","text":"Create manifests for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) for Google Cloud persistent disks (dynamically created or existing) Mount Google Cloud persistent disk PVCs as volumes in Pods Use manifests to create StatefulSets Mount Google Cloud persistent disk PVCs as volumes in StatefulSets Verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted","title":"Objectives"},{"location":"35-gcp-persistent-volume-for-gke/#task-1-create-pvs-and-pvcs","text":"In this task, you create a PVC, which triggers Kubernetes to automatically create a PV. Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure tab completion for the kubectl command-line tool. source <(kubectl completion bash) Configure access to your cluster for kubectl: gcloud container clusters get-credentials $my_cluster --zone $my_zone","title":"Task 1. Create PVs and PVCs"},{"location":"35-gcp-persistent-volume-for-gke/#create-and-apply-a-manifest-with-a-pvc","text":"Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you. You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Storage/ To show that you currently have no PVCs, execute the following command: kubectl get persistentvolumeclaim Output : No resources found in default namespace. To create the PVC, execute the following command: kubectl apply -f pvc-demo.yaml To show your newly created PVC, execute the following command: kubectl get persistentvolumeclaim Partial output NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 5s","title":"Create and apply a manifest with a PVC"},{"location":"35-gcp-persistent-volume-for-gke/#task-2-mount-and-verify-google-cloud-persistent-disk-pvcs-in-pods","text":"In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod.","title":"Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods"},{"location":"35-gcp-persistent-volume-for-gke/#mount-the-pvc-to-a-pod","text":"The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated. kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk To create the Pod with the volume, execute the following command: kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE pvc-demo-pod 0/1 ContainerCreating 0 18s If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\". To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output : Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit","title":"Mount the PVC to a Pod"},{"location":"35-gcp-persistent-volume-for-gke/#test-the-persistence-of-the-pv","text":"You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact. Delete the pvc-demo-pod. kubectl delete pod pvc-demo-pod List the Pods in the cluster. kubectl get pods Output No resources found in default namespace. There should be no Pods on the cluster. To show your PVC, execute the following command: kubectl get persistentvolumeclaim Partial output NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 22m Your PVC still exists, and was not deleted when the Pod was deleted. Redeploy the pvc-demo-pod. kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 15s The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created. To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To verify that the text file still contains your message execute the following command: cat /var/www/html/index.html Output Test webpage in a persistent volume! The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Test the persistence of the PV"},{"location":"35-gcp-persistent-volume-for-gke/#task-3-create-statefulsets-with-pvcs","text":"In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers.","title":"Task 3. Create StatefulSets with PVCs"},{"location":"35-gcp-persistent-volume-for-gke/#release-the-pvc","text":"Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod: kubectl delete pod pvc-demo-pod Confirm the Pod has been removed. kubectl get pods Output No resources found in default namespace.","title":"Release the PVC"},{"location":"35-gcp-persistent-volume-for-gke/#create-a-statefulset","text":"The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task. kind: Service apiVersion: v1 metadata: name: statefulset-demo-service spec: ports: - protocol: TCP port: 80 targetPort: 9376 type: LoadBalancer --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-demo spec: selector: matchLabels: app: MyApp serviceName: statefulset-demo-service replicas: 3 updateStrategy: type: RollingUpdate template: metadata: labels: app: MyApp spec: containers: - name: stateful-set-container image: nginx ports: - containerPort: 80 name: http volumeMounts: - name: hello-web-disk mountPath: \"/var/www/html\" volumeClaimTemplates: - metadata: name: hello-web-disk spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 30Gi To create the StatefulSet with the volume, execute the following command: kubectl apply -f statefulset-demo.yaml Output service \"statefulset-demo-service\" created statefulset.apps \"statefulset-demo\" created You now have a statefulset running behind a service named statefulset-demo-service. Verify the connection of Pods in StatefulSets Use \"kubectl describe\" to view the details of the StatefulSet: kubectl describe statefulset statefulset-demo Note the event status at the end of the output. The service and statefulset created successfully. Normal SuccessfulCreate 10s statefulset-controller Message: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success Normal SuccessfulCreate 10s statefulset-controller Message: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful List the Pods in the cluster. kubectl get pods Output NAME READY STATUS RESTARTS AGE statefulset-demo-0 1/1 Running 0 6m statefulset-demo-1 1/1 Running 0 3m statefulset-demo-2 1/1 Running 0 2m To list the PVCs, execute the following command: kubectl get pvc Output NAME STATUS VOLUME CAPACITY ACCESS hello-web-disk Bound pvc-86117ea6-...34 30Gi RWO hello-web-disk-st...-demo-0 Bound pvc-92d21d0f-...34 30Gi RWO hello-web-disk-st...-demo-1 Bound pvc-9bc84d92-...34 30Gi RWO hello-web-disk-st...-demo-2 Bound pvc-a526ecdf-...34 30Gi RWO The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod. Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet: kubectl describe pvc hello-web-disk-statefulset-demo-0","title":"Create a StatefulSet"},{"location":"35-gcp-persistent-volume-for-gke/#task-4-verify-the-persistence-of-persistent-volume-connections-to-pods-managed-by-statefulsets","text":"In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted. To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it statefulset-demo-0 -- sh Verify that there is no index.html text file in the /var/www/html directory. cat /var/www/html/index.html To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Delete the Pod where you updated the file on the PVC. kubectl delete pod statefulset-demo-0 List the Pods in the cluster. kubectl get pods You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod. Note: You need to wait until the Pod status shows that it is running again. Connect to the shell on the new statefulset-demo-0 Pod. kubectl exec -it statefulset-demo-0 -- sh Verify that the text file still contains your message. cat /var/www/html/index.html Output Test webpage in a persistent volume! The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets"},{"location":"36-gcloud-basics/","text":"Basic gcloud commands gcloud: for working with Compute Engine, Google Kubernetes Engine (GKE) and many Google Cloud services gsutil: for working with Cloud Storage kubectl: for working with GKE and Kubernetes bq: for working with BigQuery gcloud List zones in a region gcloud compute zones list | grep $REGION Set default zone gcloud config set compute/zone $ZONE Create a VM gcloud compute instances create $MY_VMNAME \\ --machine-type \"e2-standard-2\" \\ --image-project \"debian-cloud\" \\ --image-family \"debian-9\" \\ --subnet \"default\" List of virtual machine instances gcloud compute instances list Create a service account gcloud iam service-accounts create service-account-name --display-name \"service-account-name\" Provide editor access to the service account gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member serviceAccount:service-account-name@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role roles/viewer View configuration gcloud config list Authenticate as a service account in the cloud shell gcloud auth activate-service-account --key-file credentials.json List of active accounts gcloud auth list Copy file to a virtual machine gcloud compute scp index.html first-vm:index.nginx-debian.html --zone=us-central1-c Cloud storage Create bucket gsutil mb gs://$BUCKET Copy files gsutil cp gs://$SOURCE DESTINATION Get access list of a file gsutil acl get gs://$BUCKET/$FILE Set private access gsutil acl set private gs://$BUCKET/$FILE Make bucket public gsutil iam ch allUsers:objectViewer gs://$BUCKET","title":"Basic gcloud commands"},{"location":"36-gcloud-basics/#basic-gcloud-commands","text":"gcloud: for working with Compute Engine, Google Kubernetes Engine (GKE) and many Google Cloud services gsutil: for working with Cloud Storage kubectl: for working with GKE and Kubernetes bq: for working with BigQuery","title":"Basic gcloud commands"},{"location":"36-gcloud-basics/#gcloud","text":"List zones in a region gcloud compute zones list | grep $REGION Set default zone gcloud config set compute/zone $ZONE Create a VM gcloud compute instances create $MY_VMNAME \\ --machine-type \"e2-standard-2\" \\ --image-project \"debian-cloud\" \\ --image-family \"debian-9\" \\ --subnet \"default\" List of virtual machine instances gcloud compute instances list Create a service account gcloud iam service-accounts create service-account-name --display-name \"service-account-name\" Provide editor access to the service account gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member serviceAccount:service-account-name@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role roles/viewer View configuration gcloud config list Authenticate as a service account in the cloud shell gcloud auth activate-service-account --key-file credentials.json List of active accounts gcloud auth list Copy file to a virtual machine gcloud compute scp index.html first-vm:index.nginx-debian.html --zone=us-central1-c","title":"gcloud"},{"location":"36-gcloud-basics/#cloud-storage","text":"Create bucket gsutil mb gs://$BUCKET Copy files gsutil cp gs://$SOURCE DESTINATION Get access list of a file gsutil acl get gs://$BUCKET/$FILE Set private access gsutil acl set private gs://$BUCKET/$FILE Make bucket public gsutil iam ch allUsers:objectViewer gs://$BUCKET","title":"Cloud storage"},{"location":"37-migrate-for-anthos/","text":"Steps for migration Configure processing cluster Add migration source Generate the migration object, create a plan in yaml Generate artifacts [container images, yaml files] for the migrate Test the artifacts Deploy to production cluster Installing the necessary tools Create the processing cluster using this command gcloud container --project $PROJECT_ID \\ clusters create $CLUSTER_NAME \\ --zone $CLUSTER_ZONE \\ --cluster-version 1.14 \\ --machine-type \"n1-standard-4\" \\ --image-type \"UBUNTU\" \\ --num-nodes 1 \\ --enable-stackdriver-kubernetes \\ --scopes \"cloud-platform\" \\ --enable-ip-alias \\ --tags=\"http-server\" install migrate for anthos migctl setup install Specify location of the migration migctl source create ce my-ce-src --project my-project --zone zone This is for migration from GCP. Migrating from other cloud providers might require additional libraries. Create a migration plan migctl migration create test-migration --source my-ce-src --vm-id my-id --intent Image This creates the yaml that can be customized based on migration need. We can specify following intents: * Image * Data * Image and Data * PV Based Container Generate the artifacts for the migration migctl migration generate-artifacts my-migration This will create two types of images. * A runnable image * A non-runnable image layer that can be used to update the container in the future Migration yaml are created and stored in a storage bucket as an intermediate step. Get the YAML using this command migctl migration get-artifacts test-migration Deploy after editing the YAML kubectl apply -f deployment spec.yaml","title":"37 migrate for anthos"},{"location":"37-migrate-for-anthos/#steps-for-migration","text":"Configure processing cluster Add migration source Generate the migration object, create a plan in yaml Generate artifacts [container images, yaml files] for the migrate Test the artifacts Deploy to production cluster","title":"Steps for migration"},{"location":"37-migrate-for-anthos/#installing-the-necessary-tools","text":"Create the processing cluster using this command gcloud container --project $PROJECT_ID \\ clusters create $CLUSTER_NAME \\ --zone $CLUSTER_ZONE \\ --cluster-version 1.14 \\ --machine-type \"n1-standard-4\" \\ --image-type \"UBUNTU\" \\ --num-nodes 1 \\ --enable-stackdriver-kubernetes \\ --scopes \"cloud-platform\" \\ --enable-ip-alias \\ --tags=\"http-server\" install migrate for anthos migctl setup install Specify location of the migration migctl source create ce my-ce-src --project my-project --zone zone This is for migration from GCP. Migrating from other cloud providers might require additional libraries. Create a migration plan migctl migration create test-migration --source my-ce-src --vm-id my-id --intent Image This creates the yaml that can be customized based on migration need. We can specify following intents: * Image * Data * Image and Data * PV Based Container Generate the artifacts for the migration migctl migration generate-artifacts my-migration This will create two types of images. * A runnable image * A non-runnable image layer that can be used to update the container in the future Migration yaml are created and stored in a storage bucket as an intermediate step. Get the YAML using this command migctl migration get-artifacts test-migration Deploy after editing the YAML kubectl apply -f deployment spec.yaml","title":"Installing the necessary tools"},{"location":"38-kubectl-basics/","text":"Kubectl kubectl is used to communicate with the kube-APIserver on the control plane. It must be configured with the location and credentials of the kubernetes cluster. kubectl configuration is in a config file: $HOME/.kube/config Configuration contains * Target cluster name * Credentials for the cluster View the configuration kubectl config view Retrieve the credentials gcloud container clusters \\ get-credentuals $CLUSTER_NAME \\ --zone $ZONE This will update the configuration file with appropriate credentials. List the pods kubectl get pods Example of deployment object in YAML format apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 3 template: metadata: labels: app: my-app spec: containers: - name: my-app image: gcr.io/demo/my-app:1.0 ports: - containerPort: 8080 Deploy using this command kubectl apply -f $DEPLOYMENT_FILE It is also possible to deploy imperatively using the run command, that specifies all parameters inline. kubectl run $DEPLOYMENT_NAME \\ --image gcr.io/demo/my-app:1.0 \\ --replicas 3 \\ --labels app:my-app \\ --port 8080 \\ --generator deployment/apps.v1 \\ --save-config Inspect the Deployment kubectl get deployment $DEPLOYMENT_NAME This can be saved as a YAML for future reference kubectl get deployment $DEPLOYMENT_NAME -o yaml > this.yaml Scaling the deployment kubectl scale deployment $DEPLOYMENT_NAME -replicas=5 Autoscaling the deployment by specifying a minimum and maximum number of replicas kubectl autoscale deployment $DEPLOYMENT_NAME --min=5 --max=15 --cpu-percentage=75 This will create a Kubernetes object called Horizontal Pod Autoscaler. This object will scale the number of pods to match the cpu utilization. This will scale a particular deployment within a cluster, and not a cluster as a whole. Update a deployment using these commands kubectl apply -f $DEPLOYMENT_FILE This will update the deployment according to the specifications in the YAML. Secondly, deployment can be updated imperatively. kubectl set image deployment $DEPLOYMENT_NAME $IMAGE $IMAGE:$TAG Thirdly, this command will open the configuration file in vim editor. If changed and save, the update will be deployed. kubectl edit deployment/$DEPLOYMENT_NAME Rollout undo command kubectl rollout undo deployment $DEPLOYMENT_NAME Rolling out to a specific version kubectl rollout undo deployment $DEPLOYMENT_NAME --to-revision=2 View version history kubectl rollout history deployment $DEPLOYMENT_NAME --revision=2 When a deployment is edited, a rollout is triggered automatically. To change this behavior: kubectl rollout pause deployment $DEPLOYMENT_NAME The changes will be deployed in one rollout after its resumed: kubectl rollout resume deployment $DEPLOYMENT_NAME We can monitor the rollout status: kubectl rollout status deployment $DEPLOYMENT_NAME Delete a deployment kubectl delete deployment $DEPLOYMENT_NAME This will lead to deletion of all resources run by the deployment, including running pods.","title":"38 kubectl basics"},{"location":"38-kubectl-basics/#kubectl","text":"kubectl is used to communicate with the kube-APIserver on the control plane. It must be configured with the location and credentials of the kubernetes cluster. kubectl configuration is in a config file: $HOME/.kube/config Configuration contains * Target cluster name * Credentials for the cluster View the configuration kubectl config view Retrieve the credentials gcloud container clusters \\ get-credentuals $CLUSTER_NAME \\ --zone $ZONE This will update the configuration file with appropriate credentials. List the pods kubectl get pods Example of deployment object in YAML format apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 3 template: metadata: labels: app: my-app spec: containers: - name: my-app image: gcr.io/demo/my-app:1.0 ports: - containerPort: 8080 Deploy using this command kubectl apply -f $DEPLOYMENT_FILE It is also possible to deploy imperatively using the run command, that specifies all parameters inline. kubectl run $DEPLOYMENT_NAME \\ --image gcr.io/demo/my-app:1.0 \\ --replicas 3 \\ --labels app:my-app \\ --port 8080 \\ --generator deployment/apps.v1 \\ --save-config Inspect the Deployment kubectl get deployment $DEPLOYMENT_NAME This can be saved as a YAML for future reference kubectl get deployment $DEPLOYMENT_NAME -o yaml > this.yaml Scaling the deployment kubectl scale deployment $DEPLOYMENT_NAME -replicas=5 Autoscaling the deployment by specifying a minimum and maximum number of replicas kubectl autoscale deployment $DEPLOYMENT_NAME --min=5 --max=15 --cpu-percentage=75 This will create a Kubernetes object called Horizontal Pod Autoscaler. This object will scale the number of pods to match the cpu utilization. This will scale a particular deployment within a cluster, and not a cluster as a whole. Update a deployment using these commands kubectl apply -f $DEPLOYMENT_FILE This will update the deployment according to the specifications in the YAML. Secondly, deployment can be updated imperatively. kubectl set image deployment $DEPLOYMENT_NAME $IMAGE $IMAGE:$TAG Thirdly, this command will open the configuration file in vim editor. If changed and save, the update will be deployed. kubectl edit deployment/$DEPLOYMENT_NAME Rollout undo command kubectl rollout undo deployment $DEPLOYMENT_NAME Rolling out to a specific version kubectl rollout undo deployment $DEPLOYMENT_NAME --to-revision=2 View version history kubectl rollout history deployment $DEPLOYMENT_NAME --revision=2 When a deployment is edited, a rollout is triggered automatically. To change this behavior: kubectl rollout pause deployment $DEPLOYMENT_NAME The changes will be deployed in one rollout after its resumed: kubectl rollout resume deployment $DEPLOYMENT_NAME We can monitor the rollout status: kubectl rollout status deployment $DEPLOYMENT_NAME Delete a deployment kubectl delete deployment $DEPLOYMENT_NAME This will lead to deletion of all resources run by the deployment, including running pods.","title":"Kubectl"},{"location":"39-creating-gke-deployments/","text":"Task 1. Create deployment manifests and deploy to the cluster In this task, you create a deployment manifest for a Pod inside the cluster. Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure kubectl tab completion in Cloud Shell. source <(kubectl completion bash) In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Deployments/ Create a deployment manifest You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy your manifest, execute the following command: kubectl apply -f ./nginx-deployment.yaml To view a list of deployments, execute the following command: kubectl get deployments The output should look like this example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 3 0 3s Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 42s Task 2. Manually scale up and down the number of Pods in deployments Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero. In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell. Scale Pods up and down in the console Switch to the Google Cloud Console tab. On the Navigation menu ( 9a951fa6d60a98a5.png), click Kubernetes Engine > Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS > Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh. Scale Pods up and down in the shell Switch back to the Cloud Shell browser tab. In the Cloud Shell, to view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 3m To scale the Pod back up to three replicas, execute the following command: kubectl scale --replicas=3 deployment nginx-deployment To view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 4m Task 3. Trigger a deployment rollout and a deployment rollback A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout. In this task, you trigger deployment rollout, and then you trigger deployment rollback. Trigger a deployment rollout To update the version of nginx in the deployment, execute the following command: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record This updates the container image in your Deployment to nginx v1.9.1. To view the rollout status, execute the following command: kubectl rollout status deployment.v1.apps/nginx-deployment The output should look like the example. Output (do not copy) Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 1 old replicas pending termination... Waiting for rollout to finish: 1 old replicas pending termination... deployment \"nginx-deployment\" successfully rolled out To verify the change, get the list of deployments. kubectl get deployments The output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 6m View the rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true Trigger a deployment rollback To roll back an object's rollout, you can use the kubectl rollout undo command. To roll back to the previous version of the nginx deployment, execute the following command: kubectl rollout undo deployments nginx-deployment View the updated rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 <none> View the details of the latest deployment revision kubectl rollout history deployment/nginx-deployment --revision=3 The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9. Output (do not copy) deployments \"nginx-deployment\" with revision #3 Pod Template: Labels: app=nginx pod-template-hash=3123191453 Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Task 4. Define the service type in the manifest In this task, you create and verify a service that controls inbound traffic to an application. Services can be configured as ClusterIP, NodePort or LoadBalancer types. In this lab, you configure a LoadBalancer. Define service types in the manifest A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx. apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80 In the Cloud Shell, to deploy your manifest, execute the following command: kubectl apply -f ./service-nginx.yaml This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service. Verify the LoadBalancer creation To view the details of the nginx service, execute the following command: kubectl get service nginx The output should look like the example. Output (do not copy) NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE nginx 10.X.X.X X.X.X.X 60000/TCP run=nginx 1m When the external IP appears, open http://[EXTERNAL_IP]:60000/ in a new browser tab to see the server being served through network load balancing. It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated. Task 5. Perform a canary deployment A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-canary labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx track: canary Version: 1.9.1 spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment. Create the canary deployment based on the configuration file. kubectl apply -f nginx-canary.yaml When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present. kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page. Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas. kubectl scale --replicas=0 deployment nginx-deployment Verify that the only running replica is now the Canary deployment: kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment. Session affinity The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections. For example: apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer sessionAffinity: ClientIP selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80","title":"39 creating gke deployments"},{"location":"39-creating-gke-deployments/#task-1-create-deployment-manifests-and-deploy-to-the-cluster","text":"In this task, you create a deployment manifest for a Pod inside the cluster.","title":"Task 1. Create deployment manifests and deploy to the cluster"},{"location":"39-creating-gke-deployments/#connect-to-the-lab-gke-cluster","text":"In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure kubectl tab completion in Cloud Shell. source <(kubectl completion bash) In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Deployments/","title":"Connect to the lab GKE cluster"},{"location":"39-creating-gke-deployments/#create-a-deployment-manifest","text":"You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy your manifest, execute the following command: kubectl apply -f ./nginx-deployment.yaml To view a list of deployments, execute the following command: kubectl get deployments The output should look like this example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 3 0 3s Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 42s","title":"Create a deployment manifest"},{"location":"39-creating-gke-deployments/#task-2-manually-scale-up-and-down-the-number-of-pods-in-deployments","text":"Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero. In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell.","title":"Task 2. Manually scale up and down the number of Pods in deployments"},{"location":"39-creating-gke-deployments/#scale-pods-up-and-down-in-the-console","text":"Switch to the Google Cloud Console tab. On the Navigation menu ( 9a951fa6d60a98a5.png), click Kubernetes Engine > Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS > Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh.","title":"Scale Pods up and down in the console"},{"location":"39-creating-gke-deployments/#scale-pods-up-and-down-in-the-shell","text":"Switch back to the Cloud Shell browser tab. In the Cloud Shell, to view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 1/1 1 1 3m To scale the Pod back up to three replicas, execute the following command: kubectl scale --replicas=3 deployment nginx-deployment To view a list of Pods in the deployments, execute the following command: kubectl get deployments Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 4m","title":"Scale Pods up and down in the shell"},{"location":"39-creating-gke-deployments/#task-3-trigger-a-deployment-rollout-and-a-deployment-rollback","text":"A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout. In this task, you trigger deployment rollout, and then you trigger deployment rollback.","title":"Task 3. Trigger a deployment rollout and a deployment rollback"},{"location":"39-creating-gke-deployments/#trigger-a-deployment-rollout","text":"To update the version of nginx in the deployment, execute the following command: kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record This updates the container image in your Deployment to nginx v1.9.1. To view the rollout status, execute the following command: kubectl rollout status deployment.v1.apps/nginx-deployment The output should look like the example. Output (do not copy) Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 1 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 2 out of 3 new replicas updated... Waiting for rollout to finish: 1 old replicas pending termination... Waiting for rollout to finish: 1 old replicas pending termination... deployment \"nginx-deployment\" successfully rolled out To verify the change, get the list of deployments. kubectl get deployments The output should look like the example. Output (do not copy) NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 6m View the rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 <none> 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true","title":"Trigger a deployment rollout"},{"location":"39-creating-gke-deployments/#trigger-a-deployment-rollback","text":"To roll back an object's rollout, you can use the kubectl rollout undo command. To roll back to the previous version of the nginx deployment, execute the following command: kubectl rollout undo deployments nginx-deployment View the updated rollout history of the deployment. kubectl rollout history deployment nginx-deployment The output should look like the example. Your output might not be an exact match. Output (do not copy) deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 <none> View the details of the latest deployment revision kubectl rollout history deployment/nginx-deployment --revision=3 The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9. Output (do not copy) deployments \"nginx-deployment\" with revision #3 Pod Template: Labels: app=nginx pod-template-hash=3123191453 Containers: nginx: Image: nginx:1.7.9 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"Trigger a deployment rollback"},{"location":"39-creating-gke-deployments/#task-4-define-the-service-type-in-the-manifest","text":"In this task, you create and verify a service that controls inbound traffic to an application. Services can be configured as ClusterIP, NodePort or LoadBalancer types. In this lab, you configure a LoadBalancer.","title":"Task 4. Define the service type in the manifest"},{"location":"39-creating-gke-deployments/#define-service-types-in-the-manifest","text":"A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx. apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80 In the Cloud Shell, to deploy your manifest, execute the following command: kubectl apply -f ./service-nginx.yaml This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service.","title":"Define service types in the manifest"},{"location":"39-creating-gke-deployments/#verify-the-loadbalancer-creation","text":"To view the details of the nginx service, execute the following command: kubectl get service nginx The output should look like the example. Output (do not copy) NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE nginx 10.X.X.X X.X.X.X 60000/TCP run=nginx 1m When the external IP appears, open http://[EXTERNAL_IP]:60000/ in a new browser tab to see the server being served through network load balancing. It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated.","title":"Verify the LoadBalancer creation"},{"location":"39-creating-gke-deployments/#task-5-perform-a-canary-deployment","text":"A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-canary labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx track: canary Version: 1.9.1 spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment. Create the canary deployment based on the configuration file. kubectl apply -f nginx-canary.yaml When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present. kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page. Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas. kubectl scale --replicas=0 deployment nginx-deployment Verify that the only running replica is now the Canary deployment: kubectl get deployments Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment.","title":"Task 5. Perform a canary deployment"},{"location":"39-creating-gke-deployments/#session-affinity","text":"The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections. For example: apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer sessionAffinity: ClientIP selector: app: nginx ports: - protocol: TCP port: 60000 targetPort: 80","title":"Session affinity"},{"location":"40-data-reliability-conference/","text":"Date May 25, 2022 Value of data reliability Leads to bad decision Trust Waste of time Biases leading to reputation loss Loss of revenue Increased cost Steps to solve Automate as much as possible Monitor as much as possible Control releases - version control, code review and environments Design simplicity Pipeline promotion without the commotion Great expectations python library helps to set up tests in data pipeline dbt helps create pipeline and put documentaton alongside dbt allows for tests in pipelines, defined as yaml dbt has library dbt_expectation for tests Connects with github for version control What do Data scientists look for in data discoverable relevant usable accurate timely fit for purpose Workflow orchestration to reduce negative engineering If the data team spends 80% of time in negative engineering, reducing my 25% with double productivity. Prefect is open source, python based workflow orchestration List of negative engineering tasks Controlled release of data pipeline Create notifications with Slack and GitHub actions Implementation will be automatic After pull request, docker images are created automatically and pussed to image registry Kubernetes will take the latest container Objective of data observability Constantly monitor data, notify data teams so that data teams knows before the consumers of data Demo platform: https://drecon-workshop.bigeye.com/#0 Observability can be set up using SQL queries at a basic level Tools to check dbt - SQL workflows bigeye - data observability solution great expectation - data testing library in python MLFlow - hosting ML models ArgoCD - CI/CD for kubernetes Prefect - workflow orchestration tool, gcp has Workflow GitFlow Data outage It is very different from software outage, because the spectrum is large - 1. data is old 1. data is missing 1. its wrong 1. the database is missing Easy to estimte cost of ourage in e-Commerce. For data, the cost can be low if no one is looking at the data, or very high cost when the CEO makes a wrog decision. We should be truth seeking, but risk taking. While we should be making investments to improve data reliability, we should not wait to make bold decisions until we have the best data. Data reliability What it is? Contacts Christianna Clark Shailvi Wakhlu Scott Shi Handled 5B tickets when working for Uber. Randy Pitcher dbt developer advocate. Very good overview of dbt. Egor Gryaznov BigEye founder. Pavani Rangavajhula Good overview of CI/CD of data pipelines. Glen-Erik Cortes Started with MBA and now working as ML Engineering Manager. Talks about ML Ops. Jerry Shen Very articulate data scientist. Works at OpenSea. Miriah Peterson","title":"40 data reliability conference"},{"location":"40-data-reliability-conference/#date-may-25-2022","text":"","title":"Date May 25, 2022"},{"location":"40-data-reliability-conference/#value-of-data-reliability","text":"Leads to bad decision Trust Waste of time Biases leading to reputation loss Loss of revenue Increased cost","title":"Value of data reliability"},{"location":"40-data-reliability-conference/#steps-to-solve","text":"Automate as much as possible Monitor as much as possible Control releases - version control, code review and environments Design simplicity","title":"Steps to solve"},{"location":"40-data-reliability-conference/#pipeline-promotion-without-the-commotion","text":"Great expectations python library helps to set up tests in data pipeline dbt helps create pipeline and put documentaton alongside dbt allows for tests in pipelines, defined as yaml dbt has library dbt_expectation for tests Connects with github for version control","title":"Pipeline promotion without the commotion"},{"location":"40-data-reliability-conference/#what-do-data-scientists-look-for-in-data","text":"discoverable relevant usable accurate timely fit for purpose","title":"What do Data scientists look for in data"},{"location":"40-data-reliability-conference/#workflow-orchestration-to-reduce-negative-engineering","text":"If the data team spends 80% of time in negative engineering, reducing my 25% with double productivity. Prefect is open source, python based workflow orchestration List of negative engineering tasks","title":"Workflow orchestration to reduce negative engineering"},{"location":"40-data-reliability-conference/#controlled-release-of-data-pipeline","text":"Create notifications with Slack and GitHub actions Implementation will be automatic After pull request, docker images are created automatically and pussed to image registry Kubernetes will take the latest container","title":"Controlled release of data pipeline"},{"location":"40-data-reliability-conference/#objective-of-data-observability","text":"Constantly monitor data, notify data teams so that data teams knows before the consumers of data Demo platform: https://drecon-workshop.bigeye.com/#0 Observability can be set up using SQL queries at a basic level","title":"Objective of data observability"},{"location":"40-data-reliability-conference/#tools-to-check","text":"dbt - SQL workflows bigeye - data observability solution great expectation - data testing library in python MLFlow - hosting ML models ArgoCD - CI/CD for kubernetes Prefect - workflow orchestration tool, gcp has Workflow GitFlow","title":"Tools to check"},{"location":"40-data-reliability-conference/#data-outage","text":"It is very different from software outage, because the spectrum is large - 1. data is old 1. data is missing 1. its wrong 1. the database is missing Easy to estimte cost of ourage in e-Commerce. For data, the cost can be low if no one is looking at the data, or very high cost when the CEO makes a wrog decision. We should be truth seeking, but risk taking. While we should be making investments to improve data reliability, we should not wait to make bold decisions until we have the best data.","title":"Data outage"},{"location":"40-data-reliability-conference/#data-reliability","text":"What it is?","title":"Data reliability"},{"location":"40-data-reliability-conference/#contacts","text":"Christianna Clark Shailvi Wakhlu Scott Shi Handled 5B tickets when working for Uber. Randy Pitcher dbt developer advocate. Very good overview of dbt. Egor Gryaznov BigEye founder. Pavani Rangavajhula Good overview of CI/CD of data pipelines. Glen-Erik Cortes Started with MBA and now working as ML Engineering Manager. Talks about ML Ops. Jerry Shen Very articulate data scientist. Works at OpenSea. Miriah Peterson","title":"Contacts"},{"location":"41-persistent-storage-gke/","text":"Persistent storage in GKE Task 1. Create PVs and PVCs In this task, you create a PVC, which triggers Kubernetes to automatically create a PV. Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure tab completion for the kubectl command-line tool. source <(kubectl completion bash) Configure access to your cluster for kubectl: gcloud container clusters get-credentials $my_cluster --zone $my_zone Create and apply a manifest with a PVC Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you. You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Storage/ To show that you currently have no PVCs, execute the following command: kubectl get persistentvolumeclaim Output (Do not copy): No resources found in default namespace. To create the PVC, execute the following command: kubectl apply -f pvc-demo.yaml To show your newly created PVC, execute the following command: kubectl get persistentvolumeclaim Partial output (Do not copy): NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 5s Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod. Mount the PVC to a Pod The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated. kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk To create the Pod with the volume, execute the following command: kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE pvc-demo-pod 0/1 ContainerCreating 0 18s If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\". To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Test the persistence of the PV You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact. Delete the pvc-demo-pod. kubectl delete pod pvc-demo-pod List the Pods in the cluster. kubectl get pods Output (Do not copy): No resources found in default namespace. There should be no Pods on the cluster. To show your PVC, execute the following command: kubectl get persistentvolumeclaim Partial output (Do not copy): NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 22m Your PVC still exists, and was not deleted when the Pod was deleted. Redeploy the pvc-demo-pod. kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 15s The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created. To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To verify that the text file still contains your message execute the following command: cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated. Enter the following command to leave the interactive shell on the nginx container. exit Task 3. Create StatefulSets with PVCs In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers. Release the PVC Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod: kubectl delete pod pvc-demo-pod Confirm the Pod has been removed. kubectl get pods Output (Do not copy): No resources found in default namespace. Create a StatefulSet The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task. kind: Service apiVersion: v1 metadata: name: statefulset-demo-service spec: ports: - protocol: TCP port: 80 targetPort: 9376 type: LoadBalancer --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-demo spec: selector: matchLabels: app: MyApp serviceName: statefulset-demo-service replicas: 3 updateStrategy: type: RollingUpdate template: metadata: labels: app: MyApp spec: containers: - name: stateful-set-container image: nginx ports: - containerPort: 80 name: http volumeMounts: - name: hello-web-disk mountPath: \"/var/www/html\" volumeClaimTemplates: - metadata: name: hello-web-disk spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 30Gi To create the StatefulSet with the volume, execute the following command: kubectl apply -f statefulset-demo.yaml Output (Do not copy): service \"statefulset-demo-service\" created statefulset.apps \"statefulset-demo\" created You now have a statefulset running behind a service named statefulset-demo-service. Verify the connection of Pods in StatefulSets Use \"kubectl describe\" to view the details of the StatefulSet: kubectl describe statefulset statefulset-demo Note the event status at the end of the output. The service and statefulset created successfully. Normal SuccessfulCreate 10s statefulset-controller Message: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success Normal SuccessfulCreate 10s statefulset-controller Message: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE statefulset-demo-0 1/1 Running 0 6m statefulset-demo-1 1/1 Running 0 3m statefulset-demo-2 1/1 Running 0 2m To list the PVCs, execute the following command: kubectl get pvc Output (Do not copy): NAME STATUS VOLUME CAPACITY ACCESS hello-web-disk Bound pvc-86117ea6-...34 30Gi RWO hello-web-disk-st...-demo-0 Bound pvc-92d21d0f-...34 30Gi RWO hello-web-disk-st...-demo-1 Bound pvc-9bc84d92-...34 30Gi RWO hello-web-disk-st...-demo-2 Bound pvc-a526ecdf-...34 30Gi RWO The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod. Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet: kubectl describe pvc hello-web-disk-statefulset-demo-0 Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted. To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it statefulset-demo-0 -- sh Verify that there is no index.html text file in the /var/www/html directory. cat /var/www/html/index.html To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Delete the Pod where you updated the file on the PVC. kubectl delete pod statefulset-demo-0 List the Pods in the cluster. kubectl get pods You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod. Note: You need to wait until the Pod status shows that it is running again. Connect to the shell on the new statefulset-demo-0 Pod. kubectl exec -it statefulset-demo-0 -- sh Verify that the text file still contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Persistent storage in GKE"},{"location":"41-persistent-storage-gke/#persistent-storage-in-gke","text":"","title":"Persistent storage in GKE"},{"location":"41-persistent-storage-gke/#task-1-create-pvs-and-pvcs","text":"In this task, you create a PVC, which triggers Kubernetes to automatically create a PV.","title":"Task 1. Create PVs and PVCs"},{"location":"41-persistent-storage-gke/#connect-to-the-lab-gke-cluster","text":"In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 Configure tab completion for the kubectl command-line tool. source <(kubectl completion bash) Configure access to your cluster for kubectl: gcloud container clusters get-credentials $my_cluster --zone $my_zone","title":"Connect to the lab GKE cluster"},{"location":"41-persistent-storage-gke/#create-and-apply-a-manifest-with-a-pvc","text":"Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you. You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hello-web-disk spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/Storage/ To show that you currently have no PVCs, execute the following command: kubectl get persistentvolumeclaim Output (Do not copy): No resources found in default namespace. To create the PVC, execute the following command: kubectl apply -f pvc-demo.yaml To show your newly created PVC, execute the following command: kubectl get persistentvolumeclaim Partial output (Do not copy): NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 5s","title":"Create and apply a manifest with a PVC"},{"location":"41-persistent-storage-gke/#task-2-mount-and-verify-google-cloud-persistent-disk-pvcs-in-pods","text":"In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod.","title":"Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods"},{"location":"41-persistent-storage-gke/#mount-the-pvc-to-a-pod","text":"The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated. kind: Pod apiVersion: v1 metadata: name: pvc-demo-pod spec: containers: - name: frontend image: nginx volumeMounts: - mountPath: \"/var/www/html\" name: pvc-demo-volume volumes: - name: pvc-demo-volume persistentVolumeClaim: claimName: hello-web-disk To create the Pod with the volume, execute the following command: kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE pvc-demo-pod 0/1 ContainerCreating 0 18s If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\". To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit","title":"Mount the PVC to a Pod"},{"location":"41-persistent-storage-gke/#test-the-persistence-of-the-pv","text":"You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact. Delete the pvc-demo-pod. kubectl delete pod pvc-demo-pod List the Pods in the cluster. kubectl get pods Output (Do not copy): No resources found in default namespace. There should be no Pods on the cluster. To show your PVC, execute the following command: kubectl get persistentvolumeclaim Partial output (Do not copy): NAME STATUS VOLUME CAP ACCESS MODES CLASS AGE hello-web-disk Bound pvc-8...34 30Gi RWO standard 22m Your PVC still exists, and was not deleted when the Pod was deleted. Redeploy the pvc-demo-pod. kubectl apply -f pod-volume-demo.yaml List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE pvc-demo-pod 1/1 Running 0 15s The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created. To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it pvc-demo-pod -- sh To verify that the text file still contains your message execute the following command: cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Test the persistence of the PV"},{"location":"41-persistent-storage-gke/#task-3-create-statefulsets-with-pvcs","text":"In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers.","title":"Task 3. Create StatefulSets with PVCs"},{"location":"41-persistent-storage-gke/#release-the-pvc","text":"Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod: kubectl delete pod pvc-demo-pod Confirm the Pod has been removed. kubectl get pods Output (Do not copy): No resources found in default namespace.","title":"Release the PVC"},{"location":"41-persistent-storage-gke/#create-a-statefulset","text":"The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task. kind: Service apiVersion: v1 metadata: name: statefulset-demo-service spec: ports: - protocol: TCP port: 80 targetPort: 9376 type: LoadBalancer --- apiVersion: apps/v1 kind: StatefulSet metadata: name: statefulset-demo spec: selector: matchLabels: app: MyApp serviceName: statefulset-demo-service replicas: 3 updateStrategy: type: RollingUpdate template: metadata: labels: app: MyApp spec: containers: - name: stateful-set-container image: nginx ports: - containerPort: 80 name: http volumeMounts: - name: hello-web-disk mountPath: \"/var/www/html\" volumeClaimTemplates: - metadata: name: hello-web-disk spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 30Gi To create the StatefulSet with the volume, execute the following command: kubectl apply -f statefulset-demo.yaml Output (Do not copy): service \"statefulset-demo-service\" created statefulset.apps \"statefulset-demo\" created You now have a statefulset running behind a service named statefulset-demo-service.","title":"Create a StatefulSet"},{"location":"41-persistent-storage-gke/#verify-the-connection-of-pods-in-statefulsets","text":"Use \"kubectl describe\" to view the details of the StatefulSet: kubectl describe statefulset statefulset-demo Note the event status at the end of the output. The service and statefulset created successfully. Normal SuccessfulCreate 10s statefulset-controller Message: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success Normal SuccessfulCreate 10s statefulset-controller Message: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful List the Pods in the cluster. kubectl get pods Output (Do not copy): NAME READY STATUS RESTARTS AGE statefulset-demo-0 1/1 Running 0 6m statefulset-demo-1 1/1 Running 0 3m statefulset-demo-2 1/1 Running 0 2m To list the PVCs, execute the following command: kubectl get pvc Output (Do not copy): NAME STATUS VOLUME CAPACITY ACCESS hello-web-disk Bound pvc-86117ea6-...34 30Gi RWO hello-web-disk-st...-demo-0 Bound pvc-92d21d0f-...34 30Gi RWO hello-web-disk-st...-demo-1 Bound pvc-9bc84d92-...34 30Gi RWO hello-web-disk-st...-demo-2 Bound pvc-a526ecdf-...34 30Gi RWO The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod. Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet: kubectl describe pvc hello-web-disk-statefulset-demo-0","title":"Verify the connection of Pods in StatefulSets"},{"location":"41-persistent-storage-gke/#task-4-verify-the-persistence-of-persistent-volume-connections-to-pods-managed-by-statefulsets","text":"In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted. To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command: kubectl exec -it statefulset-demo-0 -- sh Verify that there is no index.html text file in the /var/www/html directory. cat /var/www/html/index.html To create a simple text message as a web page in the Pod enter the following commands: echo Test webpage in a persistent volume!>/var/www/html/index.html chmod +x /var/www/html/index.html Verify the text file contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! Enter the following command to leave the interactive shell on the nginx container. exit Delete the Pod where you updated the file on the PVC. kubectl delete pod statefulset-demo-0 List the Pods in the cluster. kubectl get pods You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod. Note: You need to wait until the Pod status shows that it is running again. Connect to the shell on the new statefulset-demo-0 Pod. kubectl exec -it statefulset-demo-0 -- sh Verify that the text file still contains your message. cat /var/www/html/index.html Output (Do not copy): Test webpage in a persistent volume! The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved. Enter the following command to leave the interactive shell on the nginx container. exit","title":"Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets"},{"location":"42-creating-gke-deployments-from-shell/","text":"Deploying Google Kubernetes Engine Clusters from Cloud Shell Objectives Use kubectl to build and manipulate GKE clusters Use kubectl and configuration files to deploy Pods Use Container Registry to store and deploy containers Task 1. Deploy GKE clusters In this task, you use Cloud Shell to deploy GKE clusters. In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 In Cloud Shell, type the following command to create a Kubernetes cluster. gcloud container clusters create $my_cluster --num-nodes 3 --zone $my_zone --enable-ip-alias This form of the command sets most options to their defaults. To view the entire set of possible options, click here You will see a number of warnings highlighting changes to default GKE cluster settings that were introduced as newer version of Kubernetes have been adopted by GKE. Task 2. Modify GKE clusters It is easy to modify many of the parameters of existing clusters in Google Cloud Console or Cloud Shell. In this task, you use Cloud Shell to modify the number of nodes in a GKE cluster. In Cloud Shell, execute the following command to modify standard-cluster-1 to have four nodes: gcloud container clusters resize $my_cluster --zone $my_zone --num-nodes=4 Note: When issuing cluster commands, you typically must specify both the cluster name and the cluster location (region or zone). When prompted with Do you want to continue (Y/n), press y to confirm. Note: You need to wait a few minutes for the cluster deployment to complete. When the operation completes, you should see on the Google Cloud Console Kubernetes Engine > Clusters page that the cluster now has four nodes. You can modify many other cluster parameters by using the gcloud container cluster command. Task 3. Connect to a GKE cluster In this task, you use Cloud Shell to authenticate to a GKE cluster and then inspect the kubectl configuration files. Authentication in Kubernetes applies both to communicating with the cluster from an external client through the kube-APIserver running on the master and to cluster containers communicating within the cluster or externally. In Kubernetes, authentication can take several forms. For GKE, authentication is typically handled with OAuth2 tokens and can be managed through Cloud Identity and Access Management across the project as a whole and, optionally, through role-based access control which can be defined and configured within each cluster. In GKE, cluster containers can use service accounts to authenticate to and access external resources. Important For Kubernetes versions before 1.12, client certificates and basic authentication are not disabled by default. These are lower security methods of authentication and should be disabled to increase cluster security. (For versions 1.12 and later both of these methods are disabled by default.). To create a kubeconfig file with the credentials of the current user (to allow authentication) and provide the endpoint details for a specific cluster (to allow communicating with that cluster through the kubectl command-line tool), execute the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone This command creates a .kube directory in your home directory if it doesn't already exist. In the .kube directory, the command creates a file named config if it doesn't already exist, which is used to store the authentication and configuration information. The config file is typically called the kubeconfig file. Open the kubeconfig file with the nano text editor: nano ~/.kube/config You can now examine all of the authentication and endpoint configuration data stored in the file. Information for both clusters should appear. The information was populated during cluster creation. Press CTRL+X to exit the nano editor. Note: The kubeconfig file can contain information for many clusters. The currently active context (the cluster that kubectl commands manipulate) is indicated by the current-context property. You don't have to run the gcloud container clusters get-credentials command to populate the kubeconfig file for clusters that you created in the same context (the same user in the same environment), because those clusters already have their details populated when the cluster is created. But you have to run the command to connect to a cluster created by another user or in another environment. The command is also an easy way to switch the active context to a different cluster. Task 4. Use kubectl to inspect a GKE cluster In this task, you use Cloud Shell and kubectl to inspect a GKE cluster. After the kubeconfig file is populated and the active context is set to a particular cluster, you can use the kubectl command-line tool to execute commands against the cluster. Most such commands ultimately trigger a REST API call against the master API server, which triggers the associated action. In Cloud Shell, execute the following command to print out the content of the kubeconfig file: kubectl config view The sensitive certificate data is replaced with DATA+OMITTED. In Cloud Shell, execute the following command to print out the cluster information for the active context: kubectl cluster-info The output describes the active context cluster. Output (do not copy) Kubernetes master is running at https://104.155.191.14 GLBCDefaultBackend is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. In Cloud Shell, execute the following command to print out the active context: kubectl config current-context A line of output indicates the active context cluster. Output (do not copy) gke_[PROJECT_ID]_us-central1-a_standard-cluster-1 PROJECT_ID is your project ID. This information is the same as the information in the current-context property of the kubeconfig file. In Cloud Shell, execute the following command to print out some details for all the cluster contexts in the kubeconfig file: kubectl config get-contexts Several lines of output indicate details about the cluster you created and an indication of which is the active context cluster. In general, this command lists some details of the clusters present in the user's kubeconfig file, including any other clusters that were created by the user as well as any manually added to the kubeconfig file. In Cloud Shell, execute the following command to change the active context: kubectl config use-context gke_${GOOGLE_CLOUD_PROJECT}_us-central1-a_standard-cluster-1 In this case you have only one cluster, so this command didn't change anything. However in the future you may have more than one cluster in a project. You can use this approach to switching the active context when your kubeconfig file has the credentials and configuration for several clusters already populated. This approach requires the full name of the cluster, which includes the gke prefix, the project ID, the location, and the display name, all concatenated with underscores. In Cloud Shell, execute the following command to view the resource usage across the nodes of the cluster: kubectl top nodes The output should look like the following example. Output (do not copy) NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% gke-standard-cluster-1-def... 29m 3% 431Mi 16% gke-standard-cluster-1-def... 45m 4% 605Mi 22% gke-standard-cluster-1-def... 40m 4% 559Mi 21% gke-standard-cluster-1-def... 34m 3% 488Mi 18% Another top command ( kubectl top pods) shows similar information across all the deployed Pods in the cluster. In Cloud Shell, execute the following command to enable bash autocompletion for kubectl : source <(kubectl completion bash) This command produces no output. In Cloud Shell, type kubectl followed by a space and press the Tab key twice. The shell outputs all the possible commands. In Cloud Shell, type kubectl co and press the Tab key twice. The shell outputs all commands starting with \"co\" (or any other text you type). Task 5. Deploy Pods to GKE clusters In this task, you use Cloud Shell to deploy Pods to GKE clusters. Use kubectl to deploy Pods to GKE Kubernetes introduces the abstraction of a Pod to group one or more related containers as a single entity to be scheduled and deployed as a unit on the same node. You can deploy a Pod that is a single container from a single container image. Or a Pod can contain many containers from many container images. In Cloud Shell, execute the following command to deploy nginx as a Pod named nginx-1: kubectl create deployment --image nginx nginx-1 This command creates a Pod named nginx with a container running the nginx image. When a repository isn't specified, the default behavior is to try and find the image either locally or in the Docker public registry. In this case, the image is pulled from the Docker public registry. In Cloud Shell, execute the following command to view all the deployed Pods in the active context cluster: kubectl get pods The output should look like the following example, but with a slightly different Pod name. Output (do not copy) NAME READY STATUS RESTARTS AGE nginx-1-74c7bbdb84-nvwsc 1/1 Running 0 9s You will now enter your Pod name into a variable that we will use throughout this lab. Using variables like this can help you minimize human error when typing long names. You must type your Pod's unique name in place of [ your_pod_name ]. export my_nginx_pod=[your_pod_name] Example (do not copy) export my_nginx_pod=nginx-1-74c7bbdb84-nvwsc Confirm that you have set the environment variable successfully by having the shell echo the value back to you: echo $my_nginx_pod Output (do not copy) nginx-1-74c7bbdb84-nvwsc In Cloud Shell, execute the following command to view the complete details of the Pod you just created. kubectl describe pod $my_nginx_pod The output should look like the following example. Details of the Pod, as well as its status and conditions and the events in its lifecycle, are displayed. Condensed Output (do not copy, edited to fit screen) Name: nginx-1-74c7bbdb84-nvwsc Namespace: default Node: gke-standard-cluster-1-default-pool-bc4ec334-0hmk/10.128.0.5 Start Time: Sun, 16 Dec 2018 14:29:38 -0500 Labels: pod-template-hash=3073668640 run=nginx-1 Annotations: kubernetes.io/limit-ranger=LimitRanger plugin set: cpu ... Status: Running IP: 10.8.3.3 Controlled By: ReplicaSet/nginx-1-74c7bbdb84 Containers: nginx-1: Container ID: docker://dce87d274e6d25300b07ec244c265d42806579fee... Image: nginx:latest Image ID: docker-pullable://nginx@sha256:87e9b6904b4286b8d41... Port: <none> Host Port: <none> State: Running Started: Sun, 16 Dec 2018 14:29:44 -0500 Ready: True Restart Count: 0 Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-tok... Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-nphcg: Type: Secret (a volume populated by a Secret) SecretName: default-token-nphcg Optional: false QoS Class: Burstable Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sche... 1m default-scheduler Successf... Normal Succ... 1m kubelet, gke-standard-cl... MountVol... Normal Pull... 1m kubelet, gke-standard-cl... pulling ... Normal Pull... 1m kubelet, gke-standard-cl... Successf... Normal Crea... 1m kubelet, gke-standard-cl... Created ... Normal Star... 1m kubelet, gke-standard-cl... Started ... ``` ### Push a file into a container To be able to serve static content through the nginx web server, you must create and place a file into the container. In Cloud Shell, type the following commands to open a file named test.html in the nano text editor. nano ~/test.html Add the following text (shell script) to the empty test.html file: This is title Hello world Press CTRL+X, then press Y and enter to save the file and exit the nano editor. In Cloud Shell, execute the following command to place the file into the appropriate location within the nginx container in the nginx Pod to be served statically: kubectl cp ~/test.html $my_nginx_pod:/usr/share/nginx/html/test.html This command copies the `test.html` file from the local home directory to the `/usr/share/nginx/html` directory of the first container in the nginx Pod. You could specify other containers in a multi-container Pod by using the -c option, followed by the name of the container. ### Expose the Pod for testing To expose a Pod to clients outside the cluster requires a service. Services are discussed elsewhere in the course and used extensively in other labs. You can use a simple command to create a service to expose a Pod. In Cloud Shell, execute the following command to create a service to expose our nginx Pod externally: kubectl expose pod $my_nginx_pod --port 80 --type LoadBalancer This command creates a LoadBalancer service, which allows the nginx Pod to accessed from internet addresses outside of the cluster. In Cloud Shell, execute the following command to view details about services in the cluster: kubectl get services The output should look like the following example. You use the external IP address in the next step. Note: You might have to repeat the command a few times before the new service has its external IP populated. Condensed Output (do not copy, edited to fit screen) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.11.240.1 443/TCP 1h nginx-1-7...wsc LoadBalancer 10.11.240.87 80:31695/TCP 3s The kubernetes service is one of the default services created or used by the cluster. The nginx service that you created is also displayed. You may need to re-run this command several times before the External IP address is displayed. Condensed Output (do not copy, edited to fit screen) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.11.240.1 443/TCP 1h nginx-1-7...wsc LoadBalancer 10.11.240.87 104.154.177.46 80:31695/TCP 1m In Cloud Shell, execute the following command to verify that the nginx container is serving the static HTML file that you copied. You replace [`EXTERNAL_IP`] with the external IP address of your service that you obtained from the output of the previous step. curl http://[EXTERNAL_IP]/test.html The file contents appear in the output. You can go to the same address in your browser to see the file rendered as HTML. Example (do not copy) curl http://104.154.177.46/test.html This is title Hello world In Cloud Shell, execute the following command to view the resources being used by the nginx Pod: kubectl top pods Output (do not copy) NAME CPU(cores) MEMORY(bytes) nginx-1-74c7bbdb84-nvwsc 0m 2Mi ## Task 6. Introspect GKE Pods In this task, you connect to a Pod to adjust settings, edit files, and make other live changes to the Pod. Important Use this process only when troubleshooting or experimenting. Because the changes you make are not made to the source image of the Pod, they won't be present in any replicas. ### Prepare the environment The preferred way of deploying Pods and other resources to Kubernetes is through configuration files, which are sometimes called manifest files. Configuration files are typically written in the YAML syntax, specifying the details of the resource. With configuration files, you can more easily specify complex options than with a long line of command-line arguments. YAML syntax is similar to, but more concise than, JSON syntax and it enables the same kind of hierarchical structuring of objects and properties. The source repository for the lab contains sample YAML files that have been prepared for you. In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/GKE_Shell/ A sample manifest YAML file for a Pod called new-nginx-pod.yaml has been provided for you: apiVersion: v1 kind: Pod metadata: name: new-nginx labels: name: new-nginx spec: containers: - name: new-nginx image: nginx ports: - containerPort: 80 ``` To deploy your manifest, execute the following command: kubectl apply -f ./new-nginx-pod.yaml Click Check my progress to verify the objective. Deploy manifest file for a Pod called new-nginx To see a list of Pods, execute the following command: kubectl get pods The output should look like the example. Output (do not copy) NAME READY STATUS RESTARTS AGE new-nginx 1/1 Running 0 9s nginx-1-74c7bbdb84-nvwsc 1/1 Running 0 55m You can see your new nginx Pod as well as the one we created earlier in the lab. Use shell redirection to connect to a Pod Some container images include a shell environment that you can launch. This shell environment might be more convenient than executing individual commands with kubectl. For instance, the nginx image includes a bash shell. In this task you use shell redirection to connect to the bash shell in your new nginx pod to carry out a sequence of actions. In Cloud Shell, execute the following command to start an interactive bash shell in the nginx container: kubectl exec -it new-nginx /bin/bash A new shell prompt appears. Output (do not copy) root@new-nginx:/# You have started an interactive bash shell in the container of the new-nginx Pod. If the Pod had several containers, you could specify one by name with the -c option. Because the nginx container image has no text editing tools by default, you need to install one. In Cloud Shell, in the nginx bash shell, execute the following commands to install the nano text editor: apt-get update apt-get install nano You need to create a test.html file in the static served directory on the nginx container. In Cloud Shell, in the nginx bash shell, execute the following commands to switch to the static files directory and create a test.html file: cd /usr/share/nginx/html nano test.html In Cloud Shell, in the nginx bash shell nano session, type the following text: <html> <header><title>This is title</title></header> <body> Hello world </body> </html> Press CTRL+X, then press Y and enter to save the file and exit the nano editor. In Cloud Shell, in the nginx bash shell, execute the following command to exit the nginx bash shell: exit To connect to and test the modified nginx container (with the new static HTML file), you could create a service. An easier way is to use port forwarding to connect to the Pod directly from Cloud Shell. In Cloud Shell, execute the following command to set up port forwarding from Cloud Shell to the nginx Pod (from port 10081 of the Cloud Shell VM to port 80 of the nginx container): kubectl port-forward new-nginx 10081:80 The output should look like the example. Output (do not copy) Forwarding from 127.0.0.1:10081 -> 80 Forwarding from [::1]:10081 -> 80 This is a foreground process, so you need to open another Cloud Shell instance to test. In the Cloud Shell menu bar, click the plus sign (+) icon to start a new Cloud Shell session. A second Cloud Shell session appears in your Cloud Shell window. You can switch between sessions by clicking the titles in the menu bar. In the second Cloud Shell session, execute the following command to test the modified nginx container through the port forwarding: curl http://127.0.0.1:10081/test.html The HTML text you placed in the test.html file is displayed. <html> <header><title>This is title</title></header> <body> Hello world </body> </html> View the logs of a Pod In the Cloud Shell menu bar, click the plus sign (+) icon to start another new Cloud Shell session. A third Cloud Shell session appears in your Cloud Shell window. As before, you can switch sessions by clicking them in the menu bar. In the third Cloud Shell window, execute the following command to display the logs and to stream new logs as they arrive (and also include timestamps) for the new-nginx Pod: kubectl logs new-nginx -f --timestamps You will see the logs display in this new window Return to the second Cloud Shell window and re-run the curl command to generate some traffic on the Pod. Review the additional log messages as they appear in the third Cloud Shell window.","title":"42 creating gke deployments from shell"},{"location":"42-creating-gke-deployments-from-shell/#deploying-google-kubernetes-engine-clusters-from-cloud-shell","text":"","title":"Deploying Google Kubernetes Engine Clusters from Cloud Shell"},{"location":"42-creating-gke-deployments-from-shell/#objectives","text":"Use kubectl to build and manipulate GKE clusters Use kubectl and configuration files to deploy Pods Use Container Registry to store and deploy containers","title":"Objectives"},{"location":"42-creating-gke-deployments-from-shell/#task-1-deploy-gke-clusters","text":"In this task, you use Cloud Shell to deploy GKE clusters. In Cloud Shell, type the following command to set the environment variable for the zone and cluster name. export my_zone=us-central1-a export my_cluster=standard-cluster-1 In Cloud Shell, type the following command to create a Kubernetes cluster. gcloud container clusters create $my_cluster --num-nodes 3 --zone $my_zone --enable-ip-alias This form of the command sets most options to their defaults. To view the entire set of possible options, click here You will see a number of warnings highlighting changes to default GKE cluster settings that were introduced as newer version of Kubernetes have been adopted by GKE.","title":"Task 1. Deploy GKE clusters"},{"location":"42-creating-gke-deployments-from-shell/#task-2-modify-gke-clusters","text":"It is easy to modify many of the parameters of existing clusters in Google Cloud Console or Cloud Shell. In this task, you use Cloud Shell to modify the number of nodes in a GKE cluster. In Cloud Shell, execute the following command to modify standard-cluster-1 to have four nodes: gcloud container clusters resize $my_cluster --zone $my_zone --num-nodes=4 Note: When issuing cluster commands, you typically must specify both the cluster name and the cluster location (region or zone). When prompted with Do you want to continue (Y/n), press y to confirm. Note: You need to wait a few minutes for the cluster deployment to complete. When the operation completes, you should see on the Google Cloud Console Kubernetes Engine > Clusters page that the cluster now has four nodes. You can modify many other cluster parameters by using the gcloud container cluster command.","title":"Task 2. Modify GKE clusters"},{"location":"42-creating-gke-deployments-from-shell/#task-3-connect-to-a-gke-cluster","text":"In this task, you use Cloud Shell to authenticate to a GKE cluster and then inspect the kubectl configuration files. Authentication in Kubernetes applies both to communicating with the cluster from an external client through the kube-APIserver running on the master and to cluster containers communicating within the cluster or externally. In Kubernetes, authentication can take several forms. For GKE, authentication is typically handled with OAuth2 tokens and can be managed through Cloud Identity and Access Management across the project as a whole and, optionally, through role-based access control which can be defined and configured within each cluster. In GKE, cluster containers can use service accounts to authenticate to and access external resources. Important For Kubernetes versions before 1.12, client certificates and basic authentication are not disabled by default. These are lower security methods of authentication and should be disabled to increase cluster security. (For versions 1.12 and later both of these methods are disabled by default.). To create a kubeconfig file with the credentials of the current user (to allow authentication) and provide the endpoint details for a specific cluster (to allow communicating with that cluster through the kubectl command-line tool), execute the following command: gcloud container clusters get-credentials $my_cluster --zone $my_zone This command creates a .kube directory in your home directory if it doesn't already exist. In the .kube directory, the command creates a file named config if it doesn't already exist, which is used to store the authentication and configuration information. The config file is typically called the kubeconfig file. Open the kubeconfig file with the nano text editor: nano ~/.kube/config You can now examine all of the authentication and endpoint configuration data stored in the file. Information for both clusters should appear. The information was populated during cluster creation. Press CTRL+X to exit the nano editor. Note: The kubeconfig file can contain information for many clusters. The currently active context (the cluster that kubectl commands manipulate) is indicated by the current-context property. You don't have to run the gcloud container clusters get-credentials command to populate the kubeconfig file for clusters that you created in the same context (the same user in the same environment), because those clusters already have their details populated when the cluster is created. But you have to run the command to connect to a cluster created by another user or in another environment. The command is also an easy way to switch the active context to a different cluster.","title":"Task 3. Connect to a GKE cluster"},{"location":"42-creating-gke-deployments-from-shell/#task-4-use-kubectl-to-inspect-a-gke-cluster","text":"In this task, you use Cloud Shell and kubectl to inspect a GKE cluster. After the kubeconfig file is populated and the active context is set to a particular cluster, you can use the kubectl command-line tool to execute commands against the cluster. Most such commands ultimately trigger a REST API call against the master API server, which triggers the associated action. In Cloud Shell, execute the following command to print out the content of the kubeconfig file: kubectl config view The sensitive certificate data is replaced with DATA+OMITTED. In Cloud Shell, execute the following command to print out the cluster information for the active context: kubectl cluster-info The output describes the active context cluster. Output (do not copy) Kubernetes master is running at https://104.155.191.14 GLBCDefaultBackend is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. In Cloud Shell, execute the following command to print out the active context: kubectl config current-context A line of output indicates the active context cluster. Output (do not copy) gke_[PROJECT_ID]_us-central1-a_standard-cluster-1 PROJECT_ID is your project ID. This information is the same as the information in the current-context property of the kubeconfig file. In Cloud Shell, execute the following command to print out some details for all the cluster contexts in the kubeconfig file: kubectl config get-contexts Several lines of output indicate details about the cluster you created and an indication of which is the active context cluster. In general, this command lists some details of the clusters present in the user's kubeconfig file, including any other clusters that were created by the user as well as any manually added to the kubeconfig file. In Cloud Shell, execute the following command to change the active context: kubectl config use-context gke_${GOOGLE_CLOUD_PROJECT}_us-central1-a_standard-cluster-1 In this case you have only one cluster, so this command didn't change anything. However in the future you may have more than one cluster in a project. You can use this approach to switching the active context when your kubeconfig file has the credentials and configuration for several clusters already populated. This approach requires the full name of the cluster, which includes the gke prefix, the project ID, the location, and the display name, all concatenated with underscores. In Cloud Shell, execute the following command to view the resource usage across the nodes of the cluster: kubectl top nodes The output should look like the following example. Output (do not copy) NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% gke-standard-cluster-1-def... 29m 3% 431Mi 16% gke-standard-cluster-1-def... 45m 4% 605Mi 22% gke-standard-cluster-1-def... 40m 4% 559Mi 21% gke-standard-cluster-1-def... 34m 3% 488Mi 18% Another top command ( kubectl top pods) shows similar information across all the deployed Pods in the cluster. In Cloud Shell, execute the following command to enable bash autocompletion for kubectl : source <(kubectl completion bash) This command produces no output. In Cloud Shell, type kubectl followed by a space and press the Tab key twice. The shell outputs all the possible commands. In Cloud Shell, type kubectl co and press the Tab key twice. The shell outputs all commands starting with \"co\" (or any other text you type).","title":"Task 4. Use kubectl to inspect a GKE cluster"},{"location":"42-creating-gke-deployments-from-shell/#task-5-deploy-pods-to-gke-clusters","text":"In this task, you use Cloud Shell to deploy Pods to GKE clusters.","title":"Task 5. Deploy Pods to GKE clusters"},{"location":"42-creating-gke-deployments-from-shell/#use-kubectl-to-deploy-pods-to-gke","text":"Kubernetes introduces the abstraction of a Pod to group one or more related containers as a single entity to be scheduled and deployed as a unit on the same node. You can deploy a Pod that is a single container from a single container image. Or a Pod can contain many containers from many container images. In Cloud Shell, execute the following command to deploy nginx as a Pod named nginx-1: kubectl create deployment --image nginx nginx-1 This command creates a Pod named nginx with a container running the nginx image. When a repository isn't specified, the default behavior is to try and find the image either locally or in the Docker public registry. In this case, the image is pulled from the Docker public registry. In Cloud Shell, execute the following command to view all the deployed Pods in the active context cluster: kubectl get pods The output should look like the following example, but with a slightly different Pod name. Output (do not copy) NAME READY STATUS RESTARTS AGE nginx-1-74c7bbdb84-nvwsc 1/1 Running 0 9s You will now enter your Pod name into a variable that we will use throughout this lab. Using variables like this can help you minimize human error when typing long names. You must type your Pod's unique name in place of [ your_pod_name ]. export my_nginx_pod=[your_pod_name] Example (do not copy) export my_nginx_pod=nginx-1-74c7bbdb84-nvwsc Confirm that you have set the environment variable successfully by having the shell echo the value back to you: echo $my_nginx_pod Output (do not copy) nginx-1-74c7bbdb84-nvwsc In Cloud Shell, execute the following command to view the complete details of the Pod you just created. kubectl describe pod $my_nginx_pod The output should look like the following example. Details of the Pod, as well as its status and conditions and the events in its lifecycle, are displayed. Condensed Output (do not copy, edited to fit screen) Name: nginx-1-74c7bbdb84-nvwsc Namespace: default Node: gke-standard-cluster-1-default-pool-bc4ec334-0hmk/10.128.0.5 Start Time: Sun, 16 Dec 2018 14:29:38 -0500 Labels: pod-template-hash=3073668640 run=nginx-1 Annotations: kubernetes.io/limit-ranger=LimitRanger plugin set: cpu ... Status: Running IP: 10.8.3.3 Controlled By: ReplicaSet/nginx-1-74c7bbdb84 Containers: nginx-1: Container ID: docker://dce87d274e6d25300b07ec244c265d42806579fee... Image: nginx:latest Image ID: docker-pullable://nginx@sha256:87e9b6904b4286b8d41... Port: <none> Host Port: <none> State: Running Started: Sun, 16 Dec 2018 14:29:44 -0500 Ready: True Restart Count: 0 Requests: cpu: 100m Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-tok... Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-nphcg: Type: Secret (a volume populated by a Secret) SecretName: default-token-nphcg Optional: false QoS Class: Burstable Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sche... 1m default-scheduler Successf... Normal Succ... 1m kubelet, gke-standard-cl... MountVol... Normal Pull... 1m kubelet, gke-standard-cl... pulling ... Normal Pull... 1m kubelet, gke-standard-cl... Successf... Normal Crea... 1m kubelet, gke-standard-cl... Created ... Normal Star... 1m kubelet, gke-standard-cl... Started ... ``` ### Push a file into a container To be able to serve static content through the nginx web server, you must create and place a file into the container. In Cloud Shell, type the following commands to open a file named test.html in the nano text editor. nano ~/test.html Add the following text (shell script) to the empty test.html file: This is title Hello world Press CTRL+X, then press Y and enter to save the file and exit the nano editor. In Cloud Shell, execute the following command to place the file into the appropriate location within the nginx container in the nginx Pod to be served statically: kubectl cp ~/test.html $my_nginx_pod:/usr/share/nginx/html/test.html This command copies the `test.html` file from the local home directory to the `/usr/share/nginx/html` directory of the first container in the nginx Pod. You could specify other containers in a multi-container Pod by using the -c option, followed by the name of the container. ### Expose the Pod for testing To expose a Pod to clients outside the cluster requires a service. Services are discussed elsewhere in the course and used extensively in other labs. You can use a simple command to create a service to expose a Pod. In Cloud Shell, execute the following command to create a service to expose our nginx Pod externally: kubectl expose pod $my_nginx_pod --port 80 --type LoadBalancer This command creates a LoadBalancer service, which allows the nginx Pod to accessed from internet addresses outside of the cluster. In Cloud Shell, execute the following command to view details about services in the cluster: kubectl get services The output should look like the following example. You use the external IP address in the next step. Note: You might have to repeat the command a few times before the new service has its external IP populated. Condensed Output (do not copy, edited to fit screen) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.11.240.1 443/TCP 1h nginx-1-7...wsc LoadBalancer 10.11.240.87 80:31695/TCP 3s The kubernetes service is one of the default services created or used by the cluster. The nginx service that you created is also displayed. You may need to re-run this command several times before the External IP address is displayed. Condensed Output (do not copy, edited to fit screen) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.11.240.1 443/TCP 1h nginx-1-7...wsc LoadBalancer 10.11.240.87 104.154.177.46 80:31695/TCP 1m In Cloud Shell, execute the following command to verify that the nginx container is serving the static HTML file that you copied. You replace [`EXTERNAL_IP`] with the external IP address of your service that you obtained from the output of the previous step. curl http://[EXTERNAL_IP]/test.html The file contents appear in the output. You can go to the same address in your browser to see the file rendered as HTML. Example (do not copy) curl http://104.154.177.46/test.html This is title Hello world In Cloud Shell, execute the following command to view the resources being used by the nginx Pod: kubectl top pods Output (do not copy) NAME CPU(cores) MEMORY(bytes) nginx-1-74c7bbdb84-nvwsc 0m 2Mi ## Task 6. Introspect GKE Pods In this task, you connect to a Pod to adjust settings, edit files, and make other live changes to the Pod. Important Use this process only when troubleshooting or experimenting. Because the changes you make are not made to the source image of the Pod, they won't be present in any replicas. ### Prepare the environment The preferred way of deploying Pods and other resources to Kubernetes is through configuration files, which are sometimes called manifest files. Configuration files are typically written in the YAML syntax, specifying the details of the resource. With configuration files, you can more easily specify complex options than with a long line of command-line arguments. YAML syntax is similar to, but more concise than, JSON syntax and it enables the same kind of hierarchical structuring of objects and properties. The source repository for the lab contains sample YAML files that have been prepared for you. In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell. git clone https://github.com/GoogleCloudPlatform/training-data-analyst Create a soft link as a shortcut to the working directory. ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s Change to the directory that contains the sample files for this lab. cd ~/ak8s/GKE_Shell/ A sample manifest YAML file for a Pod called new-nginx-pod.yaml has been provided for you: apiVersion: v1 kind: Pod metadata: name: new-nginx labels: name: new-nginx spec: containers: - name: new-nginx image: nginx ports: - containerPort: 80 ``` To deploy your manifest, execute the following command: kubectl apply -f ./new-nginx-pod.yaml Click Check my progress to verify the objective. Deploy manifest file for a Pod called new-nginx To see a list of Pods, execute the following command: kubectl get pods The output should look like the example. Output (do not copy) NAME READY STATUS RESTARTS AGE new-nginx 1/1 Running 0 9s nginx-1-74c7bbdb84-nvwsc 1/1 Running 0 55m You can see your new nginx Pod as well as the one we created earlier in the lab.","title":"Use kubectl to deploy Pods to GKE"},{"location":"42-creating-gke-deployments-from-shell/#use-shell-redirection-to-connect-to-a-pod","text":"Some container images include a shell environment that you can launch. This shell environment might be more convenient than executing individual commands with kubectl. For instance, the nginx image includes a bash shell. In this task you use shell redirection to connect to the bash shell in your new nginx pod to carry out a sequence of actions. In Cloud Shell, execute the following command to start an interactive bash shell in the nginx container: kubectl exec -it new-nginx /bin/bash A new shell prompt appears. Output (do not copy) root@new-nginx:/# You have started an interactive bash shell in the container of the new-nginx Pod. If the Pod had several containers, you could specify one by name with the -c option. Because the nginx container image has no text editing tools by default, you need to install one. In Cloud Shell, in the nginx bash shell, execute the following commands to install the nano text editor: apt-get update apt-get install nano You need to create a test.html file in the static served directory on the nginx container. In Cloud Shell, in the nginx bash shell, execute the following commands to switch to the static files directory and create a test.html file: cd /usr/share/nginx/html nano test.html In Cloud Shell, in the nginx bash shell nano session, type the following text: <html> <header><title>This is title</title></header> <body> Hello world </body> </html> Press CTRL+X, then press Y and enter to save the file and exit the nano editor. In Cloud Shell, in the nginx bash shell, execute the following command to exit the nginx bash shell: exit To connect to and test the modified nginx container (with the new static HTML file), you could create a service. An easier way is to use port forwarding to connect to the Pod directly from Cloud Shell. In Cloud Shell, execute the following command to set up port forwarding from Cloud Shell to the nginx Pod (from port 10081 of the Cloud Shell VM to port 80 of the nginx container): kubectl port-forward new-nginx 10081:80 The output should look like the example. Output (do not copy) Forwarding from 127.0.0.1:10081 -> 80 Forwarding from [::1]:10081 -> 80 This is a foreground process, so you need to open another Cloud Shell instance to test. In the Cloud Shell menu bar, click the plus sign (+) icon to start a new Cloud Shell session. A second Cloud Shell session appears in your Cloud Shell window. You can switch between sessions by clicking the titles in the menu bar. In the second Cloud Shell session, execute the following command to test the modified nginx container through the port forwarding: curl http://127.0.0.1:10081/test.html The HTML text you placed in the test.html file is displayed. <html> <header><title>This is title</title></header> <body> Hello world </body> </html>","title":"Use shell redirection to connect to a Pod"},{"location":"42-creating-gke-deployments-from-shell/#view-the-logs-of-a-pod","text":"In the Cloud Shell menu bar, click the plus sign (+) icon to start another new Cloud Shell session. A third Cloud Shell session appears in your Cloud Shell window. As before, you can switch sessions by clicking them in the menu bar. In the third Cloud Shell window, execute the following command to display the logs and to stream new logs as they arrive (and also include timestamps) for the new-nginx Pod: kubectl logs new-nginx -f --timestamps You will see the logs display in this new window Return to the second Cloud Shell window and re-run the curl command to generate some traffic on the Pod. Review the additional log messages as they appear in the third Cloud Shell window.","title":"View the logs of a Pod"},{"location":"43-setting-up-react/","text":"Steps to run react in local environment install react cli npm install -g create-react-app create app create-react-app my-react-app-name go to project folder cd my-react-app-name install dependencies npm install start live server npm start","title":"Steps to run react in local environment"},{"location":"43-setting-up-react/#steps-to-run-react-in-local-environment","text":"","title":"Steps to run react in local environment"},{"location":"43-setting-up-react/#install-react-cli","text":"npm install -g create-react-app","title":"install react cli"},{"location":"43-setting-up-react/#create-app","text":"create-react-app my-react-app-name","title":"create app"},{"location":"43-setting-up-react/#go-to-project-folder","text":"cd my-react-app-name","title":"go to project folder"},{"location":"43-setting-up-react/#install-dependencies","text":"npm install","title":"install dependencies"},{"location":"43-setting-up-react/#start-live-server","text":"npm start","title":"start live server"},{"location":"44-redis-with-gcp-cloud-func/","text":"The official documentation to connect Redis with cloud functions can be found here Setting up Redis with GCP Cloud Functions require the following steps: Set up local environment GCP permission Create redis instance Create a VPC network Create a subnet Create a serverless VPC Access Connector Deploy the function with the connector access Step 1: Set up the local environment GCP permission Set up the variable in terminal: export GOOGLE_APPLICATION_CREDENTIALS='/path/to/your/client_secret.json' This will not enable VPC connection from local environment. #ToDo Step 2: Redis instance creation In GCP, the product is called Memorystore for Redis. It is a serverless Redis service. Documentation is here gcloud command: gcloud redis instances create br-pixel-redis \\ --size=1 \\ --region us-central1 \\ --redis-version redis_6_x Save the configuration of the redis instance for using in the function. gcloud redis instances describe br-pixel-redis --region us-central1 > redis-function/redis-conf.yaml It will be something like this: authorizedNetwork: projects/my-project/global/networks/default createTime: '2018-04-09T21:47:56.824081Z' currentLocationId: us-central1-a host: 10.0.0.27 locationId: us-central1-a memorySizeGb: 2 name: projects/my-project/locations/us-central1/instances/myinstance networkThroughputGbps: 2 port: 6379 redisVersion: REDIS_6_X reservedIpRange: 10.0.0.24/29 state: READY tier: BASIC We will be using the host and port to connect with the instance. Step 3: Create a VPC network Virtual Private Cloud (VPC) networks allow for secured connection between resources within GCP. I have used the default VPC network of the GCP project for avoiding firewall configurations. Should we need to create one, here is the documentation to create a new VPC network. Step 4: Create a subnet for the network A network must have at least one subnet before you can use it. Auto mode VPC networks create subnets in each region automatically. Custom mode VPC networks start with no subnets, giving you full control over subnet creation. However, we have create a new one because subnets used for VPC connectors must have a netmask of 28. Here is the documentation to add a subnet. gcloud compute networks subnets create br-pixel-cache-subnet \\ --network default \\ --range 10.0.0.0/28 \\ --region us-central1 Valid network ranges can be found here Step 5: Create a serverless VPC Connector Serverless VPC access enables direct connection to the Virtual Private Cloud network from serverless environments such as Cloud Run, Cloud Functions or App Engine. Documentation is here gcloud compute networks vpc-access connectors create br-pixel-cache-vpc \\ --region us-central1 \\ --subnet br-pixel-cache-subnet Step 6: Deploy the Cloud Function The cloud function needs to point to the Connector using the following argument in the deploy command: gcloud functions deploy [FUNCTION_NAME] \\ --runtime python37 \\ --trigger-http \\ --region [REGION] \\ --vpc-connector projects/[PROJECT_ID]/locations/[REGION]/connectors/[CONNECTOR_NAME] \\ --set-env-vars REDISHOST=[REDIS_IP],REDISPORT=[REDIS_PORT] For example: gcloud functions deploy br-pixel-test \\ --entry-point=main \\ --allow-unauthenticated \\ --runtime=python38 \\ --region=us-central1 \\ --trigger-http \\ --memory 256MB \\ --min-instances 1 \\ --vpc-connector projects/recharge-webhooks/locations/us-central1/connectors/br-pixel-cache-vpc","title":"44 redis with gcp cloud func"},{"location":"44-redis-with-gcp-cloud-func/#step-1-set-up-the-local-environment-gcp-permission","text":"Set up the variable in terminal: export GOOGLE_APPLICATION_CREDENTIALS='/path/to/your/client_secret.json' This will not enable VPC connection from local environment. #ToDo","title":"Step 1: Set up the local environment GCP permission"},{"location":"44-redis-with-gcp-cloud-func/#step-2-redis-instance-creation","text":"In GCP, the product is called Memorystore for Redis. It is a serverless Redis service. Documentation is here gcloud command: gcloud redis instances create br-pixel-redis \\ --size=1 \\ --region us-central1 \\ --redis-version redis_6_x Save the configuration of the redis instance for using in the function. gcloud redis instances describe br-pixel-redis --region us-central1 > redis-function/redis-conf.yaml It will be something like this: authorizedNetwork: projects/my-project/global/networks/default createTime: '2018-04-09T21:47:56.824081Z' currentLocationId: us-central1-a host: 10.0.0.27 locationId: us-central1-a memorySizeGb: 2 name: projects/my-project/locations/us-central1/instances/myinstance networkThroughputGbps: 2 port: 6379 redisVersion: REDIS_6_X reservedIpRange: 10.0.0.24/29 state: READY tier: BASIC We will be using the host and port to connect with the instance.","title":"Step 2: Redis instance creation"},{"location":"44-redis-with-gcp-cloud-func/#step-3-create-a-vpc-network","text":"Virtual Private Cloud (VPC) networks allow for secured connection between resources within GCP. I have used the default VPC network of the GCP project for avoiding firewall configurations. Should we need to create one, here is the documentation to create a new VPC network.","title":"Step 3: Create a VPC network"},{"location":"44-redis-with-gcp-cloud-func/#step-4-create-a-subnet-for-the-network","text":"A network must have at least one subnet before you can use it. Auto mode VPC networks create subnets in each region automatically. Custom mode VPC networks start with no subnets, giving you full control over subnet creation. However, we have create a new one because subnets used for VPC connectors must have a netmask of 28. Here is the documentation to add a subnet. gcloud compute networks subnets create br-pixel-cache-subnet \\ --network default \\ --range 10.0.0.0/28 \\ --region us-central1 Valid network ranges can be found here","title":"Step 4: Create a subnet for the network"},{"location":"44-redis-with-gcp-cloud-func/#step-5-create-a-serverless-vpc-connector","text":"Serverless VPC access enables direct connection to the Virtual Private Cloud network from serverless environments such as Cloud Run, Cloud Functions or App Engine. Documentation is here gcloud compute networks vpc-access connectors create br-pixel-cache-vpc \\ --region us-central1 \\ --subnet br-pixel-cache-subnet","title":"Step 5: Create a serverless VPC Connector"},{"location":"44-redis-with-gcp-cloud-func/#step-6-deploy-the-cloud-function","text":"The cloud function needs to point to the Connector using the following argument in the deploy command: gcloud functions deploy [FUNCTION_NAME] \\ --runtime python37 \\ --trigger-http \\ --region [REGION] \\ --vpc-connector projects/[PROJECT_ID]/locations/[REGION]/connectors/[CONNECTOR_NAME] \\ --set-env-vars REDISHOST=[REDIS_IP],REDISPORT=[REDIS_PORT] For example: gcloud functions deploy br-pixel-test \\ --entry-point=main \\ --allow-unauthenticated \\ --runtime=python38 \\ --region=us-central1 \\ --trigger-http \\ --memory 256MB \\ --min-instances 1 \\ --vpc-connector projects/recharge-webhooks/locations/us-central1/connectors/br-pixel-cache-vpc","title":"Step 6: Deploy the Cloud Function"}]}