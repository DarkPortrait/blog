{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"01-linear-algebra-machine-learning/","title":"Linear Algebra Foundations for Machine Learning","text":"<p>Updated on 10/26/2019</p> <p>Linear algebra is a tool to help manage large data sets. It comes with techniques that makes manipulation of data very convenient.</p> <p>However, it makes more sense when described using geometry. I will describe it using only a two dimensional plane for sake of visual clarity.</p> <p>A 2-D plane means a data set with only two entries. But same ideas are application to n-dimensional plane or data sets with n entries.</p> <p>Most of the heavy work in linear algebra can be handled using computers. But these fundamental ideas are key to understanding what is happening under the hood.</p> <p>I drew in heavily from the excellent YouTube playlist called The Essence of Linear Algebra from 3Blue1Brown.</p> <p>In this playlist, Grant Sanderson explains the ideas using simple geometric operations using lines and dots on paper, i.e., a two dimensional plane.</p> <p>Here is what I learnt.</p>"},{"location":"01-linear-algebra-machine-learning/#linear-algebra-concepts","title":"Linear algebra concepts","text":"<p>Linear algebra involves writing numbers in columns (), called vectors and boxes (), called matrices.</p>"},{"location":"01-linear-algebra-machine-learning/#vector-shows-a-location","title":"Vector shows a location","text":"<p>A vector, single column of numbers (3, 5) represents a point on a cartesian 2-D plane, or any n-D space for that matter.</p> <p>You need two things to make this work- 1. Define a origin (0, 0) from where the vector points to the location of the point. 2. Define the units of direction in x and y axes.  </p>"},{"location":"01-linear-algebra-machine-learning/#the-basis-vectors","title":"The basis vectors","text":"<p>The unit vectors are called basis vectors. They tell you how far to move from the origin and in which direction. The basis vectors along the x-axis, y-axis and z-axis are called i, j and k respectively.</p>"},{"location":"01-linear-algebra-machine-learning/#linear-combination-of-vectors-and-span","title":"Linear combination of vectors and span","text":"<p>Any point on the 2-D plane can be reached with sum of two or more vectors.</p> <p>However, its not possible when two vectors are parallel. In that case, all possible linear combinations are on a straight line.</p> <p>There is also a third possibility, when the two vectors are zeros, no linear combination will get you anywhere from zero.</p> <p>The span of vectors v and w is the set of all their linear combinations.</p> <p>In a 3-D space, a span of 2 vectors will most likely be a 2-D plane, or a line if they are parallel. A span of 3 vectors in the same plane will cover the entire 3-D space, or a plane, or a line, or, in the most extreme case, just the origin.</p>"},{"location":"01-linear-algebra-machine-learning/#linear-transformation-of-the-plane","title":"Linear transformation of the plane","text":"<p>If you stretch, skew, rotate or invert the vector plane keeping- * the same origin same, and * grid lines parallel to one another, then it is called a linear transformation of the plane.</p> <p>The 2-D plane doesn't have to be with all square grids. We can define a plane where the axes are not perpendicular to one another.</p> <p>Linear transformation lets us define a vector on one plane using basis vectors of another plane.</p>"},{"location":"01-linear-algebra-machine-learning/#matrix","title":"Matrix","text":"<p>A matrix is just a row of vectors that defines a linear transformation. The first column tells where i lands, the second column defines where j lands, and so forth.</p>"},{"location":"01-linear-algebra-machine-learning/#vector-dot-products","title":"Vector dot products","text":"<p>When you multiple two vectors using dot product, you get the area of the parallelogram enclosed by the vectors. For a 3-D plane, this will be the volume.</p>"},{"location":"01-linear-algebra-machine-learning/#determinant","title":"Determinant","text":"<p>When you transform a plane using a matrix, for example, skew it. The area enclosed by vectors in that plane changes. Determinant tells you by how much the area changes.</p> <p>When talking about a 3-D plane, the determinant shows the change of volume.</p> <p>When the output of a transformation reduces the 2-D plane into a line, the determinant is zero.</p>"},{"location":"01-linear-algebra-machine-learning/#rank","title":"Rank","text":"<p>The number of dimensions of a the output of transformation is called rank.</p> <p>When you apply a matrix on a 2-D plane and the output is a line, we call it has a rank of 1.</p>"},{"location":"01-linear-algebra-machine-learning/#inverse-matrices","title":"Inverse matrices","text":"<p>An inverse matrix completes the transformation in reverse.</p> <p>The inverse cannot be found if- 1. The rank of transformation &lt; initial dimensions in the space (for example, you squish a 2-D plane into a line) 2. The determinant is zero</p>"},{"location":"01-linear-algebra-machine-learning/#change-of-basis","title":"Change of basis  <p>Matrices transform your vector space. In other words, if there is a vector space defined by your friend L,  vectors in your vector space.</p> <p>How do you explain your vectors in terms of Victor's world?</p> <p>Lets say matrix A transforms your basis to Victor's world. Therefore, A-1 does the opposite, it tells you where any vector in Victor's world lands in your world.</p>","text":""},{"location":"01-linear-algebra-machine-learning/#eigenvectors-and-eigenvalues","title":"Eigenvectors and eigenvalues <p>When you apply a matrix on a plane, the vectors change direction and are scaled. The vectors that remain on their own span, are called eigenvectors. The value by which these special vectors are scaled is called a eigenvalues.</p> <p>For example, if the transformation is stretching 2x along the horizontal axis, the x-axis is an eigenvector with eigenvalue of 2.</p> <p>Additionally, for a 3-D plane, the eigenvectors with eigenvalues of 1 are rotational axis.</p> <p>Av = lambda*v, where v is eigenvector and lambda is the eigenvalue.</p>","text":""},{"location":"01-linear-algebra-machine-learning/#extending-to-abstract-world-of-data","title":"Extending to abstract world of data","text":"<p>These ideas are explained using a 2-D, and sometimes, a 3-D plane. But you can (somewhat) easily extend it to n-D environment. Even abstract functions such as file compression or dimension reduction, when thought in light of geometric manipulation of space, starts making sense!</p> <p>This is part of ongoing series on my machine learning training.</p> <p>Please let me know what you think :)</p>"},{"location":"02-git_workflow/","title":"Creating a repo","text":"<ul> <li>git init</li> <li>git add <code>file-name</code></li> <li>git commit -m \"comment\"</li> <li>git pull <code>repo-name</code> <code>branch-name</code></li> <li>git push <code>repo-name</code> <code>branch-name</code></li> <li>git remote set-url origin git@github.com:username/repo.git</li> </ul>"},{"location":"02-git_workflow/#remove-a-repo","title":"Remove a repo","text":"<p>git remote remove <code>repo-name</code></p> <p>Git and Github are like Porn and Pornhub. You cannot create a git repo from the bash.</p>"},{"location":"03-python-virtual-environments-with-jupyter/","title":"What are Python Virtual Environments","text":"<p>Python virtual environment is a tool that helps to isolate packages by projects. This is useful to keep the list of installed packages clean. <code>pip list</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#installing-virtual-environment-in-ubuntu","title":"Installing virtual environment in Ubuntu","text":"<p>Install the python3 virtual environment package: <code>sudo apt-get install python3-venv</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#creating-a-python-virtual-environment","title":"Creating a python virtual environment","text":"<p>It is good practice to create a folder to keep all the environments. In that folder: <code>python3 -m venv my-env</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#activate-and-deactivate-a-python-virtual-environment","title":"Activate and deactivate a python virtual environment","text":"<p>Activate the environment using: <code>source myenv/bin/activate</code></p> <p>The terminal prompt will show name of the environment. </p> <p>To deactivate: <code>deactivate</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#delete-a-virtual-environment","title":"Delete a virtual environment","text":"<p>Deleting the virtual environment folder deletes the environment.  <code>rm -d my-env</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#list-packages-in-the-environment","title":"List packages in the environment","text":"<p><code>pip freeze</code> This will do <code>pip list</code> but in a format which can be used by the pip tool to install the packages.  Saving the list to a requirement.txt: <code>pip freeze &gt; requirement.txt</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#replicate-packages-from-an-environment","title":"Replicate packages from an environment","text":"<p>Using pip to install the packages listed in the requirement.txt file: <code>pip install -r requirement.txt</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#using-virtual-environments-in-jupyter-lab","title":"Using virtual environments in Jupyter Lab","text":"<p>Enable the virtual environment and install the ipykernel package. <code>pip install --user ipykernel</code></p> <p>Add the environment in jupyter using: <code>python -m ipykernel install --user --name=my-env</code></p> <p>This will show the following in terminal if done correctly. <code>Installed kernelspec myenv in /home/user/.local/share/jupyter/kernels/my-env</code></p> <p>This will add a new kernel my-env on the Jupyter Lab welcome page.</p>"},{"location":"03-python-virtual-environments-with-jupyter/#removing-environment-from-jupyter","title":"Removing environment from Jupyter","text":"<p>Delete the environment from the environment folder and then remove from the Jupyter lab. View a list of kernels available: <code>jupyter kernelspec list</code></p> <p>Now, to uninstall the kernel, you can type: <code>jupyter kernelspec uninstall my-env</code></p>"},{"location":"03-python-virtual-environments-with-jupyter/#references","title":"References","text":"<ul> <li>https://janakiev.com/blog/jupyter-virtual-envs/</li> <li>https://www.youtube.com/watch?v=Kg1Yvry_Ydk</li> </ul>"},{"location":"04-ecommerce-payments/","title":"Structure","text":"<ul> <li>Focus of the article - stakeholders involved and flow of information</li> <li>List of all the parties, what they do </li> <li>Elaborate on each section with<ul> <li>Core purpose served</li> <li>Key players</li> </ul> </li> <li>Security - list of tools <ul> <li>PCI </li> <li>SSL</li> <li>Tokenization</li> <li>Others [?]</li> </ul> </li> <li>Resource</li> </ul>"},{"location":"04-ecommerce-payments/#ecommerce-payments","title":"Ecommerce payments","text":""},{"location":"04-ecommerce-payments/#resources","title":"Resources","text":"<ul> <li>https://www.netsolutions.com/insights/12-factors-to-consider-while-choosing-a-payment-gateway-for-your-e-commerce-store/</li> <li>https://docs.google.com/spreadsheets/d/1pOC0rN_ahQZVpmNNR3dVqkX4eNxdR4xT-h25uSBJaVk/edit#gid=0</li> <li>https://www.skyverge.com/blog/woocommerce-pci-compliance/</li> <li>https://www.bigcommerce.com/blog/payment-gateways/</li> <li>https://www.ecommerce-nation.com/payment-processing-work/</li> <li>https://ecommerce-platforms.com/ecommerce-selling-advice/what-is-difference-between-a-payment-gateway-payment-processor-and-a-merchant-account</li> <li>https://jilt.com/blog/beginners-guide-to-payment-processing/</li> <li>https://www.comalytics.com/e-commerce-payment-gateways/</li> <li>https://www.quora.com/What-are-the-best-resources-to-learn-about-the-intricacies-of-payments-processing</li> </ul>"},{"location":"05-pubsub-google-clouds/","title":"05 pubsub google clouds","text":""},{"location":"05-pubsub-google-clouds/#why-pubsub","title":"Why Pub/Sub","text":"<p>Pub/Sub seems to be the zapier of Google Cloud Console. It connects senders of data, Publishers, with receivers of data, Subscribers. </p> <p>The is a fundamental problem to solve in any project involving streaming analytics.</p> <p>Why not use the zapier, instead? * Cost savings * Compatibility with GCP tools</p>"},{"location":"05-pubsub-google-clouds/#_1","title":"05 pubsub google clouds","text":"<p>The quick start guide shows how to create a topic, subscriber and publisher under a Google cloud project. This does not mention anything about connecting apps using APIs or webhooks.</p> <p>Another guide shows how to make a useless app using a number of python scripts and pubsub. </p> <p>Downloaded the Google Cloud SDK. After installation, ran the commands:</p> <pre><code>gcloud pubsub topics list\ngcloud pubsub subscriptions list\n</code></pre> <p>This document seems to be the most relevant one, but its pre-requisite is to build the Hello worlld cloud app.</p> <p>How to write the webhook / API endpoints to connect the apps? <code>https://YOUR_PROJECT_ID.REGION_ID.r.appspot.com/pubsub/push?token=YOUR_TOKEN \\</code></p> <p>Dataflow lets you create SQL Jobs and connect them with Pub/Sub topic. This becomes the publisher. NOt sure how to connect the subscriber with a webhook.</p> <p>Dataflow also has jupyter notebook that can connect with Cloud Storage.</p> <p>Data Studio BigQuery Table connection is also able to pull accurate order cout and sum of total. However, we need to find out how refunds are coming in Big Query.</p>"},{"location":"05-pubsub-google-clouds/#resources","title":"Resources","text":"<ul> <li>YouTube playlist</li> <li>YouTube playlist video 2</li> <li>Python library github</li> <li>Python library documentation</li> <li>Official documentation</li> </ul>"},{"location":"07-tmux/","title":"Tmux","text":"<p>Tmux is a popular terminal multiplexer that helps you be more productive in terminal.</p> <p>Installation</p> <pre><code>sudo apt install tmux\n</code></pre>"},{"location":"07-tmux/#resources","title":"Resources","text":"<ul> <li>Linuxize</li> <li>The missing semester</li> <li>Ham Vocke</li> <li>Screen and Tmux documentation</li> </ul>"},{"location":"08-using-system-clipboard-in-vim/","title":"08 using system clipboard in vim","text":"<p>If you are facing trouble in copying texts to and from VIM and another software, your VIM is not configured to access the system clipboard.</p>"},{"location":"08-using-system-clipboard-in-vim/#ubuntu-2004","title":"Ubuntu 20.04","text":"<p>You need to have the +clipboard feature turned on to access the system clipboard from VIM.</p> <p>In terminal, write <code>vim --version</code> and look for <code>+clipboard</code></p> <p>If it shows <code>-clipboard</code>, it means that the feature is not installed.</p> <p>I did <code>sudo apt-get vim-gtk</code> to upgrade vim and ensured clipboard feature is added.</p> <p>With this enabled, you should be able to use \"+y to copy and \"+p to paste. In my Ubuntu, Ctrl+Shift+V worked for pasting in Ubuntu.</p> <p>But \"+y is too many keys to press just for copying. I added the following in the .vimrc - <code>set clipboard=unnamedplus</code></p> <p>This makes default y access the clipboard like the \"+y combo.</p>"},{"location":"09-padi-openwater-sec1/","title":"09 padi openwater sec1","text":""},{"location":"09-padi-openwater-sec1/#introduction","title":"Introduction","text":"<ul> <li>5 sections relate to 5 key elements of training.</li> <li>PADI has courses to train for diving in new environments, for example - <ol> <li>exploring a wrekage</li> <li>diving with suits in cold water</li> </ol> </li> </ul>"},{"location":"09-padi-openwater-sec1/#section-1","title":"Section 1","text":""},{"location":"09-padi-openwater-sec1/#being-a-diver","title":"Being a diver","text":"<ul> <li>Every 10m/33ft, the water pressure increases by 1 bar/ata</li> <li>1 bar = 1 atmospheric pressure</li> <li>For example, 40m/132ft, 5 bar</li> <li>Formula is = (depth / 10) + 1 bar</li> <li>Volume is inversely proportional to pressure</li> <li>Squeeze is when water pressure increases and air volume decreases in - <ol> <li>Ears</li> <li>Sinuses</li> <li>Masks</li> </ol> </li> <li>Equalize pressure by pinching nose and blowing gently against it. </li> <li>Equalization adds more air, maintiaining the volume</li> <li>Equalize gently and every meter</li> <li>Never use ear plug while diving because they make it hard to equalize</li> <li>During ascend, decreasing pressure can cause lung rupture injuries</li> <li>To avoid, breathe at all times and never hold your breathe</li> <li>Reverse squeeze/block can happen if expanding air is trapped</li> <li>Never dive with any congestion</li> <li>Breathe slowly and gently, never stop</li> <li>Bouyancy control device (BCD) helps control bouyancy</li> <li>With more depth, bouyancy decreases or diver becomes negatively bouyant</li> <li>BCD needs to be adjusted every few meters and tested for neutrality</li> <li>Descend and ascend at a slow and controllable rate</li> <li>In recreational diving, there is always a buddy</li> </ul>"},{"location":"09-padi-openwater-sec1/#equipment-1","title":"Equipment 1","text":"<ul> <li>Manufacturers make multiple models of gears, visit local PADI shop for guidance</li> <li>Primary consideration for gears -<ol> <li>Suitability for the diver and the dive</li> <li>Fit is the size and adjustment for the diver</li> <li>Comfort means the diver can use it without issues</li> </ol> </li> <li>Fit the mask strap above ears, over the crown of head</li> <li>Snorkel goes on the left side because regulator is on the right side</li> <li>Adjustible strap fins are often used with wet suit boots</li> <li>Hand fins are called webbed gloves</li> <li>DPV or Diver Propulsion Vehicles are scooters under water</li> </ul>"},{"location":"09-padi-openwater-sec1/#scuba-kit","title":"Scuba Kit","text":"<ul> <li>Scuba kit contains four items -<ol> <li>BCD<ul> <li>Holds kit together</li> <li>Allows for adjusting bouyancy</li> </ul> </li> <li>Regulator<ul> <li>Delivers breathing air at surrounding pressure</li> <li>Directs exhale air into the water</li> </ul> </li> <li>Cyinder<ul> <li>Holds air in high pressure</li> </ul> </li> <li>Weight system<ul> <li>Holds lead weight</li> <li>Mechanism to drop in an emergency</li> </ul> </li> </ol> </li> <li>Its useful to buy package of all four items together</li> </ul>"},{"location":"09-padi-openwater-sec1/#bcd","title":"BCD","text":"<ul> <li>Elements of BCD -<ol> <li>Inflatable bladder</li> <li>Cylinder band and harness/jacket</li> <li>Low pressure inflator (LPI) mechanism - a button to inflate, deflate, manually inflate</li> <li>Overpressure / quick exhaust valves or Quick dump valves</li> <li>Weight system</li> </ol> </li> <li>Options in BCD - <ol> <li>Bouyancy capacity or lift</li> <li>Pockets and D-rings</li> <li>Soulder quick release</li> </ol> </li> <li>BCDs can be stylish</li> <li>BCD should have enough capacity to lift diver and equipments to surface</li> </ul>"},{"location":"09-padi-openwater-sec1/#regulators","title":"Regulators","text":"<ul> <li>Parts of the regulator<ol> <li>Hub - The first stage is the \u201chub\u201d of your regulator. It is a simple and reliable device that supplies air to all the other components. It connects to the cylinder valve either by screwing into it, or with a yoke (clamp system). Either way, an o-ring forms the air-tight seal between the first stage and the cylinder valve. The first stage reduces cylinder pressure to an intermediate pressure, which is 7-10 bar/100-150 psi above the surrounding pressure.</li> <li>Second stage - You breathe from the second stage. It reduces the first stage intermediate pressure to the pressure around you and delivers air only when you inhale \u2013 on demand. It has one-way valves that vent your exhalation. The purge button lets you manually release air from your cylinder.</li> <li>Alternate air source - The alternate air source (or \u201coctopus\u201d) is an extra second stage you use for sharing air with a buddy should the need arise. The most popular alternate air source is a standard second stage on a longer hose. If needed, you pass it to your buddy to share air. Many are brightly colored so a buddy can locate them easily.</li> <li>Low pressure inflator - The low-pressure inflator (LPI) hose is the hose that supplies air to your BCD inflator. When diving with a dry suit, you have two. You use the second to add air to the suit as you descend.</li> <li>SPG - The Submersible Pressure Gauge (SPG) indicates the air pressure remaining in the cylinder. </li> </ol> </li> <li>Regulator options include -  Yoke or DIN \u2013 The yoke system holds the first stage to the cylinder with a clamp system. With the DIN system, the regulator threads into the valve. The yoke system is older and more widely established, while the DIN system \u2013 which is growing in popularity \u2013 is widely used in Europe and Asia, and has a higher pressure rating. Your instructor or dive shop can advise you which system is the most popular where you are, but you can be ready for both systems. To do this, a common option is to choose a DIN regulator with a yoke adapter, useable on either type valve. Adjustable second stage \u2013 A knob allows small air flow adjustments; this option lets you keep the regulator breathing its best over the course of its maintenance cycle.</li> </ul> <p>Dive/Predive switch \u2013 This switch reduces freeflow (air released without control) when the second stage isn\u2019t in your mouth.</p> <p>Cold-water first stage \u2013 In cooler climates, the first stage can freeze, resulting in a freeflow. Special cold-water regulators reduce the likelihood of this by surrounding the first stage with a special liquid. * Regulators require professional overhaul as set by the manufacturers</p>"},{"location":"09-padi-openwater-sec1/#cylinders","title":"Cylinders","text":"<ul> <li>Have markings saying date of last visual inspection and other details</li> <li>Cylinders with an Enriched Air Nitrox (EANx) or Nitrox have more oxygen which let longer dive. PADI Enriched Air Diver course teaches how to use these.</li> <li>PADI TecRec program trains technical diving with advanced gears.</li> <li>Burst disk is safety device that releases air before cylinder fails.</li> <li>Cylinders require pressure testing every few years and annual visual testing.</li> </ul>"},{"location":"09-padi-openwater-sec1/#weight-system","title":"Weight system","text":"<ul> <li>Right distribution of weight is called trim. Trim is your orientation and balance in the water \u2013 generally, the desired trim is a natural horizontal swimming position with your feet parallel to the bottom or slightly elevated.</li> <li>Some BCDs have nonreleasable weight pockets. You use these for trim, but you should have enough releasable weight to assure positive buoyancy.</li> </ul>"},{"location":"09-padi-openwater-sec1/#diver-skills-1","title":"Diver Skills 1","text":"<ul> <li>Defog the mask</li> <li>Setting up the scuba kit</li> <li>Gearing up with buddies</li> <li>Inflating and deflating the BCD<ul> <li>It is used constantly</li> <li>Inflate before going to water </li> <li>Practise breathing and fine-tuning bouyancy together</li> </ul> </li> <li>Breathing underwater <ul> <li>Slowly and deeply</li> <li>Never stop</li> </ul> </li> <li>Hand signals</li> <li>Regulator clearing <ul> <li>Exhalation method</li> <li>Purge method</li> </ul> </li> <li>Regulator recovery</li> <li>Clearing water out of mask - hold the top and blow through nose</li> <li>Managing air supply</li> <li>Descending and equalizing - Equalize gently and often \u2013 every metre/few feet.</li> <li>Swimming underwater</li> <li>Alternate Air Source (AAS) use</li> <li>Ascents <ul> <li>Swim up</li> <li>Breathe normally</li> <li>Vent expanding air from BCD in small amounts</li> </ul> </li> <li>Emergency weight drop</li> <li>BCD oral inflation at surface</li> <li>Exiting the water</li> <li>After the dive</li> </ul>"},{"location":"10-padi-openwater-sec2/","title":"10 padi openwater sec2","text":""},{"location":"10-padi-openwater-sec2/#being-a-diver-2","title":"Being a diver 2","text":""},{"location":"10-padi-openwater-sec2/#seeing-and-hearing","title":"Seeing and hearing","text":"<ul> <li>Things appear closer than they are</li> <li>Colors are lost at different depths </li> <li>Hard to tell the direction of sound under water</li> <li>Sound travels 4x speed of air</li> </ul>"},{"location":"10-padi-openwater-sec2/#swimming-and-moving","title":"Swimming and moving","text":"<ul> <li>Relax, don't rush</li> <li>Trim properly for streamlined swim</li> </ul>"},{"location":"10-padi-openwater-sec2/#staying-warm","title":"Staying warm","text":"<ul> <li>Dry suit vs wet suit</li> <li>Hypothermia - chill and shivering</li> <li>When uncomfortable, end the dive</li> </ul>"},{"location":"10-padi-openwater-sec2/#breathing-underwater","title":"Breathing underwater","text":"<ul> <li>Air is denser underwater, takes time to breathe</li> <li>Dead Air Space is air filled spaces out of lungs, where respiration isnt happening. </li> <li>The first air you inhale (\u201cdead\u201d air) with each breath is the air from your last exhalation, so it is higher in carbon dioxide.</li> <li>Breathing slowly and deeply reduces the proportion of dead air in each breath.</li> <li>Airway control is the skill of breathing past the residual water in regulator / snorkel<ul> <li>Inhale slowly</li> <li>Tough tongue to the roof to stop water below</li> <li>Look a bit downward</li> </ul> </li> <li>Overexertion symptoms<ul> <li>Fatigue</li> <li>Labored breathing</li> <li>A feeling of suffocation or air starvation</li> <li>Weakness</li> <li>Anxiety</li> <li>Headache</li> <li>Muscle cramping</li> <li>A tendency to panic</li> </ul> </li> <li>Always breath slowly and deeply. If not, stop to relax or abort dive.</li> <li>Peak Performance Buoyancy course trains about controlling bouyancy using center to gravity, center of bouyancy, streamlined body position and breathing techniques.</li> </ul>"},{"location":"10-padi-openwater-sec2/#managing-air-pressure-with-buddy","title":"Managing air pressure with buddy","text":"<ul> <li>Calculate turn pressure - the SPG pressure at which divers turn around</li> </ul>"},{"location":"10-padi-openwater-sec2/#swimming-at-the-surface","title":"Swimming at the surface","text":"<ul> <li>Use mask, snorkel</li> </ul>"},{"location":"10-padi-openwater-sec2/#descent-in-open-water","title":"Descent in open water","text":"<ul> <li>Five steps of descent<ol> <li>Confirm that your buddies are ready.</li> <li>Orient yourselves to something at the surface or underwater, such as the boat or a landmark.</li> <li>Switch from your snorkel to your regulator.</li> <li>Check, and if necessary activate, your dive computer or timer. </li> <li>Signal \u201cdescend\u201d and, with your buddies, slowly deflate your BCD.</li> </ol> </li> <li>Begin equalizing immediately, as soon as your head goes underwater.</li> <li>You descend faster as you go deeper. Add to BCD to compensate, in small ammounts and frequently.</li> <li>Ideally, you rech target depth with neutral bouyancy</li> </ul>"},{"location":"10-padi-openwater-sec2/#ascent-in-open-water","title":"Ascent in open water","text":"<ul> <li>Five steps of ascent<ol> <li>Signal \u201cup\u201d and confirm that your buddies are ready.</li> <li>Check your dive computer to be sure you\u2019re within its limits.</li> <li>Look up and hold up your BCD deflator hose. Do not add air to your BCD. If you\u2019re properly weighted and neutrally buoyant, you only need to start swimming up gently.</li> <li>Ascend slowly \u2013 no faster than your dive computer\u2019s maximum rate.<ul> <li>Release air expanding in your BCD to control your buoyancy so you don\u2019t start to rise too fast.</li> <li>Ascend no faster than 3 metres/10 feet each 10 seconds</li> </ul> </li> <li>Look up and turn as you ascend</li> </ol> </li> <li>Ascend without reference and navigating <ul> <li>One common technique is for one buddy to navigate while the other controls the ascent. </li> </ul> </li> <li>Safety stops are normal part of ascend for prudent divers.<ul> <li>It is the pause in ascent between 6 metres/20 feet and 3 metres/10 feet (commonly 5 metres/15 feet) for three to five minutes.</li> <li>Helps avoid decompression sickness (DCS)</li> </ul> </li> <li>Keep your hand up as you break the surface. </li> <li>Continue to breathe from your regulator as you inflate your BCD. After making sure you\u2019re floating comfortably, switch to your snorkel.</li> </ul>"},{"location":"10-padi-openwater-sec2/#equipment-2","title":"Equipment 2","text":""},{"location":"10-padi-openwater-sec2/#exposure-suits-1","title":"Exposure suits 1","text":"Wet suits Dry suits Skin suits 10C to 30C Below 18C No insulation 50F to 86F Below 65F <ul> <li>Gloves and hoods for cold water</li> </ul>"},{"location":"10-padi-openwater-sec2/#cutting-tools","title":"Cutting tools","text":"<ul> <li>Four types<ul> <li>Dive knife</li> <li>Dive tool</li> <li>Shears</li> <li>Z knife</li> </ul> </li> <li>Mounting in accesible positions</li> </ul>"},{"location":"10-padi-openwater-sec2/#dive-instruments","title":"Dive instruments","text":"<ul> <li>SPG</li> <li>Compass</li> <li>Dive watches</li> <li>Dive computer</li> <li>Thermometer</li> </ul>"},{"location":"10-padi-openwater-sec2/#skills-as-diver-2","title":"Skills as diver 2","text":""},{"location":"10-padi-openwater-sec2/#deep-water-entry-giant-stride","title":"Deep water entry - giant stride","text":"<ul> <li>Enter the water fully equipped with a partially inflated BCD.</li> <li>Remember to breathe from your regulator and hold it, and your mask, as you enter. Hold them until you\u2019re stable on the surface.</li> <li>Look straight ahead and step in \u2013 don\u2019t jump.</li> <li>Signal that you\u2019re \u201cokay\u201d and clear the entry area.</li> <li>Switch to your snorkel.</li> </ul>"},{"location":"10-padi-openwater-sec2/#weight-check","title":"Weight check","text":"<ul> <li>You should float at eye level with an empty BCD and holding a normal breath. When you exhale, you should slowly sink.</li> <li>After adjusting so you float at eye level, add two kilograms/five pounds if you checked with a full cylinder. This is because the air in it has weight, so you will become lighter as you use up your air. Two kilograms/five pounds adjusts for the air in a typical cylinder.</li> </ul>"},{"location":"10-padi-openwater-sec2/#dealing-with-loose-cylinder-band","title":"Dealing with loose cylinder band","text":"<ul> <li>If it comes loose while exiting, it is often simplest to ease back into the water, slip out of the kit and then lift or carry it out of the water by the cylinder valve.</li> <li>If it comes loose before the dive, help the diver remove the scuba kit for retightening.</li> <li>If it comes loose during a dive, it is not usually an emergency, because the cylinder tends to stay with the diver. Signal the affected diver so you can tighten it, or signal your buddy if it is your cylinder band that\u2019s loose.</li> </ul>"},{"location":"10-padi-openwater-sec2/#snorkelregulator-exchange","title":"Snorkel/Regulator exchange","text":"<ul> <li>Swap, clear water, breath slowly.</li> <li>Practice until effortless.</li> </ul>"},{"location":"10-padi-openwater-sec2/#neutral-bouyancy","title":"Neutral bouyancy","text":"<ul> <li>Fin pivot - pivot up fin when inhaling, drop when exhaling</li> <li>Adjust by adding </li> </ul>"},{"location":"10-padi-openwater-sec2/#mask-removal-replacement-and-no-mask-breathing","title":"Mask removal, replacement and no mask breathing","text":"<ul> <li>Clear a fully flooded mask just as you do a partially flooded one; you just exhale a little longer.</li> <li>Train for a minute without mask to simulate the time of getting back to surface.</li> </ul>"},{"location":"10-padi-openwater-sec2/#disconnect-low-pressure-inflator","title":"Disconnect Low-pressure Inflator","text":"<ul> <li>An unlikely but possible malfunction is for your BCD (or dry suit) inflator to stick. Your response is to disconnect the hose supplying air to it</li> <li>If disconnecting is taking time, get in a vertical position and hold open the exhaust valve while continuing to disconnect the inflator hose.</li> <li>For colder water diving, you can get low-pressure hoses with oversized release fittings to make this easier while wearing gloves.</li> </ul>"},{"location":"10-padi-openwater-sec2/#air-gas-depletion-exercise","title":"Air (Gas) Depletion exercise","text":"<p>This exercise lets you experience what it feels like to run out of air, which is part of learning to respond appropriately.</p>"},{"location":"10-padi-openwater-sec2/#air-awareness-and-managin-air-supply","title":"Air awareness and managin air supply","text":"<ul> <li>Besides knowing how to read your SPG and plan your air use, develop the habit of knowing approximately how much air you have at all times.</li> <li>Should be able to answer within 20 bar/300 psi without rechecking.</li> </ul>"},{"location":"10-padi-openwater-sec2/#deep-water-exit","title":"Deep water exit","text":"<ul> <li>Switch back to your regulator before exiting.</li> </ul>"},{"location":"11-padi-openwater-sec3/","title":"11 padi openwater sec3","text":""},{"location":"11-padi-openwater-sec3/#introduction","title":"Introduction","text":"<p>This section is about planning the dive considering environment, personal limits and acquatic conditions.</p>"},{"location":"11-padi-openwater-sec3/#dive-environments-and-conditions","title":"Dive environments and conditions","text":"<ul> <li>Six conditions affect divers -<ol> <li>Temperature</li> <li>Visibility</li> <li>Water movement</li> <li>Bottom composition</li> <li>Aquatic life</li> <li>Sunlight</li> </ol> </li> </ul>"},{"location":"11-padi-openwater-sec3/#temperature","title":"Temperature","text":"<ul> <li>Climate and depth affects temperature</li> <li>It is important to base exposure protection on the temperature at the deepest part of the dive</li> </ul>"},{"location":"11-padi-openwater-sec3/#visibility","title":"Visibility","text":"<ul> <li>Defined by how far you can see a horizontal diver</li> <li>Ranges from 0 to 60m</li> <li>Factors affecting visibility<ol> <li>Water current </li> <li>Weather</li> <li>Plankton</li> <li>Nature and composition of particles in water - heavy particles sediment faster than lighter ones like mud and clay.</li> </ol> </li> <li>Diving in reduced visibility<ol> <li>Difficult to stay close to buddy</li> <li>Losing orientation while navigating</li> <li>Losing orientation while ascending or descending</li> </ol> </li> <li>PADI Underwater Navigator and Search and Recovery Diver courses</li> <li>Diving in clear water - stay within 2 sec of buddies  </li> </ul>"},{"location":"11-padi-openwater-sec3/#water-movement","title":"Water movement","text":"<ul> <li>Wave or current</li> <li>PADI Drift diving course</li> <li>Only swim against weak current</li> <li>Particularly at surface, swim perpendicular to the current</li> <li>Swim against the current to the diving location so you can return along the current</li> </ul>"},{"location":"11-padi-openwater-sec3/#bottom-composition","title":"Bottom composition","text":"<ul> <li>Types -<ol> <li>Silt/Mud</li> <li>Sand</li> <li>Rock</li> <li>Coral</li> <li>vegetation</li> </ol> </li> </ul>"},{"location":"11-padi-openwater-sec3/#sunlight","title":"Sunlight","text":"<ul> <li>Sunburn is the most common diving injury</li> <li>Sunscreens that do to affect aquatic life</li> </ul>"},{"location":"11-padi-openwater-sec3/#fresh-water-vs-salt-water","title":"Fresh water vs salt water","text":"<ul> <li>Less bouyant in fresh water</li> <li>Thermoclines are more distinct in fresh water</li> </ul>"},{"location":"11-padi-openwater-sec3/#overhead-environments","title":"Overhead environments","text":"<ul> <li>Places where divers can swim into but cannot swim up to the surface</li> <li>Requires special training</li> <li>This course is open water diving - where the diver can directly go to surface</li> </ul>"},{"location":"11-padi-openwater-sec3/#assessing-conditions","title":"Assessing conditions","text":"<ul> <li>Assess conditions based on - <ul> <li>The weather</li> <li>The season</li> <li>Water motion</li> <li>Water appearance</li> <li>Reports online and from other divers</li> <li>Dives made at similar sites in the area</li> <li>Experience</li> </ul> </li> <li>Diver should make the decision to continue with the dive or not</li> </ul>"},{"location":"11-padi-openwater-sec3/#dive-with-limits","title":"Dive with limits","text":"<ul> <li>Don't exceed the limits set by diver's experience and training</li> <li>PADI Advanced Open Water Diver course</li> </ul>"},{"location":"11-padi-openwater-sec3/#acquatic-life","title":"Acquatic life","text":"<ul> <li>Active vs passive interaction</li> </ul>"},{"location":"11-padi-openwater-sec3/#hazardous-organisms","title":"Hazardous organisms","text":"<ul> <li>Stings or punctures -<ul> <li>Jelly fish</li> <li>Sting rays</li> <li>Sea urchins</li> <li>Portuguese man-o-war</li> <li>Lion fish and scorpion fish</li> <li>Cone shells</li> <li>Fire coral and other hydroids</li> <li>Sea nettles</li> </ul> </li> <li>Bites <ul> <li>Moray eels</li> <li>Barracida and other fish</li> <li>Clawed lobsters/crabs</li> <li>Trigger fish</li> <li>Crocodilians</li> <li>Some sharks</li> <li>Snakes</li> <li>Octopus</li> </ul> </li> <li>Avoid wearing dangling, shiny jewelries </li> <li>Avoid dead body parts - jellyfish or its body parts can sting even when dead</li> <li>Managing aquatic injury<ul> <li>Assure breathing</li> <li>While diving in salt water, rinse stings with salt water</li> <li>Don't rub stings</li> <li>Vinegar on jellyfish, fire coral and other hydroid stings</li> <li>Immerse stings from fish spines, scorpion/lion fish and stingrays in hot &lt;50C/120F water</li> <li>Use forcepts to remove spines</li> <li>Treat bites like an wound</li> <li>PADI Rescue Diver, Emergency First Response Primary Care (CPR) and Secondary Care (First Aid) courses</li> </ul> </li> <li>Potentially aggressive animals<ul> <li>Watch and enjoy</li> <li>This is rare</li> <li>Don't swim towards</li> <li>If uncomfortable, slowly swim away while keeping an eye on it</li> </ul> </li> </ul>"},{"location":"11-padi-openwater-sec3/#aquatic-plant-hazards","title":"Aquatic plant hazards","text":"<ul> <li>Keep the kit streamlined</li> <li>Avoid dense areas</li> <li>Try swimming up instead of turning around if stuck</li> <li>Don't struggle, pull or twist</li> <li>Use knife or cutting tool if necessary</li> </ul>"},{"location":"11-padi-openwater-sec3/#project-aware","title":"Project AWARE","text":"<ul> <li>Non-profit organization of scuba divers to protect the environment</li> </ul>"},{"location":"11-padi-openwater-sec3/#diving-from-shore","title":"Diving from shore","text":"<ul> <li>Typically, one puts on all equipment except masks, fins, snorkel and gloves, then conducts the safety check</li> <li>Keep your mask on the surface</li> <li>Walk till chest depth then the BCD can support, and wear the fin</li> <li>Wear the fin in the beginning and walk backwards</li> <li>Ascend or descend along the line from the surface float that divers can tow</li> </ul>"},{"location":"11-padi-openwater-sec3/#shore-diving-through-mild-surf","title":"Shore diving through mild surf","text":"<ul> <li>Some shore diving involves entering and exiting through breaking waves - surf</li> <li>Surf or the surf zome is the area in which waves break</li> <li>Waves break in shallow areas because the bottom stops the water, making the top \"tip over\" and break</li> <li>If waves break along a continuous line but with significant gap, its a channel or a rip current</li> <li>Surge and undertow - avoid</li> </ul>"},{"location":"11-padi-openwater-sec3/#longshore-current","title":"Longshore current","text":"<ul> <li>Waves approaching shore at an angle causes a current parallel to the shoreline   add the image</li> <li>Plan the exit considering the current</li> </ul>"},{"location":"11-padi-openwater-sec3/#rip-current","title":"Rip current","text":"<ul> <li>Occurs when waves push water over a long obstruction, such as reef or sandbar  add the image</li> <li>Can be very strong, so don't swim against it</li> <li>Establish bouyancy and swim parallel to shore until out of the current</li> </ul>"},{"location":"11-padi-openwater-sec3/#upwelling","title":"Upwelling","text":"<ul> <li>Wind from shore can push surface water away bringing cooler deeper water to the top </li> </ul>"},{"location":"11-padi-openwater-sec3/#tides","title":"Tides","text":"<ul> <li>Differences between high and low tides can be from unoticable to 6m/20ft in some areas</li> <li>Tide affects three environmental factors related to diving - <ol> <li>Currents</li> <li>Depth</li> <li>Visibility</li> </ol> </li> <li>Tides are regional. Ask, research or look at tide tables </li> </ul>"},{"location":"11-padi-openwater-sec3/#diving-from-boats-preparations","title":"Diving from boats - preparations","text":"<ul> <li>Avoid seasickness</li> <li>Bow, stern, port, starboard, windward and leeward</li> </ul>"},{"location":"11-padi-openwater-sec3/#diving-from-boats-procedures","title":"Diving from boats - procedures","text":"<ul> <li>Roll calls</li> <li>Dive site briefing</li> <li>Recall signal and procedure</li> <li>Dive planning includes getting in and getting out of the water</li> <li>Stay where visible to crew of the boat</li> <li>Do not swim just below the surface</li> </ul>"},{"location":"11-padi-openwater-sec3/#boat-diving-exit-procedure","title":"Boat diving exit procedure","text":"<ul> <li>Stay out from underneath divers climbing th ladder</li> <li>Hang on to the rope when there is a current and wait for turn</li> <li>Switch from snorkel to regulator while climbing</li> <li>Hand up equipments, like camera</li> <li>Hold ladder, remove fins</li> <li>Breathe on regulator and keep masks on until all the way up to the deck</li> </ul>"},{"location":"11-padi-openwater-sec3/#boat-diving-with-moderate-current","title":"Boat diving with moderate current","text":"<ul> <li>The trail line</li> <li>The swim line</li> <li>The moor/anchor line</li> </ul>"},{"location":"11-padi-openwater-sec3/#dive-boat-not-in-sight","title":"Dive boat not in sight","text":"<ul> <li>Stay calm</li> <li>Use surface signalling devices</li> <li>Wait, or swim towards safety, if within reasonable distance, slowly</li> </ul>"},{"location":"11-padi-openwater-sec3/#dive-planning","title":"Dive planning","text":"<ul> <li>Advance planning - deciding to go on the dive<ul> <li>Buddy</li> <li>Dive site</li> <li>Objective</li> <li>Logistics</li> <li>Check dive conditions - weather, surf, tides, etc.</li> </ul> </li> <li>Preparation planning<ul> <li>Inspect gear</li> <li>Cylinder filled</li> <li>Recheck gear</li> <li>Pack</li> </ul> </li> <li>Last minute preparation<ul> <li>Recheck weather, surf, conditions</li> <li>Create a safety contact</li> </ul> </li> <li>Predive planning <ul> <li>Actual dive plan</li> <li>Evaluate the conditions</li> <li>Decide whether to continue on diving</li> <li>Agree on technique<ul> <li>where/how to enter</li> <li>the course to follow</li> <li>techniques during the dive</li> <li>where/how to exit</li> </ul> </li> <li>Review signals and communication</li> <li>Buddy separation procedures</li> <li>Time, depth and air supply limits</li> <li>Discuss emergency procedures</li> </ul> </li> </ul> <p>Example dive planning slate. </p>"},{"location":"12-padi-openwater-sec3-problem-management/","title":"12 padi openwater sec3 problem management","text":""},{"location":"12-padi-openwater-sec3-problem-management/#prevention","title":"Prevention","text":"<ul> <li>Recommended courses<ol> <li>Advanced open water diver course </li> <li>Rescue diver course</li> <li>Emergency first response primary and secondary care courses</li> <li>Emergency oxygen provider course</li> </ol> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#surface-problem-management-responsive-diver","title":"Surface problem management - Responsive diver","text":"<ul> <li>Most diver-in-distress situations occur at the surface</li> <li>Establish positive buoyancy on the surface<ul> <li>inflate BCD, or</li> <li>drop weights</li> </ul> </li> <li>Common surface problems<ul> <li>overexertion</li> <li>leg muscle cramps</li> <li>choking on inhaled water</li> </ul> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-a-responsive-diver-at-surface","title":"Assisting a responsive diver at surface","text":"<ul> <li>A diver who is breathing, alert and active is considered responsive.</li> <li>Two types <ul> <li>in control / not panicked</li> <li>out of control / panicked</li> </ul> </li> <li>Four steps of helping <ol> <li>Establish buoyancy for yourself and the diver. If you must make contact to help, make yourself buoyant before doing so, then inflate the victim\u2019s BCD and/or drop weights after you make contact.</li> <li>Calm the diver by reassuring, offering encouragement and asking the person to relax.</li> <li>Help the diver reestablish breathing control. </li> <li>As necessary, assist the diver to the boat or shore. Even divers who don\u2019t panic may need assistance due to leg cramps or being overly tired.</li> </ol> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#surface-problem-management-unresponsive-diver","title":"Surface problem management - Unresponsive diver","text":"<ul> <li>A diver may be unresponsive or unconscious for one of the following reasons<ul> <li>inhaling water</li> <li>extreme fatigue</li> <li>heart attack</li> <li>lung over expansion injuries</li> <li>panic</li> <li>inefficient breathing</li> <li>throat blockage</li> <li>exhaustion, among others</li> </ul> </li> <li>An unresponsive diver does not move and does not respond when tapped or spoken to.</li> <li>Confirm responsiveness if a diver floats without moving.</li> <li>A diver who surfaces alone, calls for help or shows signs of panic, then stops moving, should also be considered unresponsive until you confirm otherwise.</li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-at-surface","title":"Assisting unresponsive diver at surface","text":"<ul> <li>Four steps<ol> <li>Establish buoyancy</li> <li>Call of help</li> <li>Check for breathing</li> <li>Continue rescue breathing during the tow</li> </ol> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#underwater-problem-management","title":"Underwater problem management","text":"<ul> <li>Three ways to avoid -<ol> <li>Relax when diving</li> <li>Plan air usage and monitor closely</li> <li>Dive withing limits of experience and training</li> </ol> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#overexertion","title":"Overexertion","text":"<ul> <li>Symptoms -<ul> <li>feeling of air starvation</li> <li>exhausted</li> </ul> </li> <li>Steps - <ul> <li>Signal \u201cstop\u201d or \u201chold\u201d and indicate that you need to rest.</li> <li>Continue at a reduced pace after you recover.</li> <li>If you can\u2019t return to a relaxed state, end the dive.</li> <li>If conditions are adding to overexertion, it may be best to abort the dive.</li> </ul> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#freeflowing-regulator","title":"Freeflowing regulator","text":"<ul> <li>If regulators fail, they release air continuously (called a freeflow).</li> <li>Do not seal your mouth on the mouthpiece.</li> <li>Hold the second stage in your hand and press the mouthpiece outside your lips. Let the excess air escape freely. You may insert only one end of the mouthpiece into your mouth if it helps.</li> <li>Breathe the air you need by \u201csipping\u201d it,</li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#entanglement","title":"Entanglement","text":"<ul> <li>Prevent by -<ul> <li>Moving slowly</li> <li>Watching where you are going</li> <li>Keeping equipment streamlined</li> </ul> </li> <li>Do not turn or twist.</li> <li>Extreme cases might require having to slip out of scuba kit and putting it back on.</li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#running-low-out-of-air-underwater","title":"Running low / out of air underwater","text":"<ul> <li>Ascend</li> <li>Ascending with enough air allows you to control your ascent and to make a safety stop.</li> <li>Four options -<ol> <li>Make a normal ascent<ul> <li>Do this if you\u2019re very low on air (you feel some breathing resistance), but your cylinder isn\u2019t completely empty.</li> <li>As you ascend, you can get more air from your cylinder, because the surrounding water pressure decreases.</li> <li>Breathe lightly (but continuously) and make a controlled, continuous ascent to the surface.</li> <li>Do not attempt a safety stop.</li> </ul> </li> <li>Ascend using an alternate air source<ul> <li>Think of this as your best, all around choice when you have an alternate air source immediately available.</li> <li>During your predive safety check, confirm what alternate air sources your buddies have and how to secure them. Test breathe your alternate air source(s).</li> <li>To use an alternate air source supplied by a buddy, you need to stay close to your buddy(ies).</li> </ul> </li> <li>Controlled Emergency Swimming Ascent (CESA)<ul> <li>This is the best choice if you were completely out of air, no deeper than approximately 6 to 9 metres/20 to 30 feet, the surface is closer than your buddy(ies) or another diver, and you have no other alternate air source.</li> <li>Simply look up and swim to the surface making a continuous \u201cahhhhh\u201d sound into your regulator. The \u201cahhhhh\u201d sound assures that you exhale expanding gas, which is necessary to avoid lung overexpansion injury.</li> <li>Leave all your gear in place and keep the regulator in your mouth. Do not drop your weights to start your ascent.</li> <li>Ascend at a safe rate. The ascent gets easier as you ascend because air expanding in your BCD increases your buoyancy. Vent air as needed to maintain a proper rate.</li> </ul> </li> <li>Bouyant Emergency Ascent<ul> <li>Use this option when you are too far from your buddy(ies) or another diver, have no other alternate air source and are so deep that you doubt you can reach the surface any other way.</li> <li>You make a buoyant emergency ascent exactly like a Controlled Emergency Swimming Ascent, except you ditch your weights and exceed a safe rate.</li> <li>Again, look up and make the \u201cahhhhh\u201d sound as you ascend.</li> <li>Because you exceed a safe rate, this method has more risk than the other options (which is why it is your last choice), but is obviously better than staying on the bottom without air.</li> <li>As you near the surface, you can flare out your arms and legs to create drag and slow your ascent.</li> </ul> </li> </ol> </li> <li>You can\u2019t inflate your BCD by using the low-pressure inflator, because you have an empty cylinder. Instead, inflate your BCD orally and/or drop your weights.</li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-underwater","title":"Assisting unresponsive diver underwater","text":"<ul> <li>Priority is to get the diver to surface</li> <li>Procedure -<ol> <li>Swim the diver to the surface. If necessary, use the diver\u2019s BCD and/or drop weights to make the victim buoyant.</li> <li>If the diver\u2019s regulator is in the mouth, hold it there. If it is not, don\u2019t waste time trying to replace it.</li> <li>Ascend at a safe rate. If the diver\u2019s buoyancy becomes too great for a safe ascent rate, let the victim go. Finish a safe ascent and resume the rescue at the surface. Keep yourself safe. You can\u2019t help someone else if you have troubles of your own.</li> <li>At the surface, follow the priorities and procedures for an unresponsive diver at the surface.</li> </ol> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#first-responded-care-for-diving-related-emergencies","title":"First responded care for diving-related emergencies","text":"<ul> <li>A diver who is or was unresponsive should be considered serious medical emergencies.</li> <li>Drowning or otherwise becoming unresponsive underwater, pressure-related injuries and medical conditions not directly related to diving (like heart attack) can cause these signs (what you observe) and symptoms (what the victim feels):<ul> <li>difficulty breathing</li> <li>unconsciousness</li> <li>unclear thinking</li> <li>visual problems</li> <li>paralysis</li> <li>chest pain</li> <li>lowered alertness</li> <li>cardiac and respiratory arrest</li> </ul> </li> </ul>"},{"location":"12-padi-openwater-sec3-problem-management/#assisting-unresponsive-diver-out-of-water","title":"Assisting unresponsive diver out of water","text":"<ul> <li>General steps<ul> <li>Keep the diver\u2019s airway open and check for breathing.</li> <li>Provide rescue breathing or CPR as necessary.<ul> <li>Do not use abdominal thrusts unless you are unable to provide rescue breathing due to a suspected obstruction.</li> <li>Inhaled water, if present, does not prevent rescue breaths. You don\u2019t have to try to clear it.</li> </ul> </li> <li>If the diver is unresponsive but breathing, keep the diver lying level on the left side (recovery position). This position is not more important than transporting the diver to safety or providing rescue breaths or CPR if necessary.</li> <li>Check the diver\u2019s breathing frequently.</li> <li>If the diver has regained responsiveness, keep the diver lying down comfortably.</li> <li>Administer emergency oxygen as soon as possible.</li> <li>Keep the diver still and maintain a normal body temperature by protecting from heat or cold.</li> <li>Continue to provide care until emergency medical care arrives.</li> <li>If you can\u2019t accompany the diver to medical care, write down as much background information as possible about the individual and the dive, and attach it to the diver in a conspicuous place.<ul> <li>Provide only information relevant to care, such as the dive profile, emergency care provided, emergency contact information, and any known medical conditions.</li> <li>Write only facts. Do not speculate or guess \u2013 bad information is worse than no information.</li> </ul> </li> </ul> </li> <li>Some conditions, such as drowning, can have delayed serious, potentially fatal, consequences hours after the incident. </li> </ul>"},{"location":"13-padi-openwater-sec3-equipment3/","title":"13 padi openwater sec3 equipment3","text":""},{"location":"13-padi-openwater-sec3-equipment3/#introduction","title":"Introduction","text":"<p>Signaling devices, floats and dive flags.</p>"},{"location":"13-padi-openwater-sec3-equipment3/#surface-signaling-devices","title":"Surface signaling devices","text":"<ul> <li>Diver should carry two while diving<ul> <li>audible</li> <li>visual</li> </ul> </li> <li>Make them a standard part of the scuba kit</li> <li>Examples <ul> <li>whistles - normally attached with BCD inflator</li> <li>low pressure horns - they use cylinder air and louder. should have the whistle if cylinder is empty</li> <li>inflatable signal tube</li> <li>signal mirrors</li> <li>delayed surface marker buoys</li> <li>signal lights and flashers</li> </ul> </li> </ul>"},{"location":"13-padi-openwater-sec3-equipment3/#floats-and-flags","title":"Floats and flags","text":"<ul> <li>Surface floats - carry with a reel or line caddy to avoid entanglement</li> <li>Flags are required by law in some places</li> <li>Traditional dive flag indicates divers below and boats should keep clear</li> <li>Alpha flag indicates that boat flying it has divers in the water and cannot maneuver </li> <li>The rule of thumb of distances from the flag </li> <li>Do not assume that all boats understand the flag</li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/","title":"14 padi openwater sec3 skills2","text":""},{"location":"14-padi-openwater-sec3-skills2/#deep-water-entry-seated-back-roll","title":"Deep water entry - seated back roll","text":"<ul> <li>Used when close to water</li> <li>Steps<ul> <li>Enter with your BCD partially inflated, holding your mask and breathing from your regulator.</li> <li>Tuck your chin toward your chest.</li> <li>Lean backward until your weight rolls you in.</li> <li>After you\u2019re stable on the surface, signal you\u2019re okay (assuming you are) and clear the area so your buddy(ies) can enter.</li> </ul> </li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#remove-and-replace-weights-at-the-surface","title":"Remove and replace weights at the surface","text":"<ul> <li>Important for adjusting weight in the water</li> <li>Steps<ul> <li>The technique varies with equipment \u2013 your instructor will help you.</li> <li>Be careful not to drop the weights.</li> <li>Give your weights to someone, put them on a platform or secure them to a line.</li> <li>Be sure you have enough buoyancy and breathe from your regulator before taking weights handed to you.</li> <li>It is best if buddies put weights on one at a time, so they can assist each other if necessary.</li> <li>With most integrated weight systems, you remove the weights as you would in an emergency weight drop. You replace them the same way you install them when setting up your kit, but with some types you may have to remove and replace the entire kit.</li> <li>With many systems, it helps to tilt back in the water, almost face up, so the weight pockets tend to slide into place.</li> <li>When removing a standard nylon web belt, be sure to hold the free end and release the buckle end so the weights can\u2019t slide off.</li> </ul> </li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#cramp-release","title":"Cramp release","text":"<ul> <li>Painful, involuntary muscle contractions</li> <li>Key points<ul> <li>If you have or start to have a cramp, signal your buddy and stop.</li> <li>Allow the muscle to rest. It may help to gently stretch and massage the cramped muscle.</li> <li>After relieving a cramp, rest for a few minutes before continuing at a slower pace.</li> <li>Dehydration, cold, restricted circulation and working a muscle beyond its fitness level can all cause cramping.</li> </ul> </li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#neutral-bouyancy","title":"Neutral bouyancy","text":"<ul> <li>Use breath control to fine-tune your buoyancy. If you start to rise a bit, exhale and breathe with a slightly lower lung volume. If you start to sink, inhale and breathe with a slightly higher lung volume for a moment.</li> <li>If you can\u2019t hover by adjusting with breathing, make small adjustments to your BCD.</li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#fine-tuning-trim","title":"Fine tuning trim","text":"<ul> <li>Trim is important for streamline</li> <li>Relax in midwater to check your natural position.</li> <li>Usually, you want trim for a normal swimming position.</li> <li>You want to feel balanced and stable, both side to side and front to back.</li> <li>Your instructor will help you reposition gear and weight for good trim.</li> <li>You will learn to fine-tune your trim from one dive to the next.</li> <li>Ask your instructor about the PADI Peak Performance Buoyancy course to learn more about mastering buoyancy and trim.</li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#air-depletionalternate-air-source","title":"Air depletion/Alternate air source","text":"<ul> <li>Simulates how it feels and how to respond to out-of-air-emergency</li> <li>Signal \"out of air\" or \"share air\"</li> <li>On surface, practice manual inflation of BCD because LPI is not available in an out-of-air emergency</li> <li>Keep using the alternate air source until bouyant</li> </ul>"},{"location":"14-padi-openwater-sec3-skills2/#controlled-emergency-swimming-ascent","title":"Controlled emergency swimming ascent","text":"<ul> <li>Take a breath and exhale it continuously making a loud \u201cahhhhh\u201d sound as you swim no faster than 18 metres/60 feet per minute for at least 9 metres/30 feet (a 30-second swim).</li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/","title":"15 padi openwater sec4 equipment4","text":""},{"location":"15-padi-openwater-sec4-equipment4/#introduction","title":"Introduction","text":"<p>Description of few more equipment.</p>"},{"location":"15-padi-openwater-sec4-equipment4/#mesh-utility-bag","title":"Mesh utility bag","text":"<ul> <li>Mesh helps it drain</li> <li>Made of nylon or any other synthetic material</li> <li>Hand carry the bad while diving so it can be easily dropped in case of an emergency</li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/#slates-and-wet-books","title":"Slates and wet books","text":"<ul> <li>Uses<ul> <li>communication</li> <li>note down dive plan</li> <li>record information during the dive, such as <ol> <li>found something for revisit,</li> <li>how much air is used to reach a destination, etc</li> </ol> </li> </ul> </li> <li>There are special masks that allow radio communication, and dive computers with texting feature</li> <li>Slates are reusable but wet books are not</li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/#dive-lights","title":"Dive lights","text":"<ul> <li>PADI Nightdiver course</li> <li>Specialized options for technical diving</li> <li>Can be useful even during the day</li> <li>Attach a clip on light to secure it with BCD when not in use. Especially for larger lights, a wrist lanyard helps avoid loss, plus allows you to release the light without losing it.</li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/#logbooks-and-elogs","title":"Logbooks and elogs","text":"<ul> <li>Useful for reference for future dives</li> <li>Recording can be useful to meet experience requirements of higher training</li> <li>Dive operators can provide better support if they know my experience</li> <li>Share online</li> <li>Related information such as local emergency contact, contact of dive buddies, instructor etc. can be stored</li> <li>PADI paper logbook is available</li> <li>eLog apps are available </li> <li>Minimum recommended information:<ul> <li>Date</li> <li>Dive site (name or location)</li> <li>Dive buddy(ies)</li> <li>Dive depth and duration</li> <li>Objective/description</li> </ul> </li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/#dive-planning-software","title":"Dive planning software","text":"<ul> <li>Plans air usage, nitrogen toxicity, etc</li> <li>Some versions interface with dive computer and includes elog functions</li> <li>Mainly mobile apps but desktop apps are also available</li> </ul>"},{"location":"15-padi-openwater-sec4-equipment4/#spare-parts-kit","title":"Spare parts kit","text":"<ul> <li>Also known as save-a-dive kit</li> <li>Helps prepare for last minute replacements</li> <li>Typically contains<ul> <li>Spare mask strap, fin strap and snorkel keeper (Tip: The VelcroTM-type mask straps make good spares, because they\u2019re easy to put in place and fit almost any mask.)</li> <li>Harness/weight belt buckle</li> <li>Cable (pull) ties</li> <li>Adjustable wrench (spanner), pliers, screw drivers, hex wrenches (allen keys) or scuba tool</li> <li>Regulator mouthpiece</li> <li>Accessory clip</li> <li>Slate/wet book pencil</li> <li>Various sized cylinder valve/DIN valve o-rings</li> <li>Cement or glue appropriate for exposure suit repair</li> <li>Sunscreen and spare sunglasses (not really dive parts, but can come in handy)</li> </ul> </li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/","title":"16 padi openwater sec4 diver4","text":""},{"location":"16-padi-openwater-sec4-diver4/#health-and-fitness","title":"Health and fitness","text":"<ul> <li>Maintain a general level of physical fitness - meaning adequate fitness plus physical reserve</li> <li>Keep immunization current - especially tetanus and typhoid</li> <li>Well balanced diet and rest well before diving</li> <li>Regular physical examination. The RSTC Medical Statement provides guidelines developed by dive medical experts that any physician can use to conduct dive physicals. Your instructor will give you this form, or you can download it from padi.com</li> <li>Heart health is critical to avoid heart attack during diving</li> <li>Avoid alcohol and tobacco before diving</li> <li>Use prescription drugs with caution</li> <li>Menstruation not a problem</li> <li>Avoid diving while pregnancy</li> <li>If you feel ill before a dive, cancel it</li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/#staying-current-and-active-diver","title":"Staying current and active diver","text":"<ul> <li>Online forums including PADI Club, dive magazines</li> <li>PADI ReActivate program to review training after a break longer than 6 months</li> <li>Continue training</li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/#the-air-your-breathe","title":"The air your breathe","text":"<ul> <li>Four issues that relate to the composition of air:<ul> <li>Oxygen toxicity</li> <li>Contaminated air</li> <li>Decompression sickness</li> <li>Gas narcosis</li> </ul> </li> <li>Enriched air nitrox (EANx), also called Nitrox, is any blend of O2 and N with 22%+ O2</li> <li>Common blends in recreational diving are 32% and 36% O2</li> <li>Tec divers can use even more % of O2</li> <li>Increasing the oxygen content and decreasing the nitrogen content has advantages and disadvantages with respect to decompression sickness and oxygen toxicity problems</li> <li>PADI enriched air diver course, or instructor may train during the open water diver course</li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/#oxygen-issues","title":"Oxygen issues","text":"<ul> <li>Under high pressure, oxygen is toxic</li> <li>If a gas has oxygen in it, oxygen toxicity can result from breathing it deeper than a specific depth</li> <li>The higher the oxygen content, the shallower the limit for using it while diving</li> <li>High oxygen percentages can also create some fire/combustion risks with respect to the equipment with which it must be used</li> <li>To avoid oxygen toxicity when diving with air, don\u2019t exceed the maximum depth for recreational diving (40 metres/130 feet).</li> <li>Fire/combustion problems aren\u2019t issues when using air with standard scuba equipment.</li> <li>Recreational divers don\u2019t use 100 percent oxygen, but tec divers often do (shallower than 6 metres/20 feet, as part of their ascent procedures)</li> <li>Rebreather divers use them and trained in the PADI Rebreather diver course</li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/#contaminated-air","title":"Contaminated air","text":"<ul> <li>Compressors for filling cylinders use valves to keep away impurities</li> <li>Trace amount of CO or Oil vapor is harmless on ground but can be toxic when breathed under pressure</li> <li>Possible causes <ul> <li>Getting cylinder filled at improper source</li> <li>Improper maintenance of filling system</li> <li>More contaminant in the source than the filters can keep out</li> </ul> </li> <li>Symptoms that the air could be contaminated<ul> <li>Headache</li> <li>Nausea</li> <li>Dizziness</li> <li>Unconsciousness/unresponsiveness</li> <li>Cherry-red lips/fongernail beds (though this is difficult to see underwater)</li> </ul> </li> <li>Actions to take<ul> <li>give fresh air</li> <li>emergency oxygen</li> <li>CPR</li> <li>emergency medical care provider</li> </ul> </li> </ul>"},{"location":"16-padi-openwater-sec4-diver4/#decompression-sickness","title":"Decompression Sickness","text":"<ul> <li>The two primary factors affecting Nitrogen absorption are dive depth and time</li> <li>Nitrogen dissolves in body tissue due to the pressure (depth)</li> <li>As you ascend, the nitrogen is released by body tissue, carried by blood to lungs for exhaling</li> <li>If the amount of excess nitrogen is within accepted limits, your body normally gets rid of it harmlessly over the next several hour</li> <li>Use dive compuers or dive tables like RDP or eRDP-ML to stay withi acceptable nitrogen limits</li> <li>If the excess nitrogen in your body tissues is too high, when you ascend and surface, the nitrogen may come out of solution faster than your body can eliminate it. This can cause nitrogen bubbles to form within your blood and body tissues, much like bubbles form when you open a soda bottle and release the pressure</li> <li>Bubbles forming in the body cause a very serious medical condition called decompression sickness, also called DCS or \"the bends\"</li> <li>The signs and symptoms of DCS depend upon where bubbles form in the body. They include:<ul> <li>Paralysis</li> <li>Dizziness</li> <li>Tingling</li> <li>Joint and limb pain</li> <li>Shock</li> <li>Numbness</li> <li>Difficulty breathing</li> <li>Weakness and prolonged fatigue</li> <li>In severe cases, unconsciousness and death</li> </ul> </li> <li>DCS signs and symptoms may be clear and obvious, but they may also be subtle, like a mild to moderate, dull ache (often, but not necessarily, in the joints), mild to moderate tingling or numbness, weakness and prolonged fatigue</li> <li>They usually occur 15 minutes to 12 hours after a dive, though they can occur before surfacing, and they can occur after 12 hours</li> <li>Physiologists think that when present, the following secondary factors can contribute to developing DCS:<ul> <li>Fatigue</li> <li>Dehydration</li> <li>Cold</li> <li>Poor fitness/high body fat</li> <li>Illness</li> <li>Injuries</li> <li>Age</li> <li>Alcohol consumption before or after a dive</li> <li>Vigorous exercise before, during or immediately after the dive</li> </ul> </li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/","title":"17 padi openwater sec4 divecomputers1","text":""},{"location":"17-padi-openwater-sec4-divecomputers1/#introduction","title":"Introduction","text":"<ul> <li>Mathematical models are used to estimate Nitrogen concentration in diver's body to manage the risk of DCS.</li> <li>Available on dive computers or dive tables like the Recreational Dive Planner (table or eRDPML electronic table versions)</li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#how-dive-computers-and-tables-work","title":"How dive computers and tables work","text":"<ul> <li>Dive computers and dive tables work by using your dive time and depth information to calculate the theoretical amount of nitrogen in your body</li> <li>Dive computers measure depth and time throughout a dive (and after) and apply the information to the decompression model electronically. A computer constantly updates the theoretical nitrogen in your body based on your dive depths and time, and compares it to the model</li> <li>Dive computers have become the most common method of calculating decompression information</li> <li>Depth from a depth gauge and the time from a timer used to look up limit information on the RDP Table </li> <li>The eRDPML is a calculator-format electronic dive table. You enter your depth/time information, and it looks it up on the table for you</li> <li>Decompression models are highly reliable, but they cannot account for individual variations in physiology, such as the secondary factors you learned in the last subsection</li> <li>Do not ascend faster than 18m / 60ft per min</li> <li>Always stay withing the limits of dive computer</li> <li>Make a safety stop in 5m /15ft</li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#no-stop-diving","title":"No stop diving","text":"<ul> <li>No stop diving is swimming directly to surface without unacceptable risk of decompression sickness</li> <li>No stop limit or No decompression limit (NDL) is the maximum time diver can spend at a depth and still ascend directly to surface</li> <li>If no stop limit is exceeded, one or more decompression stops must be taken</li> <li>Decompression stops are at specific depth for prescribed times</li> <li>In recreational diving, decompression stops are emergency procedures</li> <li>No stop times for two different computers can be different</li> <li>Computer constantly updates your remaining no stop time based on your dive profile \u2013 your actual depths, and your times at each depth \u2013 and the limits set by the decompression model</li> <li>Multilevel diving is possible with computers because it updates no stop time as you ascend</li> <li>Using a table, divers must abide by the no stop limit of the deepest depth</li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#repetitive-diving","title":"Repetitive diving","text":"<ul> <li>No stop limits are less after the first dive because of nitrogen from the first dive</li> <li>The nitrogen left in your body after a dive is called residual nitrogen</li> <li>A dive made while you still have residual nitrogen is called a repetitive dive</li> <li>Body nitrogen level return to normal after 12 hours and the next dive is considered first dive or clean dive</li> <li>A surface interval is the time you spend at the surface between two dives</li> <li>Because your dive computer tracks your personal theoretical nitrogen levels continuously during all your dives and all your surface intervals, you must use the same computer the entire diving day, on all dives, and not share it with another diver</li> <li>RDP addresses repetitive dive using three tables<ul> <li>The first table assigns a Pressure Group (as a letter) that represents the theoretical amount of residual nitrogen from your dive time and depth  </li> <li>This Pressure Group represents having less theoretical residual nitrogen in the body. </li> <li>The third table shows you no stop times for each depth adjusted for your Pressure Group at the start of the dive. </li> </ul> </li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#planning-dives-with-computer","title":"Planning dives with computer","text":"<ul> <li>Four advantages of dive computers<ul> <li>Easier to use than tables</li> <li>Help offset human errors</li> <li>Gives more time underwater</li> <li>Have other useful features, such as,<ul> <li>Measure water temperature</li> <li>Download to dive log</li> </ul> </li> </ul> </li> <li>Dive computer information<ul> <li>No stop (no decompression) limits - for planning</li> <li>Depth</li> <li>Elapsed time</li> <li>No stop time remaining</li> <li>Ascent rate</li> <li>Emergency decompression</li> <li>Previous dive information</li> </ul> </li> <li>Air supply commonly limits your dive \u2013 not your no stop time</li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#diving-with-computer","title":"Diving with computer","text":"<ul> <li>Six guidelines to follow when diving with computers<ol> <li>Dive the plan</li> <li>Stay within computers limit</li> <li>Follow the most conservative computer</li> <li>Watch SPG - There are air-integrated computers</li> <li>Start with the deepest dive</li> <li>Ascend slowly</li> </ol> </li> <li>In case of failure<ul> <li>Signal buddies and ascend, make a safety stop. Wait 12 hours or more before diving with another computer, or use a table</li> <li>If you use two computers, dive using the backup computer</li> </ul> </li> </ul>"},{"location":"17-padi-openwater-sec4-divecomputers1/#underwater-worlds-embassador","title":"Underwater world's embassador","text":"<ul> <li>Poor dive techniques, and neglect, can damage fragile aquatic life</li> <li>To learn more about how you can help in preserving the underwater world, see your PADI operator and visit projectaware.org</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/","title":"18 padi openwater sec4 skills4","text":""},{"location":"18-padi-openwater-sec4-skills4/#introduction","title":"Introduction","text":"<p>Another set of skills to learn as part of the course.</p>"},{"location":"18-padi-openwater-sec4-skills4/#deep-water-entry","title":"Deep Water Entry","text":"<ul> <li>Put on Scuba Kit at the Surface, Controlled Seated Entry</li> <li>Useful when diving from small boats without enough space to kit up</li> <li>Useful when entering from heights too great for giant stride</li> <li>Completely assemble your gear, check your SPG for enough air and be sure the cylinder valve is open</li> <li>Although you\u2019re not wearing your kit, you go through the steps of your predive safety check</li> <li>If your BCD has integrated weights but will float adequately, then weights may be in place. For a separate weight system or if your gear won\u2019t float, you will put the weights on in the water after putting on your scuba kit. Don\u2019t enter the water without your BCD while wearing your weight system</li> <li>After you and your buddy(ies) are in your gear, finish the F \u2013 Final Check - step of your predive safety check, making sure that all buckles are secure, no hoses are trapped, nothing dangles and everything is where it should be</li> <li>In many instances, the controlled seated entry is suitable for getting into the water while wearing scuba gear. Partially inflate your BCD and breathe from your regulator before entering. As you turn to ease into the water, be sure your cylinder has cleared the platform edge.</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#helping-a-tired-buddy","title":"Helping a Tired Buddy","text":"<ul> <li>Establish bouyancy for yourself and the diver who needs help</li> <li>Use the cylinder valve tow, with you and the diver floating face up, for short distances</li> <li>Use the tired diver push (modified tired-swimmer carry) for longer tows</li> <li>Swim at a slow, steady pace to the exit</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#neutral-buoyancy-visual-reference-descents-swimming-and-ascents-near-sensitive-environments","title":"Neutral Buoyancy \u2013 Visual Reference Descents, Swimming and Ascents Near Sensitive Environments","text":"<ul> <li>Combines neutral bouyancy skill with descending</li> <li>Control descent by controlling bouyancy</li> <li>Descent to 1-1.5m above the floor</li> <li>Learn to touch anything only intentionally</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#no-mask-swim","title":"No Mask Swim","text":"<ul> <li>You may close your eyes if you have contacts, though you would keep them open if you really had to ascend without your mask</li> <li>Otherwise, keep your eyes open because you can see enough to be useful</li> <li>Swim with your buddy at least 15 metres/50 feet. Control your buoyancy, equalize your ears, etc. as you normally would. Your buddy will guide you (especially if you have your eyes closed)</li> <li>Concentrate on breathing through your mouth. Exhale through your nose if the water tickles it a bit</li> <li>After the swim, replace your mask and clear it</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#freeflow-regulator-breathing","title":"Freeflow Regulator Breathing","text":"<ul> <li>Depress the purge button to simulate free flow</li> <li>Hold the mouthpiece against your lips \u2013 don\u2019t seal your mouth on it</li> <li>Allow excess air to escape; \u201csip\u201d the air you need from the flow</li> <li>It may help to insert just one side of the mouthpiece into your mouth, with the other side out</li> <li>Practice for at least 30 seconds</li> <li>Check your SPG after practicing. You may be surprised how much air you used. This emphasizes the need to start your ascent immediately when breathing from a freeflowing regulator</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#bcd-oral-inflation-underwater","title":"BCD Oral Inflation Underwater","text":"<ul> <li>This is similar to orally inflating at the surface, except that you take each breath from your regulator and there is no need to kick up as you inhale</li> <li>Blow a third to half your breath into the BCD by pressing the deflator (exhaust) button while you blow in (and only then). You won\u2019t put in as much air per breath as at the surface, because you\u2019re only adjusting to become neutrally buoyant</li> <li>Let go of the deflator button while you\u2019re not blowing in, so the BCD does not deflate</li> <li>Remember to blow a steady stream of bubbles when the regulator isn\u2019t in your mouth</li> <li>Replace the regulator, clear it and resume breathing after blowing into your BCD. Wait a moment to see how your buoyancy has changed before making further adjustments</li> <li>Do not continue a dive with a malfunctioning low pressure inflator. Use oral inflation/normal deflation to maintain buoyancy control as you abort the dive</li> <li>Once neutrally buoyant, hover for a least a minute using breath control and small adjustments</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#skin-diving-skills","title":"Skin Diving Skills","text":"<ul> <li>Skin diving skills or free diving skills is diving with mask, fins and snorkel, but no scuba</li> <li>Breathe from your diaphragm before making a breath-hold dive. This is sometimes called \u201cstomach breathing,\u201d because your diaphragm is the muscle over your stomach. To do this, breathe so your stomach area expands.</li> <li>Hyperventilation (breathing deeper and/or more rapidly than normal) is no longer preferred as a breath-hold technique because it can lower carbon dioxide levels so low that your body can run out of oxygen before you get the urge to breathe. If done improperly, it can cause you to lose consciousness and drown</li> <li>Although some divers do this by limiting hyperventilation to only 2 or 3 deep breaths, it is better to breath-hold using relaxed diaphragm breathing, which allows you to hold your breath just as long (some evidence suggests longer)</li> <li>After returning to the surface, rest until your body restores normal oxygen and carbon dioxide levels</li> <li>Breathe normally from your diaphragm \u2013 a strong exhalation after surfacing from a long breath hold can cause you to become faint</li> <li>If you feel dizzy or lightheaded, or feel tingling in your hands, arms or feet, stop diving down. Rest, relax and breathe at the surface</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#buddy-contact","title":"Buddy Contact","text":"<ul> <li>When skin diving, your buddy remains on the surface when you dive down, and vice versa</li> <li>Thee one-up, one-down technique allows your buddy to come to your aid if you need help</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#safety","title":"Safety","text":"<ul> <li>If skin diving deeper than 10 metres/30 feet and/or with breath-hold times longer than a minute interests you, see your instructor about specialized training</li> </ul>"},{"location":"18-padi-openwater-sec4-skills4/#exit-remove-scuba-kit-in-water","title":"Exit \u2013 Remove Scuba Kit in Water","text":"<ul> <li>Begin by inflating your BCD to establish positive buoyancy</li> <li>Remove and hand up your weights, or attach them to a line. This may not be necessary if your kit will float with integrated weights still in it</li> <li>To exit the water without a ladder, push up and kick hard to lift your torso, then turn and sit on the swim step or boat side (as appropriate). Alternatively, lift yourself up, bend forward at the waist, and lower yourself so you\u2019re face down on the deck/boat, then roll face up</li> <li>Sometimes you need to secure your kit to a gear line before exiting the water. This is especially true when there\u2019s a current</li> </ul>"},{"location":"19-padi-openwater-sec5-divecomputers2/","title":"19 padi openwater sec5 divecomputers2","text":""},{"location":"19-padi-openwater-sec5-divecomputers2/#planning-a-minimum-surface-interval","title":"Planning a Minimum Surface Interval","text":"<ul> <li>Planning a repetitive dive for a specific depth and time requires finding a minimum surface interval</li> <li>This is determining how long to wait after the first dive to have the no stop time you want at the planned depth of the repetitive dive</li> <li>3 ways to find a minimum surface interval with a computer: wait and check, use the dive computer plan mode and use a tablet/smartphone</li> </ul>"},{"location":"19-padi-openwater-sec5-divecomputers2/#flying-after-diving-and-altitude-diving","title":"Flying After Diving and Altitude Diving","text":"<ul> <li>As of Sep 2020, the dive medical community\u2019s recommendations for flying after diving are:</li> <li>For no stop dives<ul> <li>Single dives (no repetitive dive) \u2013 A minimum preflight surface interval of 12 hours is suggested.</li> <li>Repetitive dives or multiday dives (diving every day for several days in a row) \u2013 A minimum preflight surface interval of 18 hours is suggested.</li> </ul> </li> <li>Dives requiring emergency decompression stops:<ul> <li>A minimum preflight surface interval greater than 18 hours is suggested.</li> </ul> </li> <li>These recommendations are based on a cabin altitude pressure range of 600-2400 metres/2000-8000 fee</li> <li>Need to use altitude diving procedures if diving at an altitude of 300 metres/1000 feet or higher</li> <li>Dive computer might have settings to adjust for high altitude diving</li> </ul>"},{"location":"19-padi-openwater-sec5-divecomputers2/#cold-andor-strenuous-dives","title":"Cold and/or Strenuous Dives","text":"<ul> <li>Can dissolve more than usual levels of nitrogen in body tissue</li> <li>Plan cold/strenuous dives as though they are 4 metres/10 feet deeper than their actual depth</li> <li>Safety stops are recommended at the end of all dives, but they\u2019re especially wise after a cold and/or strenuous dive</li> </ul>"},{"location":"19-padi-openwater-sec5-divecomputers2/#emergency-decompression-stops","title":"Emergency Decompression Stops","text":"<ul> <li>Safety stops keep you well within limits, whereas emergency decompression stops return you from outside limits</li> <li>In recreational diving, required emergency decompression stops are emergency situations only. They mean you either failed to monitor your dive computer (or timer and depth gauge), or something forced you to overstay your time at depth</li> <li>If you don\u2019t have enough air to complete an emergency decompression stop, stop as long as you can, but save enough air to surface and exit safely</li> <li>If you didn\u2019t complete the entire emergency decompression stop (or accidentally skipped it altogether), after ascending, relax, breathe 100 percent emergency oxygen if available and monitor yourself for decompression sickness symptoms. Don\u2019t dive again for at least 24 hours</li> </ul>"},{"location":"19-padi-openwater-sec5-divecomputers2/#first-aid-and-treatment-for-decompression-illness","title":"First Aid and Treatment for Decompression Illness","text":"<ul> <li>The term decompression illness (DCI) is used to describe both lung overexpansion injuries and decompression sickness</li> <li>Steps to help in DCI:<ul> <li>The diver should stop all diving</li> <li>Check for breathing. Provide CPR as needed</li> <li>Contact emergency medical care. Some areas have diver emergency services for consultation and to coordinate with local medical services</li> <li>Keep the diver lying down and provide emergency oxygen</li> <li>Monitor the diver and take steps to prevent shock</li> <li>If the diver is unresponsive but breathing normally, lay the diver level, left side down, head supported, breathing oxygen</li> <li>Continue this care until emergency medical personnel arrive</li> </ul> </li> <li>Almost all cases of decompression illness require treatment in a recompression chamber</li> </ul>"},{"location":"20-padi-openwater-sec5-diver5/","title":"20 padi openwater sec5 diver5","text":""},{"location":"20-padi-openwater-sec5-diver5/#gas-narcosis","title":"Gas Narcosis","text":"<ul> <li>Many gases, including oxygen and nitrogen, cause an intoxicating effect under pressure. This is called gas narcosis. Signs and symptoms may include<ul> <li>Feeling intoxicated (drunk or \u201chigh\u201d)</li> <li>Loss of coordination</li> <li>Slowed thinking</li> <li>Slowed reactions</li> <li>Inappropriate laughter</li> <li>Depression</li> <li>False sense of security</li> <li>Ignoring or disregard for safety</li> <li>Anxiety and/or panic (when under stress at depth)</li> </ul> </li> <li>Gas narcosis is not thought to be harmful itself. The hazard is that it impairs the good judgment, clear thinking and timely responses you need to avoid and manage problems underwater</li> <li>Some gases \u2013 such as helium \u2013 are not narcotic, and tec divers breathe gas mixes with helium, which helps manage narcosis</li> <li>Gas narcosis is thought to be caused by increased dissolved gases in body tissues slowing nerve impulses that travel in the brain and nervous system</li> <li>Most divers usually begin to notice gas narcosis at a depth of approximately 30m / 100ft</li> </ul>"},{"location":"20-padi-openwater-sec5-diver5/#navigating-underwater","title":"Navigating underwater","text":"<ul> <li>Hold your compass so it\u2019s relatively level (so the north needle can rotate) and so the lubber line is aligned with the centerline of your body</li> <li>Look over the compass, not down on it. This allows you to watch where you\u2019re going while continuing to read it</li> <li>PADI Underwater Navigator Course</li> </ul>"},{"location":"21-padi-sec5-skills5/","title":"21 padi sec5 skills5","text":""},{"location":"21-padi-sec5-skills5/#remove-and-replace-the-scuba-kit-underwater","title":"Remove and Replace the Scuba Kit Underwater","text":"<ul> <li>This is one of the few skills in which it may be important to be negatively buoyant, s</li> <li>you could kneel on the bottom if necessary</li> <li>In open water, if possible, do this on insensitive bottom away from fragile aquati</li> <li>life. If dealing with entanglement or another safety situation, however, you probably hav</li> <li>no choice about where you do it</li> <li>With a weight-integrated BCD with a small or moderate amount of weight, or using a BCD without integrated weights, completely deflate your BCD and kneel on the bottom with your left knee down and your right knee up. If you have a lot of weight (while wearing a dry suit, for example), you may want enough air in the BCD so that it is not too negative to handle once you remove it</li> <li>Remove the BCD starting with your left arm, so the regulator stays in your mouth</li> <li>Keep the kit upright and do not let go of it. Stand it on your right knee. You may keep your right arm partially through the harness for control</li> <li>To keep stable on the bottom with weight-integrated BCDs, keep the unit close and positioned so that you don\u2019t float away from it. Keeping it on your knee is one effective way to do this</li> <li>Start with the right arm when you put it back on, so the regulator stays in your mouth</li> <li>Before fastening the straps, check to be sure you won\u2019t trap any hoses or accessories</li> </ul>"},{"location":"21-padi-sec5-skills5/#remove-and-replace-weight-system-underwater","title":"Remove and Replace Weight System Underwater","text":"<ul> <li>The main reasons for removing and replacing your weight system underwater are to make trim adjustments, to remove weight you can retrieve later, to replace a dislodged weight pouch or to untwist a weight belt</li> <li>This is the other skill in which it often helps to be negatively buoyant (again, on insensitive bottom), though it may not be necessary, depending upon your weight system and the amount of weight you\u2019re wearing</li> <li>Deflate your BCD and kneel</li> <li>Remove the weight pocket and rest it on your knee. If using a weight belt, release it and pull it from behind your back with the buckle (left side) free, and drape it over your knee</li> <li>Resting the weight on your knee helps you keep your balance</li> <li>Your instructor will show you the technique(s) suited to your kit</li> </ul>"},{"location":"21-padi-sec5-skills5/#descents-and-ascents-without-reference","title":"Descents and Ascents Without Reference","text":"<ul> <li>Equalize regularly as you descend</li> <li>Control your buoyancy. Slow descents and ascents are easier to control</li> <li>When ascending, watch your computer and be sure you ascend within its ascent rate</li> <li>When you ascend, make a safety stop by hovering in midwater at 5 metres/15 feet</li> <li>Practice this about half way to the surface in confined water</li> <li>If you need to navigate while descending and/or ascending, one buddy navigates while the other controls the depth and the descent/ascent rate</li> </ul>"},{"location":"21-padi-sec5-skills5/#minidive","title":"Minidive","text":"<ul> <li>Dive planning, including SPG pressures and time for ending the dives</li> <li>Predive safety checks</li> <li>Use the PADI Skill Practice and Dive Planning Slate for dive planning and the predive safety check</li> <li>Entry</li> <li>Five point descents</li> <li>Neutral buoyancy practice</li> <li>Practicing skills with your buddy(ies)</li> <li>Air supply awareness and management</li> <li>Ending the dives based on SPG pressure or time</li> <li>Five point ascents</li> <li>Exits from water</li> </ul>"},{"location":"21-padi-sec5-skills5/#confined-water-dive-five","title":"Confined Water Dive Five","text":"<ul> <li>Briefing</li> <li>Equipment assembly, dive planning, gearing up and predive safety check</li> <li>Entry</li> <li>Weight and trim check and adjustment</li> <li>Five point descent without reference</li> <li>Remove and replace scuba kit underwater</li> <li>Remove and replace weight system underwater</li> <li>Minidive</li> <li>Dive planning, entry</li> <li>Weight check and neutral buoyancy practice</li> <li>Avoiding simulated sensitive zone or objects</li> <li>Simulated emergencies, skill practice, games/objectives</li> <li>Five point ascent without reference</li> <li>Exit</li> <li>Debrief</li> </ul>"},{"location":"21-padi-sec5-skills5/#open-water-dive-three","title":"Open Water Dive Three","text":"<ul> <li>Briefing and dive planning</li> <li>Assemble, put on and adjust equipment</li> <li>Predive safety check</li> <li>Entry</li> <li>Weight and trim check, adjustment</li> <li>Controlled five point descent with visual reference</li> <li>Buoyancy control \u2013 establish neutral buoyancy with oral BCD inflation and hover</li> <li>Remove, replace and clear mask</li> <li>Underwater exploration</li> <li>Exit</li> <li>Debrief and log dive</li> <li>Post dive equipment care</li> </ul>"},{"location":"21-padi-sec5-skills5/#open-water-dive-four","title":"Open Water Dive Four","text":"<ul> <li>Briefing and dive planning</li> <li>Assemble, put on and adjust equipment</li> <li>Predive safety check</li> <li>Entry</li> <li>Weight and trim check, adjustment</li> <li>Five point free descent without reference</li> <li>Underwater exploration</li> <li>Ascent with safety stop</li> <li>Exit</li> <li>Debrief and log dive</li> <li>Post dive equipment care</li> </ul>"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/","title":"22 ubuntu bluetooth uninstall reinstall","text":"<p>My ubuntu 20.04 system on Dell XPS 9350 [?] could not connect with anything on bluetooth. </p> <p>Here is how it was fixed. </p>"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/#step-1-uninstall-bluetooth","title":"Step 1: Uninstall bluetooth","text":"<pre><code>sudo apt-get autoremove blueman bluez-utils bluez bluetooth\n</code></pre>"},{"location":"22-ubuntu-bluetooth-uninstall-reinstall/#step-2-reinstall-bluetooth","title":"Step 2: Reinstall bluetooth","text":"<pre><code>sudo apt install blueman -y &amp;&amp; blueman-manager\n</code></pre>"},{"location":"23-advanced-big-query/","title":"Objectives","text":"<ul> <li>Minimizing I/O</li> <li>Caching results of previous queries</li> <li>Performing efficient joins</li> <li>Avoid over-whelming single workers</li> <li>Using approximate aggregation functions</li> </ul>"},{"location":"23-advanced-big-query/#minimize-io","title":"Minimize I/O","text":"<p>A query that computes the sum of three columns will be slower than a query that computes the sum of two columns, but most of the performance difference will be due to reading more data, not the extra addition. Therefore, a query that computes the mean of a column will be nearly as fast as a query whose aggregation method is to compute the variance of the data (even though computing variance requires BigQuery to keep track of both the sum and the sum of the squares) because most of the overhead of simple queries is caused by I/O, not by computation. * Do not use <code>SELECT *</code> * Reduce data being read * Reduce number of expensive computations</p> <p>Suppose we wish to find the total distance traveled by each bicycle in our dataset. A naive way to do this would be to find the distance traveled in each trip undertaken by each bicycle and sum them up:</p> <pre><code>WITH\n  trip_distance AS (\nSELECT\n  bike_id,\n  ST_Distance(ST_GeogPoint(s.longitude,\n      s.latitude),\n    ST_GeogPoint(e.longitude,\n      e.latitude)) AS distance\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire,\n  `bigquery-public-data`.london_bicycles.cycle_stations s,\n  `bigquery-public-data`.london_bicycles.cycle_stations e\nWHERE\n  start_station_id = s.id\n  AND end_station_id = e.id )\nSELECT\n  bike_id,\n  SUM(distance)/1000 AS total_distance\nFROM\n  trip_distance\nGROUP BY\n  bike_id\nORDER BY\n  total_distance DESC\nLIMIT\n  5\n</code></pre> <p>Computing the distance is a pretty expensive operation and we can avoid joining the cycle_stations table against the cycle_hire table if we precompute the distances between all pairs of stations:</p> <pre><code>WITH\n  stations AS (\nSELECT\n  s.id AS start_id,\n  e.id AS end_id,\n  ST_Distance(ST_GeogPoint(s.longitude,\n      s.latitude),\n    ST_GeogPoint(e.longitude,\n      e.latitude)) AS distance\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_stations s,\n  `bigquery-public-data`.london_bicycles.cycle_stations e ),\ntrip_distance AS (\nSELECT\n  bike_id,\n  distance\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire,\n  stations\nWHERE\n  start_station_id = start_id\n  AND end_station_id = end_id )\nSELECT\n  bike_id,\n  SUM(distance)/1000 AS total_distance\nFROM\n  trip_distance\nGROUP BY\n  bike_id\nORDER BY\n  total_distance DESC\nLIMIT\n  5\n</code></pre>"},{"location":"23-advanced-big-query/#cache-intermediate-results","title":"Cache intermediate results","text":"<p>It is possible to improve overall performance at the expense of increased I/O by taking advantage of temporary tables and materialized views.</p> <p>For example, suppose you have a number of queries that start out by finding the typical duration of trips between a pair of stations. The WITH clause (also called a Common Table Expression) improves readability but does not improve query speed or cost since results are not cached. The same holds for views and subqueries as well. If you find yourself using a WITH clause, view, or a subquery often, one way to potentially improve performance is to store the result into a table (or materialized view). First you will need to create a dataset named mydataset in the EU region (where the bicycle data resides) under your project in BigQuery.</p> <pre><code>CREATE OR REPLACE TABLE\n  mydataset.typical_trip AS\nSELECT\n  start_station_name,\n  end_station_name,\n  APPROX_QUANTILES(duration, 10)[OFFSET (5)] AS typical_duration,\n  COUNT(duration) AS num_trips\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire\nGROUP BY\n  start_station_name,\n  end_station_name\n</code></pre> <p>Use the table created to find days when bicycle trips are much longer than usual:</p> <pre><code>SELECT\n  EXTRACT (DATE\n  FROM\n    start_date) AS trip_date,\n  APPROX_QUANTILES(duration / typical_duration, 10)[OFFSET(5)] AS ratio,\n  COUNT(*) AS num_trips_on_day\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire AS hire\nJOIN\n  mydataset.typical_trip AS trip\nON\n  hire.start_station_name = trip.start_station_name\n  AND hire.end_station_name = trip.end_station_name\n  AND num_trips &gt; 10\nGROUP BY\n  trip_date\nHAVING\n  num_trips_on_day &gt; 10\nORDER BY\n  ratio DESC\nLIMIT\n  10\n</code></pre>"},{"location":"23-advanced-big-query/#accelerate-queries-with-bi-engine","title":"Accelerate queries with BI Engine","text":"<p>If there are tables that you access frequently in Business Intelligence (BI) settings such as dashboards with aggregations and filters, one way to speed up your queries is to employ BI Engine. It will automatically store relevant pieces of data in memory (either actual columns from the table or derived results), and will use a specialized query processor tuned for working with mostly in-memory data. You can reserve the amount of memory (up to a current maximum of 10 GB) that BigQuery should use for its cache from the BigQuery Admin Console, under BI Engine.</p> <p>Make sure to reserve this memory in the same region as the dataset you are querying. Then, BigQuery will start to cache tables, parts of tables, and aggregations in memory and serve results faster.</p> <p>A primary use case for BI Engine is for tables that are accessed from dashboard tools such as Google Data Studio. By providing memory allocation for a BI Engine reservation, we can make dashboards that rely on a BigQuery backend much more responsive.</p>"},{"location":"23-advanced-big-query/#efficient-joins","title":"Efficient joins","text":"<p>Joining two tables requires data coordination and is subject to limitations imposed by the communication bandwidth between slots. If it is possible to avoid a join, or reduce the amount of data being joined, do so.</p> <p>Denormalization One way to improve the read performance and avoid joins is to give up on storing data efficiently, and instead add redundant copies of data. This is called denormalization.</p> <p>Thus, instead of storing the bicycle station latitudes and longitudes separately from the cycle hire information, we could create a denormalized table: </p> <pre><code>CREATE OR REPLACE TABLE\n  mydataset.london_bicycles_denorm AS\nSELECT\n  start_station_id,\n  s.latitude AS start_latitude,\n  s.longitude AS start_longitude,\n  end_station_id,\n  e.latitude AS end_latitude,\n  e.longitude AS end_longitude\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire AS h\nJOIN\n  `bigquery-public-data`.london_bicycles.cycle_stations AS s\nON\n  h.start_station_id = s.id\nJOIN\n  `bigquery-public-data`.london_bicycles.cycle_stations AS e\nON\n  h.end_station_id = e.id\n</code></pre> <p>Then, all subsequent queries will not need to carry out the join because the table will contain the necessary location information for all trips. In this case, you are trading off storage and reading more data against the computational expense of a join. It is quite possible that the cost of reading more data from disk will outweigh the cost of the join -- you should measure whether denormalization brings performance benefits.</p>"},{"location":"23-advanced-big-query/#avoid-self-joins-of-large-tables","title":"Avoid self-joins of large tables","text":"<p>Self-joins happen when a table is joined with itself. While BigQuery supports self-joins, they can lead to performance degradation if the table being joined with itself is very large. In many cases, you can avoid the self-join by taking advantage of SQL features such as aggregation and window functions.</p> <p>Let\u2019s look at an example. One of the BigQuery public datasets is the dataset of baby names published by the US Social Security Administration. It is possible to query the dataset to find the most common male names in 2015 in the state of Massachusetts (Make sure your query is running in the US region by selecting More &gt; Query settings &gt; Processing location):</p> <pre><code>SELECT\n  name,\n  number AS num_babies\nFROM\n  `bigquery-public-data`.usa_names.usa_1910_current\nWHERE\n  gender = 'M'\n  AND year = 2015\n  AND state = 'MA'\nORDER BY\n  num_babies DESC\nLIMIT\n  5\n</code></pre> <p>What are the most common names assigned to both male and female babies in the country over all the years in the dataset? A naive way to solve this problem involves reading the input table twice and doing a self-join:</p> <pre><code>WITH\nmale_babies AS (\nSELECT\n  name,\n  number AS num_babies\nFROM\n  `bigquery-public-data`.usa_names.usa_1910_current\nWHERE\n  gender = 'M' ),\nfemale_babies AS (\nSELECT\n  name,\n  number AS num_babies\nFROM\n  `bigquery-public-data`.usa_names.usa_1910_current\nWHERE\n  gender = 'F' ),\nboth_genders AS (\nSELECT\n  name,\n  SUM(m.num_babies) + SUM(f.num_babies) AS num_babies,\n  SUM(m.num_babies) / (SUM(m.num_babies) + SUM(f.num_babies)) AS frac_male\nFROM\n  male_babies AS m\nJOIN\n  female_babies AS f\nUSING\n  (name)\nGROUP BY\n  name )\nSELECT\n  *\nFROM\n  both_genders\nWHERE\n  frac_male BETWEEN 0.3\n  AND 0.7\nORDER BY\n  num_babies DESC\nLIMIT\n  5\n</code></pre> <p>The result is incorrect.</p> <p>A faster, more elegant (and correct!) solution is to recast the query to read the input only once and avoid the self-join completely.</p> <pre><code>WITH\nall_babies AS (\nSELECT\n  name,\n  SUM(\n  IF\n    (gender = 'M',\n      number,\n      0)) AS male_babies,\n  SUM(\n  IF\n    (gender = 'F',\n      number,\n      0)) AS female_babies\nFROM\n  `bigquery-public-data.usa_names.usa_1910_current`\nGROUP BY\n  name ),\nboth_genders AS (\nSELECT\n  name,\n  (male_babies + female_babies) AS num_babies,\n  SAFE_DIVIDE(male_babies,\n    male_babies + female_babies) AS frac_male\nFROM\n  all_babies\nWHERE\n  male_babies &gt; 0\n  AND female_babies &gt; 0 )\nSELECT\n  *\nFROM\n  both_genders\nWHERE\n  frac_male BETWEEN 0.3\n  AND 0.7\nORDER BY\n  num_babies DESC\nLIMIT\n  5\n</code></pre>"},{"location":"23-advanced-big-query/#reduce-data-being-joined","title":"Reduce data being joined","text":"<p>It is possible to carry out the query above with an efficient join as long as we reduce the amount of data being joined by grouping the data by name and gender early on:</p> <p>Try the following query:</p> <pre><code>WITH\nall_names AS (\nSELECT\n  name,\n  gender,\n  SUM(number) AS num_babies\nFROM\n  `bigquery-public-data`.usa_names.usa_1910_current\nGROUP BY\n  name,\n  gender ),\nmale_names AS (\nSELECT\n  name,\n  num_babies\nFROM\n  all_names\nWHERE\n  gender = 'M' ),\nfemale_names AS (\nSELECT\n  name,\n  num_babies\nFROM\n  all_names\nWHERE\n  gender = 'F' ),\nratio AS (\nSELECT\n  name,\n  (f.num_babies + m.num_babies) AS num_babies,\n  m.num_babies / (f.num_babies + m.num_babies) AS frac_male\nFROM\n  male_names AS m\nJOIN\n  female_names AS f\nUSING\n  (name) )\nSELECT\n  *\nFROM\n  ratio\nWHERE\n  frac_male BETWEEN 0.3\n  AND 0.7\nORDER BY\n  num_babies DESC\nLIMIT\n  5\n</code></pre> <p>The early grouping served to trim the data early in the query, before the query performs a JOIN. That way, shuffling and other complex operations only executed on the much smaller data and remain quite efficient. The query above finished in 2 seconds and returned the correct result.</p>"},{"location":"23-advanced-big-query/#use-a-window-function-instead-of-a-self-join","title":"Use a window function instead of a self-join","text":"<p>Suppose you wish to find the duration between a bike being dropped off and it being rented again, i.e., the duration that a bicycle stays at the station. This is an example of a dependent relationship between rows. It might appear that the only way to solve this is to join the table with itself, matching the end_date of one trip against the start_date of the next. (Make sure your query is running in the EU region by selecting More &gt; Query settings &gt; Processing location)</p> <p>You can, however, avoid a self-join by using a window function:</p> <pre><code>SELECT\n  bike_id,\n  start_date,\n  end_date,\n  TIMESTAMP_DIFF( start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station\nFROM\n  `bigquery-public-data`.london_bicycles.cycle_hire\nLIMIT\n  5\n</code></pre> <p>Using this, we can compute the average time that a bicycle is unused at each station and rank stations by that measure:</p> <pre><code>WITH\nunused AS (\n  SELECT\n    bike_id,\n    start_station_name,\n    start_date,\n    end_date,\n    TIMESTAMP_DIFF(start_date, LAG(end_date) OVER (PARTITION BY bike_id ORDER BY start_date), SECOND) AS time_at_station\n  FROM\n    `bigquery-public-data`.london_bicycles.cycle_hire )\nSELECT\n  start_station_name,\n  AVG(time_at_station) AS unused_seconds\nFROM\n  unused\nGROUP BY\n  start_station_name\nORDER BY\n  unused_seconds ASC\nLIMIT\n  5\n</code></pre>"},{"location":"23-advanced-big-query/#join-with-precomputed-values","title":"Join with precomputed values","text":"<p>Sometimes, it can be helpful to precompute functions on smaller tables, and then join with the precomputed values rather than repeat an expensive calculation each time.</p> <p>For example, suppose we wish to find the pair of stations between which our customers ride bicycles at the fastest pace. To compute the pace (minutes per kilometer) at which they ride, we need to divide the duration of the ride by the distance between stations.</p> <p>We could create a denormalized table with distances between stations and then compute the average pace:</p> <pre><code>WITH\n  denormalized_table AS (\n  SELECT\n    start_station_name,\n    end_station_name,\n    ST_DISTANCE(ST_GeogPoint(s1.longitude,\n        s1.latitude),\n      ST_GeogPoint(s2.longitude,\n        s2.latitude)) AS distance,\n    duration\n  FROM\n    `bigquery-public-data`.london_bicycles.cycle_hire AS h\n  JOIN\n    `bigquery-public-data`.london_bicycles.cycle_stations AS s1\n  ON\n    h.start_station_id = s1.id\n  JOIN\n    `bigquery-public-data`.london_bicycles.cycle_stations AS s2\n  ON\n    h.end_station_id = s2.id ),\n  durations AS (\n  SELECT\n    start_station_name,\n    end_station_name,\n    MIN(distance) AS distance,\n    AVG(duration) AS duration,\n    COUNT(*) AS num_rides\n  FROM\n    denormalized_table\n  WHERE\n    duration &gt; 0\n    AND distance &gt; 0\n  GROUP BY\n    start_station_name,\n    end_station_name\n  HAVING\n    num_rides &gt; 100 )\nSELECT\n  start_station_name,\n  end_station_name,\n  distance,\n  duration,\n  duration/distance AS pace\nFROM\n  durations\nORDER BY\n  pace ASC\nLIMIT\n  5\n</code></pre> <p>Alternately, we can use the cycle_stations table to precompute the distance between every pair of stations (this is a self-join) and then join it with the reduced-size table of average duration between stations:</p> <pre><code>WITH\n  distances AS (\n  SELECT\n    a.id AS start_station_id,\n    a.name AS start_station_name,\n    b.id AS end_station_id,\n    b.name AS end_station_name,\n    ST_DISTANCE(ST_GeogPoint(a.longitude,\n        a.latitude),\n      ST_GeogPoint(b.longitude,\n        b.latitude)) AS distance\n  FROM\n    `bigquery-public-data`.london_bicycles.cycle_stations a\n  CROSS JOIN\n    `bigquery-public-data`.london_bicycles.cycle_stations b\n  WHERE\n    a.id != b.id ),\n  durations AS (\n  SELECT\n    start_station_id,\n    end_station_id,\n    AVG(duration) AS duration,\n    COUNT(*) AS num_rides\n  FROM\n    `bigquery-public-data`.london_bicycles.cycle_hire\n  WHERE\n    duration &gt; 0\n  GROUP BY\n    start_station_id,\n    end_station_id\n  HAVING\n    num_rides &gt; 100 )\nSELECT\n  start_station_name,\n  end_station_name,\n  distance,\n  duration,\n  duration/distance AS pace\nFROM\n  distances\nJOIN\n  durations\nUSING\n  (start_station_id,\n    end_station_id)\nORDER BY\n  pace ASC\nLIMIT\n  5\n</code></pre>"},{"location":"23-advanced-big-query/#avoid-overwhelming-a-worker","title":"Avoid overwhelming a worker","text":"<p>Some operations (e.g. ordering) have to be carried out on a single worker. Having to sort too much data can overwhelm a worker\u2019s memory and result in a \u201cresources exceeded\u201d error. Avoid overwhelming the worker with too much data. As the hardware in Google data centers is upgraded, what \u201ctoo much\u201d means in this context expands over time. Currently, this is on the order of one GB.</p>"},{"location":"23-advanced-big-query/#limiting-large-sorts","title":"Limiting large sorts","text":"<p>Let\u2019s say that we wish to go through the rentals and number them 1, 2, 3, etc. in the order that the rental ended. We could do that using the ROW_NUMBER() function</p> <pre><code>SELECT\n  rental_id,\n  ROW_NUMBER() OVER(ORDER BY end_date) AS rental_number\nFROM\n  `bigquery-public-data.london_bicycles.cycle_hire`\nORDER BY\n  rental_number ASC\nLIMIT\n  5\n</code></pre> <p>We might want to consider whether it is possible to limit the large sorts and distribute them. Indeed, it is possible to extract the date from the rentals and then sort trips within each day:</p> <pre><code>WITH\n  rentals_on_day AS (\n  SELECT\n    rental_id,\n    end_date,\n    EXTRACT(DATE\n    FROM\n      end_date) AS rental_date\n  FROM\n    `bigquery-public-data.london_bicycles.cycle_hire` )\nSELECT\n  rental_id,\n  rental_date,\n  ROW_NUMBER() OVER(PARTITION BY rental_date ORDER BY end_date) AS rental_number_on_day\nFROM\n  rentals_on_day\nORDER BY\n  rental_date ASC,\n  rental_number_on_day ASC\nLIMIT\n  5\n</code></pre> <p>This is twice as faster because the sorting can be done on just a single day of data at a time.</p>"},{"location":"23-advanced-big-query/#data-skew","title":"Data skew","text":"<p>The same problem of overwhelming a worker (in this case, overwhelm the memory of the worker) can happen during an ARRAY_AGG with GROUP BY if one of the keys is much more common than the others.</p> <p>Because there are more than 3 million GitHub repositories and the commits are well distributed among them, this query succeeds (make sure you execute the query in the US processing center):</p> <pre><code>SELECT\n  repo_name,\n  ARRAY_AGG(STRUCT(author,\n      committer,\n      subject,\n      message,\n      trailer,\n      difference,\n      encoding)\n  ORDER BY\n    author.date.seconds)\nFROM\n  `bigquery-public-data.github_repos.commits`,\n  UNNEST(repo_name) AS repo_name\nGROUP BY\n  repo_name\n</code></pre> <p>Most of the people using GitHub live in only a few time zones, so grouping by the timezone fails -- we are asking a single worker to sort a significant fraction of 750GB:</p> <pre><code>SELECT\n  author.tz_offset,\n  ARRAY_AGG(STRUCT(author,\n      committer,\n      subject,\n      message,\n      trailer,\n      difference,\n      encoding)\n  ORDER BY\n    author.date.seconds)\nFROM\n  `bigquery-public-data.github_repos.commits`\nGROUP BY\n  author.tz_offset\n</code></pre> <p>If you do require sorting all the data, use more granular keys (i.e. distribute the group\u2019s data over more workers) and then aggregate the results corresponding to the desired key. For example, instead of grouping only by the time zone, it is possible to group by both timezone and repo_name and then aggregate across repos to get the actual answer for each timezone:</p> <pre><code>SELECT\n  repo_name,\n  author.tz_offset,\n  ARRAY_AGG(STRUCT(author,\n      committer,\n      subject,\n      message,\n      trailer,\n      difference,\n      encoding)\n  ORDER BY\n    author.date.seconds)\nFROM\n  `bigquery-public-data.github_repos.commits`,\n  UNNEST(repo_name) AS repo_name\nGROUP BY\n  repo_name,\n  author.tz_offset\n</code></pre>"},{"location":"23-advanced-big-query/#approximate-aggregation-functions","title":"Approximate aggregation functions","text":"<p>BigQuery provides fast, low-memory approximations of aggregate functions. Instead of using COUNT(DISTINCT \u2026), we can use APPROX_COUNT_DISTINCT on large data streams when a small statistical uncertainty in the result is tolerable.</p>"},{"location":"23-advanced-big-query/#approximate-count","title":"Approximate count","text":"<p>We can find the number of unique GitHub repositories using:</p> <pre><code>SELECT\n  COUNT(DISTINCT repo_name) AS num_repos\nFROM\n  `bigquery-public-data`.github_repos.commits,\n  UNNEST(repo_name) AS repo_name\n</code></pre> <p>Using the approximate function:</p> <pre><code>SELECT\n  APPROX_COUNT_DISTINCT(repo_name) AS num_repos\nFROM\n  `bigquery-public-data`.github_repos.commits,\n  UNNEST(repo_name) AS repo_name\n</code></pre>"},{"location":"23-advanced-big-query/#resources","title":"Resources","text":"<ul> <li>Big Query Documentation on Query Performance</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/","title":"24 freedomticket week1 intro to selling","text":""},{"location":"24-freedomticket-week1-intro-to-selling/#101-intro","title":"1.01 Intro","text":"<ul> <li>Helium10 Elite: Paid content and offline events</li> <li>Freedom Ticket Extra: $77/mo QA session access</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#102-good-opportunity","title":"1.02 Good opportunity","text":"<ul> <li>FBA: Fulfilled by amazon</li> <li>FBM: Fulfilled by merchants</li> <li>Important things to focus on: Data and Marketing</li> <li>Aggregator buy brands at 2.5 to 5x multiples </li> <li>Non-US markets are less competitive</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#103-make-money","title":"1.03 Make money","text":"<ul> <li>Arbitrage<ul> <li>retail</li> <li>online</li> </ul> </li> <li>Merchandize</li> <li>Kindle Direct Publishing</li> <li>Wholesale</li> <li>Dropshipping</li> <li>Superstore<ul> <li>niche</li> <li>lifestyle</li> </ul> </li> <li>Affiliates</li> <li>Private labels can command 20-30% profit margin</li> <li>There are 2 types of seller account <ul> <li>individual [free, but higher charge per sale]</li> <li>professional [$40/mo, added features, recommended]</li> </ul> </li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#104-risks","title":"1.04 Risks","text":"<ul> <li>Financing is important. There are lines of credit specific for amazon sellers.</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#105-principles-of-success","title":"1.05 Principles of success","text":"<ul> <li>Three points <ul> <li>Product listing</li> <li>Pick profitable products </li> <li>Drive traffic </li> </ul> </li> <li>Rule of 3: amazon will take a third of sales price, a third is product cost. The remaining third is the profit.</li> <li>Abandon products after 6 months. Compare with a benchmark such a stock market. </li> <li>Reviews are the currency of amazon.</li> <li>The first 20 reviews are critical.</li> <li>5 metrics<ul> <li>Keywords demand</li> <li>Aim for 20% profit</li> <li>Relative rank, in relative to competition [BSR: best seller rank]</li> <li>ROI, 150% or more</li> <li>Cash flow [do not run out of stock]</li> </ul> </li> <li>3 yr plan<ul> <li>Learn and make mistakes </li> <li>Optimize and add products </li> <li>Prepare to sell profitable amazon business </li> </ul> </li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#106-private-label","title":"1.06 Private Label","text":"<ul> <li>Most control, make most money</li> <li>Easier to sell to aggregator or to strategic buyer</li> <li>Differentiating by<ul> <li>Licensing</li> <li>Packaging</li> <li>Bundling</li> <li>Value add by design</li> </ul> </li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#107-how-to-start-with-less-money","title":"1.07 How to start with less money","text":"<ul> <li>Start small <ul> <li>Kindle direct publisher </li> <li>Arbitrage</li> <li>Use other people's money </li> </ul> </li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#108-examples-of-opportunities","title":"1.08 Examples of opportunities","text":"<ul> <li>Helium10 blackbox for keywords</li> <li>Use X-Ray browser plugin</li> <li>Supplier search to find supplier in alibaba</li> <li>ROI calculator to estimate profit margin</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#109-glossary","title":"1.09 Glossary","text":"<ul> <li>Brand gating: Restriction by amazon to stop others from using the brand. Hard to get for new brands.</li> <li>Child: Variant of \"parent\" product. </li> <li>EXW: Exworks, you pay all the expenses from factory door.</li> <li>FOB: Freight on board, all the way to port is paid by factory.</li> <li>DDP: delivery duty paid, all the way to warehouse door is paid by factory.</li> <li>FNSKU: Specific code on top of ASIN. Relevant for same product sold by multiple vendors.</li> <li>Highjacker: Someone taking advantage of your brand and piggytailing sales.</li> <li>Landed cost: Total cost to get the product to amazon warehouse.</li> <li>LTL: Less than truckload.</li> <li>Keywords: how seller describes the product.</li> <li>Search terms: how users search.</li> <li>Prep center: service of bundling, editing products before shipping to customers.</li> <li>Removal order: item shipped back to you.</li> <li>Ungated: get permission to sell specific brands.</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#110-financing","title":"1.10 Financing","text":"<ul> <li>2.5 times of initial inventory cost [landed cost].</li> <li>Amazon pays after 3-5 weeks. plan for fund to reorder inventory.</li> <li>Start with 1 product, minimum variation.</li> <li>Don't sell below $20</li> <li>Consider high value items that have low velocity.</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#111-profit-and-valuation-spreadsheets","title":"1.11 Profit and valuation spreadsheets","text":"<ul> <li>Spreadsheet models to estimate profit and valuation.</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#112-intro-to-projectx","title":"1.12 Intro to ProjectX","text":"<ul> <li>Bonus case study videos</li> </ul>"},{"location":"24-freedomticket-week1-intro-to-selling/#113-additional-resources","title":"1.13 Additional resources","text":"<ul> <li>Serious seller pod cast </li> <li>AM/PM podcast for general ecommerce</li> <li>helium10 blog</li> <li>Facebook group</li> <li>Instagram</li> <li>YouTube </li> <li>(Trusted Partner Directory)[directory.Helium10.com]</li> <li>(Academy)[academy.helium10.com]</li> <li>Helium10 Elite</li> </ul>"},{"location":"25-gcp-essential-infrastructure-cloud-sql/","title":"Implementing Cloud SQL","text":""},{"location":"25-gcp-essential-infrastructure-cloud-sql/#objectives","title":"Objectives","text":"<p>Create a Cloud SQL database</p> <p>Configure a virtual machine to run a proxy</p> <p>Create a connection between an application and Cloud SQL</p> <p>Connect an application to Cloud SQL using Private IP address</p> <p>By the end of this lab, we will have 2 working instances of the Wordpress frontend connected over 2 different connection types to their SQL instance backend.</p>"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#step-1-create-a-cloud-sql-database","title":"Step 1: Create a Cloud SQL Database","text":"<p>In this task, you configure a SQL server according to Google Cloud best practices and create a Private IP connection.</p> <p>On the Navigation menu (Navigation menu), click SQL. Click Create instance. Click Choose MySQL. Specify the following, and leave the remaining settings as their defaults: Property    Value Instance ID wordpress-db Root password   type a password Region  us-central1 Zone    Any Database Version    MySQL 5.7</p> <pre><code>Note the root password; it will be used in a later step and referred to as [ROOT_PASSWORD].\n</code></pre> <p>Expand Show configuration options.</p> <p>Expand the Machine type section.</p> <p>Provision the right amount of vCPU and memory. To choose a Machine Type, click the dropdown menu, and then explore your options.</p> <pre><code>A few points to consider:\n\nShared-core machines are good for prototyping, and are not covered by Cloud SLA.\nEach vCPU is subject to a 250 MB/s network throughput cap for peak performance. Each additional core increases the network cap, up to a theoretical maximum of 2000 MB/s.\nFor performance-sensitive workloads such as online transaction processing (OLTP), a general guideline is to ensure that your instance has enough memory to contain the entire working set and accommodate the number of active connections.\n</code></pre> <p>For this lab, select standard from the dropdown menu, and then select 1 vCPU, 3.75 GB.</p> <p>Next, expand the Storage section and then choose Storage type and Storage capacity.</p> <pre><code>A few points to consider:\n\nSSD (solid-state drive) is the best choice for most use cases. HDD (hard-disk drive) offers lower performance, but storage costs are significantly reduced, so HDD may be preferable for storing data that is infrequently accessed and does not require very low latency.\nThere is a direct relationship between the storage capacity and its throughput.\n</code></pre> <p>Click each of the capacity options to see how it affects the throughput. Reset the option to 10GB.</p> <pre><code>Setting your storage capacity too low without enabling an automatic storage increase can cause your instance to lose its SLA.\n</code></pre> <p>Expand the Connections section.</p> <p>Select Private IP.</p> <p>In the Network dropdown, select default.</p> <p>Click the Set up Connection button that appears.</p> <p>In the panel to the right, click Enable API, click Use an automatically allocated IP range, click Continue, and then click Create Connection.</p> <p>Click Create Instance at the bottom of the page to create the database instance.</p>"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-2-configure-a-proxy-on-a-virtual-machine","title":"Task 2: Configure a proxy on a virtual machine","text":"<p>When your application does not reside in the same VPC connected network and region as your Cloud SQL instance, use a proxy to secure its external connection.</p> <p>In order to configure the proxy, you need the Cloud SQL instance connection name.</p> <pre><code>The lab comes with 2 virtual machines preconfigured with Wordpress and its dependencies. You can view the startup script and service account access by clicking on a virtual machine name. Notice that we used the principle of least privilege and only allow SQL access for that VM. There's also a network tag and a firewall preconfigured to allow port 80 from any host.\n</code></pre> <p>On the Navigation menu (Navigation menu) click Compute Engine.</p> <p>Click SSH next to wordpress-us-west1-proxy.</p> <p>Download the Cloud SQL Proxy and make it executable:</p> <pre><code>wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy &amp;&amp; chmod +x cloud_sql_proxy\n</code></pre> <p>In order to start the proxy, you need the connection name of the Cloud SQL instance. Keep your SSH window open and return to the Cloud Console.</p> <p>On the Navigation menu (Navigation menu), click SQL.</p> <p>Click on the wordpress-db instance and wait for a green checkmark next to its name, which indicates that it is operational (this could take a couple of minutes).</p> <p>Note the Instance connection name; it will be used later and referred to as [SQL_CONNECTION_NAME].</p> <p>In addition, for the application to work, you need to create a table. Click Databases.</p> <p>Click Create database, type wordpress, which is the name the application expects, and then click Create.</p> <p>Return to the SSH window and save the connection name in an environment variable, replacing [SQL_CONNECTION_NAME] with the unique name you copied in a previous step.</p> <pre><code>export SQL_CONNECTION=[SQL_CONNECTION_NAME]\n</code></pre> <p>To verify that the environment variable is set, run:</p> <pre><code>echo $SQL_CONNECTION\n</code></pre> <p>To activate the proxy connection to your Cloud SQL database and send the process to the background, run the following command:</p> <pre><code>./cloud_sql_proxy -instances=$SQL_CONNECTION=tcp:3306 &amp;\n</code></pre> <p>The expected output is</p> <pre><code>Listening on 127.0.0.1:3306 for [SQL_CONNECTION_NAME]\nReady for new connections\n</code></pre> <p>Press ENTER</p> <pre><code>The proxy will listen on 127.0.0.1:3306 (localhost) and proxy that connects securely to your Cloud SQL over a secure tunnel using the machine's external IP address.\n</code></pre>"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-3-connect-an-application-to-the-cloud-sql-instance","title":"Task 3: Connect an application to the Cloud SQL instance","text":"<p>In this task, you will connect a sample application to the Cloud SQL instance.</p> <p>Configure the Wordpress application. To find the external IP address of your virtual machine, query its metadata:</p> <pre><code>curl -H \"Metadata-Flavor: Google\" http://169.254.169.254/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip &amp;&amp; echo\n</code></pre> <p>Go to the wordpress-us-west1-proxy external IP address in your browser and configure the Wordpress application.</p> <p>Click Let's Go.</p> <p>Specify the following, replacing [ROOT_PASSWORD] with the password you configured upon machine creation, and leave the remaining settings as their defaults:</p> <p>Property    Value Username    root Password    [ROOT_PASSWORD] Database Host   127.0.0.1</p> <pre><code>You are using 127.0.0.1, localhost as the Database IP because the proxy you initiated listens on this address and redirects that traffic to your SQL server securely.\n</code></pre> <p>When a 'Success!' window appears, remove the text after the IP address in your web browser's address bar and press ENTER. You'll be presented with a working Wordpress Blog!</p>"},{"location":"25-gcp-essential-infrastructure-cloud-sql/#task-4-connect-to-cloud-sql-via-internal-ip","title":"Task 4: Connect to Cloud SQL via internal IP","text":"<p>If you can host your application in the same region and VPC connected network as your Cloud SQL, you can leverage a more secure and performant configuration using Private IP.</p> <p>By using Private IP, you will increase performance by reducing latency and minimize the attack surface of your Cloud SQL instance because you can communicate with it exclusively over internal IPs.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click SQL.</p> <p>Click wordpress-db.</p> <p>Note the Private IP address of the Cloud SQL server; it will be referred to as [SQL_PRIVATE_IP].</p> <p>On the Navigation menu, click Compute Engine.</p> <pre><code>Notice that wordpress-us-private-ip is located at us-central1, where your Cloud SQL is located, which enables you to leverage a more secure connection.\n</code></pre> <p>Copy the external IP address of wordpress-us-private-ip, paste it in a browser window, and press ENTER.</p> <p>Click Let's Go.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value Username    root Password    type the [ROOT_PASSWORD] configured when the Cloud SQL instance was created Database Host   [SQL_PRIVATE_IP]</p> <p>Click Submit.</p> <pre><code>Notice that this time you are creating a direct connection to a Private IP, instead of configuring a proxy. That connection is private, which means that it doesn't egress to the internet and therefore benefits from better performance and security.\nClick Run the installation. An 'Already Installed!' window is displayed, which means that your application is connected to the Cloud SQL server over private IP.\n</code></pre> <p>In your web browser's address bar, remove the text after the IP address and press ENTER. You'll be presented with a working Wordpress Blog!</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/","title":"Resource Monitoring","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#objectives","title":"Objectives","text":"<p>In this lab, you learn how to perform the following tasks:</p> <p>Explore Cloud Monitoring</p> <p>Add charts to dashboards</p> <p>Create alerts with multiple conditions</p> <p>Create resource groups</p> <p>Create uptime checks</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#tasks","title":"Tasks","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#task-1-create-a-cloud-monitoring-workspace","title":"Task 1: Create a Cloud Monitoring workspace","text":"<p>Multiple projects can be connected to a workspace.</p> <p>Access to the monitoring workspace will provide access to metrics of all projects connected to the workspace. As a result, we need to create workspaces to manage access of Dev Ops by project. </p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-2-custom-dashboards-metric-explorer","title":"Task 2: Custom dashboards, Metric explorer","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#task-3-alerting-policies-notification-channels","title":"Task 3: Alerting policies, Notification channels","text":"<p>Its better to alert for symptoms instead of errors.</p> <p>Email, slack channel, pub/sub, webhooks etc. </p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-4-resource-groups","title":"Task 4: Resource groups","text":"<p>Can make groups of resources based on name, type, labels, etc.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-5-uptime-monitoring","title":"Task 5: Uptime monitoring","text":"<p>Check uptime at a set interval.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#error-reporting-and-debugging","title":"Error Reporting and Debugging","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#objectives_1","title":"Objectives","text":"<p>In this lab, you learn how to perform the following tasks:</p> <p>Launch a simple Google App Engine application</p> <p>Introduce an error into the application</p> <p>Explore Cloud Error Reporting</p> <p>Use Cloud Debugger to identify the error in the code</p> <p>Fix the bug and monitor in Cloud Operations</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-1-create-an-application","title":"Task 1: Create an application","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#get-and-test-the-application","title":"Get and test the application","text":"<p>In the Cloud Console, launch Cloud Shell by clicking Activate Cloud Shell ( 857dc9d7dd799cb2.png). If prompted, click Continue.</p> <p>To create a local folder and get the App Engine Hello world application, run the following commands:</p> <pre><code>mkdir appengine-hello\ncd appengine-hello\ngsutil cp gs://cloud-training/archinfra/gae-hello/* .\n</code></pre> <p>To run the application using the local development server in Cloud Shell, run the following command:</p> <pre><code>dev_appserver.py $(pwd)\n</code></pre> <p>In Cloud Shell, click Web Preview &gt; Preview on port 8080 to view the application. You may have to collapse the Navigation menu pane to access the Web Preview icon.</p> <p>In Cloud Shell, press Ctrl+C to exit the development server.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#deploy-the-application-to-app-engine","title":"Deploy the application to App Engine","text":"<p>To deploy the application to App Engine, run the following command:</p> <pre><code>gcloud app deploy app.yaml\n</code></pre> <p>If prompted for a region, enter the number corresponding to a region.</p> <p>When prompted, type Y to continue.</p> <p>When the process is done, verify that the application is working by running the following command:</p> <pre><code>gcloud app browse\n</code></pre>"},{"location":"26-gcp-essential-infrastructure-monitoring/#introduce-an-error-to-break-the-application","title":"Introduce an error to break the application","text":"<p>To examine the main.py file, run the following command:</p> <pre><code>cat main.py\n</code></pre> <p>To use the sed stream editor to change the import library to the nonexistent webapp22, run the following command:</p> <pre><code>sed -i -e 's/webapp2/webapp22/' main.py\n</code></pre> <p>To verify the change you made in the main.py file, run the <code>cat main.py</code> again.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#re-deploy-the-application-to-app-engine","title":"Re-deploy the application to App Engine","text":"<p>To re-deploy the application to App Engine, run the following command:</p> <pre><code>gcloud app deploy app.yaml --quiet\n</code></pre> <pre><code>The --quiet flag disables all interactive prompts when running gcloud commands. If input is required, defaults will be used. In this case, it avoids the need for you to type Y when prompted to continue the deployment.\n</code></pre> <p>When the process is done, verify that the application is broken by running the following command:</p> <pre><code>gcloud app browse\n</code></pre> <p>If Cloud Shell does not detect your browser, click the link in the Cloud Shell output to view your app.</p> <p>If needed, press Ctrl+C to exit development mode.</p> <p>Leave Cloud Shell open.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#task-2-explore-cloud-error-reporting","title":"Task 2: Explore Cloud Error Reporting","text":""},{"location":"26-gcp-essential-infrastructure-monitoring/#view-error-reporting-and-trigger-additional-errors","title":"View Error Reporting and trigger additional errors","text":"<p>In the Cloud Console, on the Navigation menu ( 7a91d354499ac9f1.png), click Error Reporting.</p> <p>You should see an error regarding the failed import of webapp22.</p> <p>Click Auto reload.</p> <p>In Cloud Shell, run the following command:</p> <pre><code>gcloud app browse\n</code></pre> <p>Click the link several times to generate more errors.</p>"},{"location":"26-gcp-essential-infrastructure-monitoring/#view-details-and-identify-the-cause","title":"View details and identify the cause","text":"<p>Click the Error name: ImportError: No module named webapp22.</p> <p>Now you can see a detailed graph of the errors. The Response Code field shows the explicit error: a 500 Internal Server Error.</p> <p>For Stack trace sample, click Parsed. This opens the Cloud Debugger, showing the error in the code!</p>"},{"location":"27-gcp-elastic-cloud-vpn/","title":"Virtual Private Networks (VPN)","text":""},{"location":"27-gcp-elastic-cloud-vpn/#objectives","title":"Objectives","text":"<p>Create VPN gateways in each network</p> <p>Create VPN tunnels between the gateways</p> <p>Verify VPN connectivity</p>"},{"location":"27-gcp-elastic-cloud-vpn/#task-1-explore-the-networks-and-instances","title":"Task 1: Explore the networks and instances","text":"<p>Ping VM instances external IP using </p> <pre><code>ping -c 3 &lt;external IP address&gt;\n</code></pre>"},{"location":"27-gcp-elastic-cloud-vpn/#task-2-create-the-vpn-gateways-and-tunnels","title":"Task 2: Create the VPN gateways and tunnels","text":"<p>Establish private communication between the two VM instances by creating VPN gateways and tunnels between the two networks.</p>"},{"location":"27-gcp-elastic-cloud-vpn/#reserve-two-static-ip-addresses","title":"Reserve two static IP addresses","text":"<p>Reserve one static IP address for each VPN gateway.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network &gt; External IP addresses.</p> <p>Click Reserve static address.</p> <p>Specify the following, and leave the remaining settings as their defaults: Property    Value (type value or select option as specified) Name    vpn-1-static-ip IP version  IPv4 Region  us-central1</p> <p>Click Reserve.</p> <p>Repeat the same for vpn-2-static-ip.</p> <p>Click Reserve static address.</p> <p>Specify the following, and leave the remaining settings as their defaults: Property    Value (type value or select option as specified) Name    vpn-2-static-ip IP version  IPv4 Region  europe-west1</p>"},{"location":"27-gcp-elastic-cloud-vpn/#create-the-vpn-1-gateway-and-tunnel1to2","title":"Create the vpn-1 gateway and tunnel1to2","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click Hybrid Connectivity &gt; VPN.</p> <p>Click Create VPN Connection.</p> <p>If asked, select Classic VPN, and then click Continue.</p> <p>Specify the following in the VPN gateway section, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified) Name    vpn-1 Network vpn-network-1 Region  us-central1 IP address  vpn-1-static-ip</p> <p>Specify the following in the Tunnels section, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified) Name    tunnel1to2 Remote peer IP address  [VPN-2-STATIC-IP] IKE pre-shared key  gcprocks Routing options Route-based Remote network IP ranges    10.1.3.0/24 [internal ip address for vm instance 2]</p> <p>Equivalent command line: </p> <pre><code>gcloud compute target-vpn-gateways create vpn-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --network=vpn-network-1 \n\ngcloud compute forwarding-rules create vpn-1-rule-esp --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=ESP --target-vpn-gateway=vpn-1 \n\ngcloud compute forwarding-rules create vpn-1-rule-udp500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=500 --target-vpn-gateway=vpn-1 \n\ngcloud compute forwarding-rules create vpn-1-rule-udp4500 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --address=34.71.63.22 --ip-protocol=UDP --ports=4500 --target-vpn-gateway=vpn-1 \n\ngcloud compute vpn-tunnels create tunnel1to2 --project=qwiklabs-gcp-03-bc85da7e9da5 --region=us-central1 --peer-address=34.79.170.227 --shared-secret=gcprocks --ike-version=2 --local-traffic-selector=0.0.0.0/0 --remote-traffic-selector=0.0.0.0/0 --target-vpn-gateway=vpn-1 \n\ngcloud compute routes create tunnel1to2-route-1 --project=qwiklabs-gcp-03-bc85da7e9da5 --network=vpn-network-1 --priority=1000 --destination-range=10.1.3.0/24 --next-hop-vpn-tunnel=tunnel1to2 --next-hop-vpn-tunnel-region=us-central1\n</code></pre> <p>Click Create.</p>"},{"location":"27-gcp-elastic-cloud-vpn/#create-the-vpn-2-gateway-and-tunnel2to1","title":"Create the vpn-2 gateway and tunnel2to1","text":"<p>Click VPN setup wizard.</p> <p>If asked, select Classic VPN, and then click Continue.</p> <p>Specify the following in the VPN gateway section, and leave the remaining settings as their defaults: Property    Value (type value or select option as specified) Name    vpn-2 Network vpn-network-2 Region  europe-west1 IP address  vpn-2-static-ip</p> <p>Specify the following in the Tunnels section, and leave the remaining settings as their defaults: Property    Value (type value or select option as specified) Name    tunnel2to1 Remote peer IP address  [VPN-1-STATIC-IP] IKE pre-shared key  gcprocks Routing options Route-based Remote network IP ranges    10.5.4.0/24 [internal ip address for vm instance 1]</p>"},{"location":"27-gcp-elastic-cloud-vpn/#task-3-verify-vpn-connectivity","title":"Task 3: Verify VPN connectivity","text":"<p>Now we should be able to ping both internal and external IP addresses.</p>"},{"location":"27-gcp-elastic-cloud-vpn/#verify-server-1-to-server-2-connectivity","title":"Verify server-1 to server-2 connectivity","text":"<p>In the Cloud Console, on the Navigation menu, click Compute Engine &gt; VM instances.</p> <p>For server-1, click SSH to launch a terminal and connect.</p> <p>To test connectivity to server-2's internal IP address, run the following command:</p> <pre><code>ping -c 3 &lt;insert server-2's internal IP address here&gt;\n</code></pre>"},{"location":"27-gcp-elastic-cloud-vpn/#remove-the-external-ip-addresses","title":"Remove the external IP addresses","text":"<p>Now that you verified VPN connectivity, you can remove the instances' external IP addresses. For demonstration purposes, just do this for the server-1 instance.</p> <p>On the Navigation menu, click Compute Engine &gt; VM instances. Select the server-1 instance and click Stop. Wait for the instance to stop.</p> <pre><code>Instances need to be stopped before you can make changes to their network interfaces.\n</code></pre> <p>Click on the name of the server-1 instance to open the VM instance details page. Click Edit. For Network interfaces, click the Edit icon (Edit). Change External IP to None. Click Done. Click Save and wait for the instance details to update. Click Start. Click Start again to confirm that you want to start the VM instance. Return to the VM instances page and wait for the instance to start. Notice that External IP is set to None for the server-1 instance.</p>"},{"location":"28-http-load-balancer/","title":"Configuring an HTTP Load Balancer with Autoscaling","text":"<p>Google Cloud HTTP(S) load balancing is implemented at the edge of Google's network in Google's points of presence (POP) around the world. User traffic directed to an HTTP(S) load balancer enters the POP closest to the user and is then load-balanced over Google's global network to the closest backend that has sufficient available capacity.</p>"},{"location":"28-http-load-balancer/#objectives","title":"Objectives","text":"<ol> <li>Create a health check firewall rule</li> <li>Create a NAT configuration using Cloud Router</li> <li>Create a custom image for a web server</li> <li>Create an instance template based on the custom image</li> <li>Create two managed instance groups</li> <li>Configure an HTTP load balancer with IPv4 and IPv6</li> <li>Stress test an HTTP load balancer</li> </ol>"},{"location":"28-http-load-balancer/#task-1-configure-a-health-check-firewall-rule","title":"Task 1. Configure a health check firewall rule","text":"<p>Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.</p>"},{"location":"28-http-load-balancer/#create-the-health-check-rule","title":"Create the health check rule","text":"<p>Create a firewall rule to allow health checks.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network &gt; Firewall. Notice the existing ICMP, internal, RDP, and SSH firewall rules.</p> <p>Each Google Cloud project starts with the default network and these firewall rules.</p> <p>Click Create Firewall Rule.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    fw-allow-health-checks</p> <p>Network default</p> <p>Targets Specified target tags</p> <p>Target tags allow-health-checks</p> <p>Source filter   IP Ranges</p> <p>Source IP ranges    130.211.0.0/22 and 35.191.0.0/16</p> <p>Protocols and ports Specified protocols and ports</p> <pre><code>Make sure to include the /22 and /16 in the Source IP ranges.\n</code></pre> <p>Select tcp and specify port 80. Click Create.</p>"},{"location":"28-http-load-balancer/#task-2-create-a-nat-configuration-using-cloud-router","title":"Task 2: Create a NAT configuration using Cloud Router","text":"<p>The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses.</p> <p>Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer.</p>"},{"location":"28-http-load-balancer/#create-the-cloud-router-instance","title":"Create the Cloud Router instance","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click Network services &gt; Cloud NAT.</p> <p>Click Get started.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Gateway name    nat-config</p> <p>Network default</p> <p>Region  us-central1</p> <p>Click Cloud Router, and select Create new router.</p> <p>For Name, type nat-router-us-central1.</p> <p>Click Create.</p> <p>In Create a NAT gateway, click Create.</p>"},{"location":"28-http-load-balancer/#task-3-create-a-custom-image-for-a-web-server","title":"Task 3: Create a custom image for a web server","text":"<p>Create a custom web server image for the backend of the load balancer.</p>"},{"location":"28-http-load-balancer/#create-a-vm","title":"Create a VM","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine &gt; VM instances.</p> <p>Click Create Instance.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    webserver</p> <p>Region  us-central1</p> <p>Zone    us-central1-a</p> <p>Series  N1</p> <p>Machine type    f1-micro (1 vCPU)</p> <p>Under Boot disk, select change.</p> <p>click Show advanced option.</p> <p>Under deletion rule, select keep boot disk</p> <p>Click Select</p> <p>Click Networking, disks, security, management, sole-tenancy.</p> <p>Click Networking.</p> <pre><code>- For Network tags, type allow-health-checks.\n- Under Network interfaces , click default.\n- Under External IP dropdown, select None.\n</code></pre> <p>Click Done.</p> <p>Click Create.</p>"},{"location":"28-http-load-balancer/#customize-the-vm","title":"Customize the VM","text":"<p>For webserver, click SSH to launch a terminal and connect.</p> <p>If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry.</p> <p>To install Apache2, run the following commands:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apache2\n</code></pre> <p>To start the Apache server, run the following command:</p> <pre><code>sudo service apache2 start\n</code></pre> <p>To test the default page for the Apache2 server, run the following command:</p> <pre><code>curl localhost\n</code></pre> <p>The default page for the Apache2 server should be displayed.</p>"},{"location":"28-http-load-balancer/#set-the-apache-service-to-start-at-boot","title":"Set the Apache service to start at boot","text":"<p>The software installation was successful. However, when a new VM is created using this image, the freshly booted VM does not have the Apache web server running. Use the following command to set the Apache service to automatically start on boot. Then test it to make sure it works.</p> <p>In the webserver SSH terminal, set the service to start on boot:</p> <pre><code>sudo update-rc.d apache2 enable\n</code></pre> <p>In the Cloud Console, select webserver, and then click Reset. In the confirmation dialog, click Reset. Reset will stop and reboot the machine. It keeps the same IPs and the same persistent boot disk, but memory is wiped. Therefore, if the Apache service is available after the reset, the update-rc command was successful.</p> <p>Check the server by connecting via SSH to the VM and entering the following command:</p> <pre><code>sudo service apache2 status\n</code></pre> <p>NOTE: If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry . The result should show Started The Apache HTTP Server.</p>"},{"location":"28-http-load-balancer/#prepare-the-disk-to-create-a-custom-image","title":"Prepare the disk to create a custom image","text":"<p>Verify that the boot disk will not be deleted when the instance is deleted.</p> <p>On the VM instances page, click webserver to view the VM instance details.</p> <p>Under Boot disk, verify that When deleting instance is set to Keep disk.</p> <p>Return to the VM instances page, click webserver, and click Delete.</p> <p>In the confirmation dialog, click Delete.</p> <p>In the left pane, click Disks and verify that the webserver disk exists.</p>"},{"location":"28-http-load-balancer/#create-the-custom-image","title":"Create the custom image","text":"<p>In the left pane, click Images.</p> <p>Click Create image.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    mywebserver</p> <p>Source  Disk</p> <p>Source disk webserver</p> <p>Click Create.</p> <p>You have created a custom image that multiple identical webservers can be started from. At this point, you could delete the webserver disk.</p> <p>The next step is to use that image to define an instance template that can be used in the managed instance groups.</p>"},{"location":"28-http-load-balancer/#task-4-configure-an-instance-template-and-create-instance-groups","title":"Task 4. Configure an instance template and create instance groups","text":"<p>A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the HTTP load balancer.</p>"},{"location":"28-http-load-balancer/#configure-the-instance-template","title":"Configure the instance template","text":"<p>An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine &gt; Instance templates.</p> <p>Click Create instance template.</p> <p>For Name, type mywebserver-template.</p> <p>For Series, select N1.</p> <p>For Machine type, select f1-micro (1 vCPU).</p> <p>For Boot disk, click Change.</p> <p>Click Custom images.</p> <p>For Image, Select mywebserver.</p> <p>Click Select.</p> <p>Click Management, security, disks, networking, sole tenancy.</p> <p>Click Networking.</p> <pre><code>- For Network tags, type allow-health-checks.\n- Under External IP dropdown, select None.\n</code></pre> <p>Click Create.</p>"},{"location":"28-http-load-balancer/#create-the-managed-instance-groups","title":"Create the managed instance groups","text":"<p>Create a managed instance group in us-central1 and one in europe-west1.</p> <p>On the Navigation menu, click Compute Engine &gt; Instance groups.</p> <p>Click Create Instance group.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    us-central1-mig</p> <p>Location    Multiple zones</p> <p>Region  us-central1</p> <p>Instance template   mywebserver-template</p> <p>Under Autoscaling metrics, click on the edit pencil icon.</p> <p>Under Metric type, select HTTP load balancing utilization.</p> <p>Enter Target HTTP load balancing utilization to 80.</p> <p>Click Done.</p> <p>Set Cool down period to 60 seconds.</p> <p>Enter Minimum number of instances 1 and Maximum number of instances 2.</p> <pre><code>Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load.\n</code></pre> <p>For Health check, select Create a health check.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (select option as specified)</p> <p>Name    http-health-check</p> <p>Protocol    TCP</p> <p>Port    80</p> <p>Managed instance group health checks proactively signal to delete and recreate instances that become unhealthy.</p> <p>Click Save and continue.</p> <p>For Initial delay, type 60. This is how long the Instance Group waits after initializing the boot-up of a VM before it tries a health check. You don't want to wait 5 minutes for this during the lab, so you set it to 1 minute.</p> <p>Click Create.</p> <p>Click OK.</p> <p>NOTE: If a warning window will appear stating that There is no backend service attached to the instance group. Ignore this; you will configure the load balancer with a backend service in the next section of the lab.</p> <p>Repeat the same procedure for europe-west1-mig in europe-west1:</p> <p>Click Create Instance group.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    europe-west1-mig</p> <p>Location    Multiple zones</p> <p>Region  europe-west1</p> <p>Instance template   mywebserver-template</p> <p>Autoscaling metrics &gt; Metric Type   HTTP load balancing utilization</p> <p>Target HTTP load balancing utilization  80</p> <p>Minimum number of instances 1</p> <p>Maximum number of instances 2</p> <p>Cool down period    60</p> <p>For Health check, select http-health-check (TCP).</p> <p>For Initial delay, type 60.</p> <p>Click Create.</p> <p>Click OK in the dialog window.</p>"},{"location":"28-http-load-balancer/#task-5-configure-the-http-load-balancer","title":"Task 5. Configure the HTTP load balancer","text":"<p>Configure the HTTP load balancer to balance traffic between the two backends (us-central1-mig in us-central1 and europe-west1-mig in europe-west1)</p>"},{"location":"28-http-load-balancer/#start-the-configuration","title":"Start the configuration","text":"<p>On the Navigation menu, click Network Services &gt; Load balancing.</p> <p>Click Create load balancer.</p> <p>Under HTTP(S) Load Balancing, click Start configuration.</p> <p>Select From Internet to my VMs, then click Continue.</p> <p>For Name, type http-lb.</p>"},{"location":"28-http-load-balancer/#configure-the-backend","title":"Configure the backend","text":"<p>Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata.</p> <p>Click Backend configuration.</p> <p>For Backend services &amp; backend buckets, click Create a backend service.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (select option as specified)</p> <p>Name    http-backend</p> <p>Backend type    Instance group</p> <p>Instance group  us-central1-mig</p> <p>Port numbers    80</p> <p>Balancing mode  Rate</p> <p>Maximum RPS 50</p> <p>Capacity    100</p> <p>This configuration means that the load balancer attempts to keep each instance of us-central1-mig at or below 50 requests per second (RPS).</p> <p>Click Done.</p> <p>Click Add backend.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (select option as specified)</p> <p>Instance group  europe-west1-mig</p> <p>Port numbers    80</p> <p>Balancing mode  Utilization</p> <p>Maximum backend utilization 80</p> <p>Capacity    100</p> <p>This configuration means that the load balancer attempts to keep each instance of europe-west1-mig at or below 80% CPU utilization.</p> <p>Click Done.</p> <p>For Health Check, select http-health-check (TCP).</p> <p>Check the Enable logging checkbox.</p> <p>Specify Sample rate as 1.</p> <p>Click Create.</p>"},{"location":"28-http-load-balancer/#configure-the-frontend","title":"Configure the frontend","text":"<p>The host and path rules determine how your traffic will be directed. For example, you could direct video traffic to one backend and direct static traffic to another backend. However, you are not configuring the host and path rules in this lab.</p> <p>Click Frontend configuration.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Protocol    HTTP</p> <p>IP version  IPv4</p> <p>IP address  Ephemeral</p> <p>Port    80</p> <p>Click Done.</p> <p>Click Add Frontend IP and port.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Protocol    HTTP</p> <p>IP version  IPv6</p> <p>IP address  Ephemeral</p> <p>Port    80</p> <p>Click Done.</p> <p>HTTP(S) load balancing supports both IPv4 and IPv6 addresses for client traffic. Client IPv6 requests are terminated at the global load balancing layer and then proxied over IPv4 to your backends.</p>"},{"location":"28-http-load-balancer/#review-and-create-the-http-load-balancer","title":"Review and create the HTTP load balancer","text":"<p>Click Review and finalize. Review the Backend services and Frontend. Click Create. Wait for the load balancer to be created. Click on the name of the load balancer (http-lb). Note the IPv4 and IPv6 addresses of the load balancer for the next task. They will be referred to as <code>[LB_IP_v4]</code> and <code>[LB_IP_v6]</code>, respectively. The IPv6 address is the one in hexadecimal format.</p>"},{"location":"28-http-load-balancer/#task-6-stress-test-the-http-load-balancer","title":"Task 6. Stress test the HTTP load balancer","text":"<p>Now that you have created the HTTP load balancer for your backends, it is time to verify that traffic is forwarded to the backend service.</p>"},{"location":"28-http-load-balancer/#access-the-http-load-balancer","title":"Access the HTTP load balancer","text":"<p>Open a new tab in your browser and navigate to <code>http://[LB_IP_v4]</code>. Make sure to replace <code>[LB_IP_v4]</code> with the IPv4 address of the load balancer. Accessing the HTTP load balancer might take a couple of minutes. In the meantime, you might get a 404 or 502 error. Keep trying until you see the page of one of the backends.</p>"},{"location":"28-http-load-balancer/#stress-test-the-http-load-balancer","title":"Stress test the HTTP load balancer","text":"<p>Create a new VM to simulate a load on the HTTP load balancer. Then determine whether traffic is balanced across both backends when the load is high.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine &gt; VM instances.</p> <p>Click Create instance.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    stress-test</p> <p>Region  us-west1</p> <p>Zone    us-west1-c</p> <p>Series  N1</p> <p>Machine type    f1-micro (1 vCPU)</p> <p>Because us-west1 is closer to us-central1 than to europe-west1, traffic should be forwarded only to us-central1-mig (unless the load is too high).</p> <p>For Boot Disk, click Change.</p> <p>Click Custom images.</p> <p>For Image, select mywebserver.</p> <p>Click Select.</p> <p>Click Create. Wait for the stress-test instance to be created.</p> <p>For stress-test, click SSH to launch a terminal and connect.</p> <p>To create an environment variable for your load balancer IP address, run the following command:</p> <pre><code>export LB_IP=&lt;Enter your [LB_IP_v4] here&gt;\n</code></pre> <p>Verify it with echo:</p> <pre><code>echo $LB_IP\n</code></pre> <p>To place a load on the load balancer, run the following command:</p> <pre><code>ab -n 500000 -c 1000 http://$LB_IP/\n</code></pre> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services &gt; Load balancing.</p> <p>Click Backends.</p> <p>Click http-backend.</p> <p>Monitor the Frontend Location (Total inbound traffic) between North America and the two backends for a couple of minutes.</p> <p>At first, traffic should just be directed to us-central1-mig, but as the RPS increases, traffic is also directed to europe-west1-mig. This demonstrates that by default traffic is forwarded to the closest backend, but if the load is very high, traffic can be distributed across the backends.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click Compute Engine &gt; Instance groups.</p> <p>Click on us-central1-mig to open the instance group page.</p> <p>Click Observability to monitor the number of instances and LB capacity.</p> <p>Repeat the same for the europe-west1-mig instance group.</p> <p>Depending on the load, you might see the backends scale to accommodate the load.</p>"},{"location":"29-internal-load-balancer/","title":"Configuring an Internal Load Balancer","text":""},{"location":"29-internal-load-balancer/#task-1-configure-internal-traffic-and-health-check-firewall-rules","title":"Task 1. Configure internal traffic and health check firewall rules.","text":"<p>Configure firewall rules to allow internal traffic connectivity from sources in the 10.10.0.0/16 range. This rule allows incoming traffic from any client located in the subnet.</p> <p>Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load-balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.</p>"},{"location":"29-internal-load-balancer/#explore-the-my-internal-app-network","title":"Explore the my-internal-app network","text":"<p>The network my-internal-app with subnet-a and subnet-b and firewall rules for RDP, SSH, and ICMP traffic have been configured for you.</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network &gt; VPC networks. Notice the my-internal-app network with its subnets: subnet-a and subnet-b.</p> <p>Each Google Cloud project starts with the default network. In addition, the my-internal-app network has been created for you as part of your network diagram.</p> <p>You will create the managed instance groups in subnet-a and subnet-b. Both subnets are in the us-central1 region because an internal load balancer is a regional service. The managed instance groups will be in different zones, making your service immune to zonal failures.</p>"},{"location":"29-internal-load-balancer/#create-the-firewall-rule-to-allow-traffic-from-any-sources-in-the-10100016-range","title":"Create the firewall rule to allow traffic from any sources in the 10.10.0.0/16 range","text":"<p>Create a firewall rule to allow traffic in the 10.10.0.0/16 subnet.</p> <p>On the Navigation menu (Navigation menu), click VPC network &gt; Firewall. Notice the app-allow-icmp and app-allow-ssh-rdp firewall rules.</p> <p>These firewall rules have been created for you.</p> <p>Click Create Firewall Rule.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    fw-allow-lb-access</p> <p>Network my-internal-app</p> <p>Targets Specified target tags</p> <p>Target tags backend-service</p> <p>Source filter   IP ranges</p> <p>Source IP ranges    10.10.0.0/16</p> <p>Protocols and ports Allow all</p> <p>Click create.</p>"},{"location":"29-internal-load-balancer/#create-the-health-check-rule","title":"Create the health check rule","text":"<p>Create a firewall rule to allow health checks.</p> <p>On the Navigation menu (Navigation menu), click VPC network &gt; Firewall.</p> <p>Click Create Firewall Rule.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    fw-allow-health-checks</p> <p>Network my-internal-app</p> <p>Targets Specified target tags</p> <p>Target tags backend-service</p> <p>Source filter   IP Ranges</p> <p>Source IP ranges    130.211.0.0/22 and 35.191.0.0/16</p> <p>Protocols and ports Specified protocols and ports</p> <p>Make sure to include the /22 and /16 in the Source IP ranges.</p> <p>For tcp, specify port 80.</p> <p>Click Create.</p>"},{"location":"29-internal-load-balancer/#task-2-create-a-nat-configuration-using-cloud-router","title":"Task 2: Create a NAT configuration using Cloud Router","text":"<p>The Google Cloud VM backend instances that you setup in Task 3 will not be configured with external IP addresses.</p> <p>Instead, you will setup the Cloud NAT service to allow these VM instances to send outbound traffic only through the Cloud NAT, and receive inbound traffic through the load balancer.</p>"},{"location":"29-internal-load-balancer/#create-the-cloud-router-instance","title":"Create the Cloud Router instance","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click Network services &gt; Cloud NAT.</p> <p>Click Get started.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Gateway name    nat-config</p> <p>VPC network my-internal-app</p> <p>Region  us-central1</p> <p>Click Cloud Router, and select Create new router.</p> <p>For Name, type nat-router-us-central1.</p> <p>Click Create.</p> <p>In Create a NAT gateway, click Create.</p> <p>Wait until the NAT Gateway Status changes to Running before moving onto the next task.</p>"},{"location":"29-internal-load-balancer/#task-3-configure-instance-templates-and-create-instance-groups","title":"Task 3. Configure instance templates and create instance groups","text":"<p>A managed instance group uses an instance template to create a group of identical instances. Use these to create the backends of the internal load balancer.</p>"},{"location":"29-internal-load-balancer/#configure-the-instance-templates","title":"Configure the instance templates","text":"<p>An instance template is an API resource that you can use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties. Create an instance template for both subnets of the my-internal-app network.</p> <p>On the Navigation menu (Navigation menu), click Compute Engine &gt; Instance templates.</p> <p>Click Create instance template.</p> <p>For Name, type instance-template-1</p> <p>Under Machine configuration, For Series, Select N1.</p> <p>Machine type f1-micro(1 vCPU).</p> <p>Click Management, security, disks, networking, sole tenancy.</p> <p>Click Management.</p> <p>Under Metadata, specify the following:</p> <p>Key Value</p> <p>startup-script-url  gs://cloud-training/gcpnet/ilb/startup.sh</p> <p>The startup-script-url specifies a script that is executed when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region, and zone of the VM instance. You can explore this script here.</p> <pre><code>#! /bin/bash\n\napt-get update \napt-get install -y apache2 php\napt-get install -y wget\ncd /var/www/html\nrm index.html -f\nrm index.php -f\nwget https://storage.googleapis.com/cloud-training/gcpnet/ilb/index.php\nMETA_REGION_STRING=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/zone\" -H \"Metadata-Flavor: Google\")\nREGION=`echo \"$META_REGION_STRING\" | awk -F/ '{print $4}'`\nsed -i \"s|region-here|$REGION|\" index.php\n</code></pre> <p>Click Networking.</p> <p>For Network interfaces, specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Network my-internal-app</p> <p>Subnet  subnet-a</p> <p>Network tags    backend-service</p> <p>External IP None</p> <p>The network tag backend-service ensures that the firewall rule to allow traffic from any sources in the 10.10.0.0/16 subnet and the Health Check firewall rule applies to these instances.</p> <p>Click Create. Wait for the instance template to be created.</p> <p>Create another instance template for subnet-b by copying instance-template-1:</p> <p>Select the instance-template-1 and click Copy.</p> <p>Click Management, security, disks, networking, sole tenancy.</p> <p>Click Networking.</p> <p>For Network interfaces, select subnet-b as the Subnet.</p> <p>Click Create.</p>"},{"location":"29-internal-load-balancer/#create-the-managed-instance-groups","title":"Create the managed instance groups","text":"<p>Create a managed instance group in subnet-a (us-central1-a) and subnet-b (us-central1-b).</p> <p>On the Navigation menu (Navigation menu), click Compute Engine &gt; Instance groups.</p> <p>Click Create Instance group.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    instance-group-1</p> <p>Location    Single zone</p> <p>Region  us-central1</p> <p>Zone    us-central1-a</p> <p>Instance template   instance-template-1</p> <p>Autoscaling &gt; metrics type (Click the pencil edit icon) CPU utilization</p> <p>Target CPU utilization  80, click Done.</p> <p>Cool-down period    45</p> <p>Minimum number of instances 1</p> <p>Maximum number of instances 5</p> <p>Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. Just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load.</p> <p>Click Create.</p> <p>Repeat the same procedure for instance-group-2 in us-central1-b:</p> <p>Click Create Instance group.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    instance-group-2</p> <p>Location    Single zone</p> <p>Region  us-central1</p> <p>Zone    us-central1-b</p> <p>Instance template   instance-template-2</p> <p>Autoscaling &gt; metric type (Click the pencil edit icon)  CPU utilization</p> <p>Target CPU utilization  80, click Done.</p> <p>Cool-down period    45</p> <p>Minimum number of instances 1</p> <p>Maximum number of instances 5</p> <p>Click Create.</p>"},{"location":"29-internal-load-balancer/#verify-the-backends","title":"Verify the backends","text":"<p>Verify that VM instances are being created in both subnets and create a utility VM to access the backends' HTTP sites.</p> <p>On the Navigation menu, click Compute Engine &gt; VM instances. Notice two instances that start with instance-group-1 and instance-group-2.</p> <p>These instances are in separate zones, and their internal IP addresses are part of the subnet-a and subnet-b CIDR blocks.</p> <p>Click Create Instance.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    utility-vm</p> <p>Region  us-central1</p> <p>Zone    us-central1-f</p> <p>Series  N1</p> <p>Machine type    f1-micro (1 vCPU)</p> <p>Boot disk   Debian GNU/Linux 10 (buster)</p> <p>Click Management, security, disks, networking, sole tenancy.</p> <p>Click Networking.</p> <p>For Network interfaces, click the pencil icon to edit.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Network my-internal-app</p> <p>Subnetwork  subnet-a</p> <p>Primary internal IP Ephemeral (Custom)</p> <p>Custom ephemeral IP address 10.10.20.50</p> <p>External IP None</p> <p>Click Done.</p> <p>Click Create.</p> <p>Note that the internal IP addresses for the backends are 10.10.20.2 and 10.10.30.2.</p> <p>If these IP addresses are different, replace them in the two curl commands below.</p> <p>For utility-vm, click SSH to launch a terminal and connect. If you see the Connection via Cloud Identity-Aware Proxy Failed popup, click Retry.</p> <p>To verify the welcome page for instance-group-1-xxxx, run the following command:</p> <pre><code>curl 10.10.20.2\n</code></pre> <p>The output should look like this (do not copy; this is example output):</p> <pre><code>&lt;h1&gt;Internal Load Balancing Lab&lt;/h1&gt;&lt;h2&gt;Client IP&lt;/h2&gt;Your IP address : 10.10.20.50&lt;h2&gt;Hostname&lt;/h2&gt;Server Hostname:\n instance-group-1-1zn8&lt;h2&gt;Server Location&lt;/h2&gt;Region and Zone: us-central1-a\n</code></pre> <p>To verify the welcome page for instance-group-2-xxxx, run the following command:</p> <pre><code>curl 10.10.30.2\n</code></pre> <p>The output should look like this (do not copy; this is example output):</p> <pre><code>&lt;h1&gt;Internal Load Balancing Lab&lt;/h1&gt;&lt;h2&gt;Client IP&lt;/h2&gt;Your IP address : 10.10.20.50&lt;h2&gt;Hostname&lt;/h2&gt;Server Hostname:\n instance-group-2-q5wp&lt;h2&gt;Server Location&lt;/h2&gt;Region and Zone: us-central1-b\n</code></pre> <p>This will be useful when verifying that the internal load balancer sends traffic to both backends.</p> <p>Close the SSH terminal to utility-vm</p>"},{"location":"29-internal-load-balancer/#task-4-configure-the-internal-load-balancer","title":"Task 4. Configure the internal load balancer","text":"<p>Configure the internal load balancer to balance traffic between the two backends (instance-group-1 in us-central1-a and instance-group-2 in us-central1-b)</p>"},{"location":"29-internal-load-balancer/#start-the-configuration","title":"Start the configuration","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click Network Services &gt; Load balancing. Click Create load balancer. Under TCP Load Balancing, click Start configuration. For Internet facing or internal only, select Only between my VMs. Choosing Only between my VMs makes this load balancer internal. This choice requires the backends to be in a single region (us-central1) and does not allow offloading TCP processing to the load balancer.</p> <p>Click Continue.</p> <p>For Name, type my-ilb.</p>"},{"location":"29-internal-load-balancer/#configure-the-regional-backend-service","title":"Configure the regional backend service","text":"<p>The backend service monitors instance groups and prevents them from exceeding configured usage.</p> <p>Click Backend configuration.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (select option as specified)</p> <p>Region  us-central1</p> <p>Network my-internal-app</p> <p>Instance group  instance-group-1 (us-central1-a)</p> <p>Click Done.</p> <p>Click Add backend.</p> <p>For Instance group, select instance-group-2 (us-central1-b).</p> <p>Click Done.</p> <p>For Health Check, select Create a health check.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (select option as specified)</p> <p>Name    my-ilb-health-check</p> <p>Protocol    TCP</p> <p>Port    80</p> <p>Check interval  10 sec</p> <p>Timeout 5 sec</p> <p>Healthy threshold   2</p> <p>Unhealthy threshold 3</p> <p>Health checks determine which instances can receive new connections. This HTTP health check polls instances every 10 seconds, waits up to 5 seconds for a response, and treats 2 successful or 3 failed attempts as healthy threshold or unhealthy threshold, respectively.</p> <p>Click Save and continue.</p> <p>Verify that there is a blue check mark next to Backend configuration in the Cloud Console. If there isn't, double-check that you have completed all the steps above.</p>"},{"location":"29-internal-load-balancer/#configure-the-frontend","title":"Configure the frontend","text":"<p>The frontend forwards traffic to the backend.</p> <p>Click Frontend configuration.</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Subnetwork  subnet-b</p> <p>Internal IP &gt; IP address    Reserve static internal IP address</p> <p>Specify the following, and leave the remaining settings as their defaults:</p> <p>Property    Value (type value or select option as specified)</p> <p>Name    my-ilb-ip</p> <p>Static IP address   Let me choose</p> <p>Custom IP address   10.10.30.5</p> <p>Click Reserve.</p> <p>For Ports, type 80.</p> <p>Click Done.</p>"},{"location":"29-internal-load-balancer/#review-and-create-the-internal-load-balancer","title":"Review and create the internal load balancer","text":"<p>Click Review and finalize. Review the Backend and Frontend. Click Create. Wait for the load balancer to be created before moving to the next task.</p>"},{"location":"29-internal-load-balancer/#task-5-test-the-internal-load-balancer","title":"Task 5. Test the internal load balancer","text":"<p>Verify that the my-ilb IP address forwards traffic to instance-group-1 in us-central1-a and instance-group-2 in us-central1-b.</p> <p>Access the internal load balancer On the Navigation menu, click Compute Engine &gt; VM instances.</p> <p>For utility-vm, click SSH to launch a terminal and connect.</p> <p>To verify that the internal load balancer forwards traffic, run the following command:</p> <pre><code>curl 10.10.30.5\n</code></pre> <p>The output should look like this (do not copy; this is example output):</p> <pre><code>&lt;h1&gt;Internal Load Balancing Lab&lt;/h1&gt;&lt;h2&gt;Client IP&lt;/h2&gt;Your IP address : 10.10.20.50&lt;h2&gt;Hostname&lt;/h2&gt;Server Hostname:\n instance-group-2-1zn8&lt;h2&gt;Server Location&lt;/h2&gt;Region and Zone: us-central1-b\n</code></pre> <p>As expected, traffic is forwarded from the internal load balancer (10.10.30.5) to the backend.</p> <p>Run the same command a couple of times</p>"},{"location":"30-deployment-manager/","title":"Automating the Deployment of Infrastructure Using Deployment Manager","text":""},{"location":"30-deployment-manager/#overview","title":"Overview","text":"<p>Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Cloud Platform services, such as Cloud Storage, Compute Engine, and Cloud SQL, configured to work together.</p> <p>In this lab, you create a Deployment Manager configuration with a template to automate the deployment of Google Cloud infrastructure. Specifically, you deploy one auto mode network with a firewall rule and two VM instances</p>"},{"location":"30-deployment-manager/#objectives","title":"Objectives","text":"<p>Create a configuration for an auto mode network</p> <p>Create a configuration for a firewall rule</p> <p>Create a template for VM instances</p> <p>Create and deploy a configuration</p> <p>Verify the deployment of a configuration</p>"},{"location":"30-deployment-manager/#task-1-configure-the-network","title":"Task 1. Configure the network","text":"<p>A configuration describes all the resources you want for a single deployment.</p>"},{"location":"30-deployment-manager/#verify-that-the-deployment-manager-api-is-enabled","title":"Verify that the Deployment Manager API is enabled","text":"<p>In the Cloud Console, on the Navigation menu (Navigation menu), click APIs &amp; services &gt; Library.</p> <p>In the search bar, type Deployment Manager, and click the result for Cloud Deployment Manager V2 API.</p>"},{"location":"30-deployment-manager/#start-the-cloud-shell-editor","title":"Start the Cloud Shell Editor","text":"<p>To write the configuration and the template, you use the Cloud Shell Editor.</p> <p>In the Cloud Console, click Activate Cloud Shell (Cloud Shell).</p> <p>If prompted, click Continue.</p> <p>Run the following commands:</p> <p>mkdir dminfra cd dminfra</p> <p>In Cloud Shell, click Open Editor (Cloud Shell Editor).</p> <p>In the left pane of the code editor, expand the dminfra folder.</p>"},{"location":"30-deployment-manager/#create-the-auto-mode-network-configuration","title":"Create the auto mode network configuration","text":"<p>A configuration is a file written in YAML syntax that lists each of the resources you want to create and their respective resource properties. A configuration must contain a resources: section followed by the list of resources to create. Start the configuration with the mynetwork resource.</p> <p>To create a new file, click File &gt; New File.</p> <p>Name the new file config.yaml, and then open it.</p> <p>Copy the following base code into config.yaml:</p> <pre><code>resources:\n# Create the auto-mode network\n- name: [RESOURCE_NAME]\n  type: [RESOURCE_TYPE]\n  properties:\n    #RESOURCE properties go here\n</code></pre> <p>In config.yaml, replace <code>[RESOURCE_NAME]</code> with mynetwork</p> <p>To get a list of all available network resource types in Google Cloud, run the following command in Cloud Shell:</p> <pre><code>gcloud deployment-manager types list | grep network\n</code></pre> <p>The output should look like this (do not copy; this is example output):</p> <pre><code>compute.beta.subnetwork\ncompute.alpha.subnetwork\ncompute.v1.subnetwork\ncompute.beta.network\ncompute.v1.network\ncompute.alpha.network\n</code></pre> <p>Locate compute.v1.network, which is the type needed to create a VPC network using Deployment Manager.</p> <p>By definition, an auto mode network automatically creates a subnetwork in each region. </p>"},{"location":"30-deployment-manager/#task-2-configure-the-firewall-rule","title":"Task 2. Configure the firewall rule","text":"<p>In order to allow ingress traffic instances in mynetwork, you need to create a firewall rule.</p>"},{"location":"30-deployment-manager/#add-the-firewall-rule-to-the-configuration","title":"Add the firewall rule to the configuration","text":"<p>Add a firewall rule that allows HTTP, SSH, RDP, and ICMP traffic on mynetwork.</p> <p>To get a list of all available firewall rule resource types in Google Cloud, run the following command in Cloud Shell:</p> <pre><code>gcloud deployment-manager types list | grep firewall\n</code></pre> <p>The output should look like this (do not copy; this is example output):</p> <pre><code>compute.v1.firewall\ncompute.alpha.firewall\ncompute.beta.firewall\n</code></pre> <p>Locate compute.v1.firewall, which is the type needed to create a firewall rule using Deployment Manager.</p> <p>The final <code>config.yaml</code></p> <pre><code>imports:\n- path: instance-template.jinja\n\nresources:\n# Create the auto-mode network\n- name: mynetwork\n  type: compute.v1.network\n  properties:\n    autoCreateSubnetworks: true\n# Create the firewall rule\n- name: mynetwork-allow-http-ssh-rdp-icmp\n  type: compute.v1.firewall\n  properties:\n    network: $(ref.mynetwork.selfLink)\n    sourceRanges: [\"0.0.0.0/0\"]\n    allowed:\n    - IPProtocol: TCP\n      ports: [22, 80, 3389]\n    - IPProtocol: ICMP\n\n# Create the mynet-us-vm instance\n- name: mynet-us-vm\n  type: instance-template.jinja\n  properties:\n    zone: us-central1-a\n    machineType: n1-standard-1\n    network: $(ref.mynetwork.selfLink)\n    subnetwork: regions/us-central1/subnetworks/mynetwork\n\n# Create the mynet-eu-vm instance\n- name: mynet-eu-vm\n  type: instance-template.jinja\n  properties:\n    zone: europe-west1-d\n    machineType: n1-standard-1\n    network: $(ref.mynetwork.selfLink)  \n    subnetwork: regions/europe-west1/subnetworks/mynetwork\n</code></pre>"},{"location":"30-deployment-manager/#task-3-create-a-template-for-vm-instances","title":"Task 3. Create a template for VM instances","text":"<p>Deployment Manager allows you to use Python or Jinja2 templates to parameterize your configuration. This allows you to reuse common deployment paradigms such as networks, firewall rules, and VM instances.</p>"},{"location":"30-deployment-manager/#create-the-vm-instance-template","title":"Create the VM instance template","text":"<p>Because you will be creating two similar VM instances, create a VM instance template.</p> <p>To create a new file, click File &gt; New File.</p> <p>Name the new file instance-template.jinja, and then open it.</p> <p>Properties to define:</p> <p>machineType: Machine type and zone zone: Instance zone networkInterfaces: Network and subnetwork that VM is attached to accessConfigs: Required to give the instance a public IP address (required in this lab). To create instances with only an internal IP address, remove the accessConfigs section. disks: The boot disk, its name and image Most properties are defined as template properties, which you will provide values for from the top-level configuration (config.yaml).</p> <p>To get a list of all available instance resource types in Google Cloud, run the following command in Cloud Shell:</p> <pre><code>gcloud deployment-manager types list | grep instance\n</code></pre> <p>Locate compute.v1.instance, which is the type needed to create a VM instance using Deployment Manager.</p> <p>Final <code>instance-template.jinja</code>:</p> <pre><code> resources:\n- name: {{ env[\"name\"] }}\n  type: compute.v1.instance  \n  properties:\n     machineType: zones/{{ properties[\"zone\"] }}/machineTypes/{{ properties[\"machineType\"] }}\n     zone: {{ properties[\"zone\"] }}\n     networkInterfaces:\n      - network: {{ properties[\"network\"] }}\n        subnetwork: {{ properties[\"subnetwork\"] }}\n        accessConfigs:\n        - name: External NAT\n          type: ONE_TO_ONE_NAT\n     disks:\n      - deviceName: {{ env[\"name\"] }}\n        type: PERSISTENT\n        boot: true\n        autoDelete: true\n        initializeParams:\n          sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/family/debian-9\n</code></pre>"},{"location":"30-deployment-manager/#deploy-the-configuration","title":"Deploy the configuration","text":"<pre><code>gcloud deployment-manager deployments create dminfra --config=config.yaml --preview\n</code></pre> <pre><code>gcloud deployment-manager deployments update dminfra\n</code></pre> <p>Or, directly deploy:</p> <pre><code>gcloud deployment-manager deployments create dminfra --config=config.yaml\n</code></pre> <p>Delete using:</p> <pre><code>gcloud deployment-manager deployments delete dminfra\n</code></pre>"},{"location":"30-deployment-manager/#task-5-verify-your-deployment","title":"Task 5. Verify your deployment","text":"<p>Verify your network in the Cloud Console</p> <p>In the Cloud Console, on the Navigation menu (Navigation menu), click VPC network &gt; VPC networks.</p> <p>View the mynetwork VPC network with a subnetwork in every region.</p> <p>On the Navigation menu, click VPC network &gt; Firewall.</p> <p>Sort the firewall rules by Network.</p> <p>View the mynetwork-allow-http-ssh-rdp-icmp firewall rule for mynetwork.</p> <p>Verify your VM instances in the Cloud Console</p> <p>On the Navigation menu (Navigation menu), click Compute Engine &gt; VM instances.</p> <p>View the mynet-us-vm and mynet-eu-vm instances.</p> <p>Note the internal IP address for mynet-eu-vm.</p> <p>For mynet-us-vm, click SSH to launch a terminal and connect.</p> <p>To test connectivity to mynet-eu-vm's internal IP address, run the following command in the SSH terminal (replacing mynet-eu-vm's internal IP address with the value noted earlier):</p> <pre><code>ping -c 3 &lt;Enter mynet-eu-vm's internal IP here&gt;\n</code></pre> <p>This should work because both VM instances are on the same network and the firewall rule allows ICMP traffic!</p>"},{"location":"31-aws-session/","title":"31 aws session","text":""},{"location":"31-aws-session/#date-april-1-2022","title":"Date: April 1. 2022","text":""},{"location":"31-aws-session/#module-1","title":"Module 1","text":"<p>Client server model: We are making a request and some server in the backend is fulfilling the request.</p> <p>Fundamental principles of cloud computing:  1. Services on demand 1. Avoid large upfront investment/capex 1. Provision computing resources as needed - elasticity 1. Pay for what we use</p> <p>Baked in vs Bolt on</p> <p>AWS has 200+ services ready to go</p> <p>AWS core services categories 1. compute 1. networking an content delivery 1. storage 1. database 1. security, identity and compliance 1. monitoring and analytics</p>"},{"location":"31-aws-session/#module-2-compute","title":"Module 2: compute","text":"<p>Instance types :  1. General purpose : t series  1. Compute optimized :  1. Accelerated computing :  1. Storage optimized : high IOPS 1. Memory optimized</p> <p>ASG = auto scaling group = add or removing instance</p> <p>ELB = Elastic load balancer = distributes workload across several amazon EC2 instances, provide a single point of contact for the asg</p> <p>Messaging services</p> <p>Monolithic appliation vs micro services  SQS = Simple Queue service SNS = Simple notification service = sending messgaes for the specific topics they have subscribed to</p> <p>AWS Lambda AWS Containers = ECS vs EKS AWS Fargate = serverless containers where orchestration is taken care of. higher level of abstraction than EKS. </p>"},{"location":"31-aws-session/#module-3-global-infrastructure-and-reliability","title":"Module 3: Global infrastructure and reliability","text":"<p>Regions consists of 2+ availability zones. </p> <p>Within the availability zones there will be multiple data centers.</p> <p>Cloudfront is CDN</p>"},{"location":"31-aws-session/#module-4-network","title":"Module 4: Network","text":"<p>Network access Control List ? = stateless security groups are stateful and deny all inbound traffic by default internet gateway is used to [?] Route 53 - DNS port number is 53</p> <p>Missed this session :(</p>"},{"location":"31-aws-session/#module-5-database-and-storage","title":"Module 5: Database and storage","text":"<p>Storage * block storage  * ec2 instances has local storage called local storage. <code>Ephemeral storage</code>  * ebs snapshots - stores delta in data for backup * s3 object storage with 6 classes  * file storage - multiple clients can access same data with a shared file structure</p> <p>Databases  * relational database service - a managed service for the data service * amazon aurora -      * 6 copies in 3 availability zones      * for high data durability     * cut down on IOPS and reduce costs thereby * dynamoDB - serverless key-value database * aws database migration service  * redshift - data warehouse for etl jobs * documentDB - for mongoDB * neptune - highly connected dataset * qldb - blockchain * managed blockchain for ledger database * elasticache - caching layer to improve performance * dynamoBD accelerator - improve performance </p>"},{"location":"31-aws-session/#module-6-security","title":"Module 6: Security","text":"<ul> <li>Shared responsibility model</li> <li>DDoS attack prevention : AWS Shield </li> <li>AWS Key Management Service - create and manage crypto keys </li> </ul> <p>Missed this section :(</p>"},{"location":"31-aws-session/#module-7-monitoring","title":"Module 7: Monitoring","text":"<ul> <li>cloudtrail- log service, track user activity and api usage</li> <li>cloudwatch - monitor architecture, aws resources and applications, metrics from a dashbaord</li> <li>aws trusted advisor - reduce costs, improve performance, improve security. dashboard shows cost optimization, performance, security, fault tolerance, service limits - no problem green, recommendations yellow, actions red</li> </ul>"},{"location":"31-aws-session/#module-8-pricing-and-support","title":"Module 8: Pricing and support","text":"<p>Pricing: pay as you go, commitment for 1-3 years,volume discount.</p>"},{"location":"31-aws-session/#module-9-migration-and-innovation","title":"Module 9: Migration and Innovation","text":"<p>AWS cloud adoption framework Six areas of focus , in 2 key areas  1. Business capabilities      * Business     * People     * Governance 1. Technical capabilities     * Platform     * Security     * Operations</p> <p>Six R's - migration strategies * Rehost - lift &amp; shift * Replatform * Refactor / rearchitect  * Repurchase * Retain * Retire</p> <p>AWS Snow Family * Copy data using a device</p> <p>Five pillars of well architected framework</p>"},{"location":"31-aws-session/#module-10-preparing-for-the-certified-cloud-practitioner","title":"Module 10: Preparing for the certified cloud practitioner","text":"<p>4 domains : 1. cloud concepts 2. security and compliance 3. technology 4. billing and pricing</p>"},{"location":"32-gcp-cloud-build/","title":"32 gcp cloud build","text":""},{"location":"32-gcp-cloud-build/#enable-apis","title":"Enable APIs","text":"<ul> <li>Cloud Build API</li> <li>Container Registry API</li> </ul>"},{"location":"32-gcp-cloud-build/#building-containers-with-dockerfile-and-cloud-build","title":"Building Containers with DockerFile and Cloud Build","text":"<p>You can write build configuration files to provide instructions to Cloud Build as to which tasks to perform when building a container. These build files can fetch dependencies, run unit tests, analyses and more. In this task, you'll create a DockerFile and use it as a build configuration script with Cloud Build. You will also create a simple shell script (quickstart.sh) which will represent an application inside the container.</p> <p>On the Google Cloud Console title bar, click Activate Cloud Shell.</p> <p>When prompted, click Continue.</p> <p>Cloud Shell opens at the bottom of the Google Cloud Console window.</p> <p>Create an empty <code>quickstart.sh</code></p> <p>Add:</p> <pre><code>#!/bin/sh\necho \"Hello, world! The time is $(date).\"\n</code></pre> <p>Create an Dockerfile</p> <pre><code>FROM alpine\nCOPY quickstart.sh /\nCMD [\"/quickstart.sh\"]\n</code></pre> <p>In Cloud Shell, run the following command to make the quickstart.sh script executable.</p> <pre><code>chmod +x quickstart.sh\n</code></pre> <p>In Cloud Shell, run the following command to build the Docker container image in Cloud Build.</p> <pre><code>gcloud builds submit --tag gcr.io/${GOOGLE_CLOUD_PROJECT}/quickstart-image .\n</code></pre> <p>Docker image is built and pushed to Container Registry.</p>"},{"location":"32-gcp-cloud-build/#building-containers-with-a-build-configuration-file-and-cloud-build","title":"Building Containers with a build configuration file and Cloud Build","text":"<p>Cloud Build also supports custom build configuration files. In this task you will incorporate an existing Docker container using a custom YAML-formatted build file with Cloud Build.</p> <p>In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> <p>Create a soft link as a shortcut to the working directory.</p> <pre><code>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s\n</code></pre> <p>Change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Cloud_Build/a\n</code></pre> <p>A sample custom cloud build configuration file called cloudbuild.yaml has been provided for you in this directory as well as copies of the Dockerfile and the quickstart.sh script you created in the first task.</p> <p>View the could build configuration file.</p> <pre><code>cat cloudbuild.yaml\n</code></pre> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]\nimages:\n- 'gcr.io/$PROJECT_ID/quickstart-image'\n</code></pre> <p>This file instructs Cloud Build to use Docker to build an image using the Dockerfile specification in the current local directory, tag it with <code>gcr.io/$PROJECT_ID/quickstart-image</code> (<code>$PROJECT_ID</code> is a substitution variable automatically populated by Cloud Build with the project ID of the associated project) and then push that image to Container Registry.</p> <p>In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:</p> <pre><code>gcloud builds submit --config cloudbuild.yaml .\n</code></pre> <p>The build output to Cloud Shell should be the same as before. When the build completes, a new version of the same image is pushed to Container Registry.</p>"},{"location":"32-gcp-cloud-build/#building-and-testing-containers-with-a-build-configuration-file-and-cloud-build","title":"Building and Testing Containers with a build configuration file and Cloud Build","text":"<p>The true power of custom build configuration files is their ability to perform other actions, in parallel or in sequence, in addition to simply building containers: running tests on your newly built containers, pushing them to various destinations, and even deploying them to Kubernetes Engine. In this lab, we will see a simple example: a build configuration file that tests the container it built and reports the result to its calling environment.</p> <p>In Cloud Shell, change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Cloud_Build/b\n</code></pre> <p>Three files are necessary for this step. <code>quickstart.sh</code></p> <pre><code>#!/bin/sh\nif [ -z \"$1\" ]\nthen\n        echo \"Hello, world! The time is $(date).\"\n        exit 0\nelse \n        exit 1\nfi\n</code></pre> <p><code>cloudbuild.yaml</code></p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]\n- name: 'gcr.io/$PROJECT_ID/quickstart-image'\n  args: ['fail']\nimages:\n- 'gcr.io/$PROJECT_ID/quickstart-image\n</code></pre> <p><code>Dockerfile</code></p> <pre><code>FROM alpine\nCOPY quickstart.sh /\nCMD [\"/quickstart.sh\"]\n</code></pre> <p>In Cloud Shell, execute the following command to start a Cloud Build using cloudbuild.yaml as the build configuration file:</p> <pre><code>gcloud builds submit --config cloudbuild.yaml .\n</code></pre> <p>Output from the command that ends with <code>FAILURE</code></p> <p>Confirm that your command shell knows that the build failed</p> <pre><code>echo $?\n</code></pre>"},{"location":"33-gcp-deploying-kubernetes/","title":"33 gcp deploying kubernetes","text":""},{"location":"33-gcp-deploying-kubernetes/#objectives","title":"Objectives","text":"<p>Use the Google Cloud Console to build and manipulate GKE clusters</p> <p>Use the Google Cloud Console to deploy a Pod</p> <p>Use the Google Cloud Console to examine the cluster and Pods</p>"},{"location":"33-gcp-deploying-kubernetes/#task-1-deploy-gke-clusters","title":"Task 1. Deploy GKE clusters","text":"<p>Using cloud console:</p> <p>On navigation menu, click on Kubernetes Engine &gt; Clusters</p> <p>Click Create to begin creating a GKE cluster. In the GKE Standard cluster option select configure in the next screen.</p> <p>Examine the console UI and the controls to change the cluster name, the cluster location, Kubernetes version, the number of nodes, and the node resources such as the machine type in the default node pool.</p> <p>Clusters can be created across a region or in a single zone. A single zone is the default. When you deploy across a region the nodes are deployed to three separate zones and the total number of nodes deployed will be three times higher.</p> <p>Change the cluster name to standard-cluster-1 and zone to us-central1-a. Leave all the values at their defaults and click Create. The cluster begins provisioning.</p> <p>Note: You need to wait a few minutes for the cluster deployment to complete.</p> <p>Click the cluster name standard-cluster-1 to view the cluster details</p> <p>You can scroll down the page to view more details.</p> <p>Click the Storage and Nodes tabs under the cluster name (standard-cluster-1) at the top to view more of the cluster details.</p>"},{"location":"33-gcp-deploying-kubernetes/#task-2-modify-gke-clusters","title":"Task 2. Modify GKE clusters","text":"<p>It is easy to modify many of the parameters of existing clusters using either the Google Cloud Console or Cloud Shell. In this task, you use the Google Cloud Console to modify the size of GKE clusters.</p> <p>In the Google Cloud Console, click NODES at the top of the details page for standard-cluster-1. In Node Pools section, click default-pool. In the Google Cloud Console, click RESIZE at the top of the Node Pool Details page. Change the number of nodes from 3 to 4 and click RESIZE.</p>"},{"location":"33-gcp-deploying-kubernetes/#task-3-deploy-a-sample-workload","title":"Task 3. Deploy a sample workload","text":"<p>In this task, using the Google Cloud console you will deploy a Pod running the nginx web server as a sample workload.</p> <p>In the Google Cloud Console, on the Navigation menu( Navigation menu), click Kubernetes Engine &gt; Workloads.</p> <p>Click Deploy to show the Create a deployment wizard.</p> <p>Click Continue to accept the default container image, nginx:latest, which deploys 3 Pods each with a single container running the latest version of nginx. Scroll to the bottom of the window and click the Deploy button leaving the Configuration details at the defaults.</p> <p>When the deployment completes your screen will refresh to show the details of your new nginx deployment.</p>"},{"location":"33-gcp-deploying-kubernetes/#task-4-view-details-about-workloads-in-the-google-cloud-console","title":"Task 4. View details about workloads in the Google Cloud Console","text":"<p>In this task, you view details of your GKE workloads directly in the Google Cloud Console.</p> <p>In the Google Cloud Console, on the Navigation menu (Navigation menu), click Kubernetes Engine &gt; Workloads.</p> <p>In the Google Cloud Console, on the Kubernetes Engine &gt; Workloads page, click nginx-1. You may see Pods (3/3) as the default deployment will start with three pods but will scale back to 1 after a few minutes. You can continue with the lab.</p> <p>This displays the overview information for the workload showing details like resource utilization charts, links to logs, and details of the Pods associated with this workload.</p> <p>In the Google Cloud Console, click the Details tab for the nginx-1 workload. The Details tab shows more details about the workload including the Pod specification, number and status of Pod replicas and details about the horizontal Pod autoscaler.</p> <p>Click the Revision History tab. This displays a list of the revisions that have been made to this workload.</p> <p>Click the Events tab. This tab lists events associated with this workload.</p> <p>And then the YAML tab. This tab provides the complete YAML file that defines this components and full configuration of this sample workload.</p> <p>Still in the Google Cloud Console's Details tab for the nginx-1 workload, click the Overview tab, scroll down to the Managed Pods section and click the name of one of the Pods to view the details page for that Pod.</p> <p>Note:</p> <p>The default deployment will start with three pods but will scale back to 1 after a few minutes so you may need to refresh the Overview page to make sure you have a valid Pod to inspect.</p> <p>The Pod Details page provides information on the Pod configuration and resource utilization and the node where the Pod is running.</p> <p>In the Pod details page, you can click the Events and Logs tabs to view event details and links to container logs in Cloud Operations.</p> <p>Click the YAML tab to view the detailed YAML file for the Pod configuration.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/","title":"Creating Google Kubernetes Engine Deployments","text":""},{"location":"34-gcp-creating-kubernetes-engine-deployments/#objectives","title":"Objectives","text":"<p>Create deployment manifests, deploy to cluster, and verify Pod rescheduling as nodes are disabled</p> <p>Trigger manual scaling up and down of Pods in deployments</p> <p>Trigger deployment rollout (rolling update to new version) and rollbacks</p> <p>Perform a Canary deployment</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#activate-cloud-shell","title":"Activate cloud shell","text":"<p>Google Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Google Cloud Shell provides command-line access to your GCP resources.</p> <p>This can be activated from the Cloud shell button on the GCP top right toolbar.</p> <p><code>gcloud</code> is the command-line tool for Google Cloud Platform. It comes pre-installed on Cloud Shell and supports tab-completion.</p> <p>list the active account name with this command:</p> <pre><code>gcloud auth list\n</code></pre> <p>list the project ID with this command:</p> <pre><code>gcloud config list project\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-1-create-deployment-manifests-and-deploy-to-the-cluster","title":"Task 1. Create deployment manifests and deploy to the cluster","text":"<p>In this task, you create a deployment manifest for a Pod inside the cluster.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#connect-to-the-lab-gke-cluster","title":"Connect to the lab GKE cluster","text":"<p>In Cloud Shell, type the following command to set the environment variable for the zone and cluster name.</p> <pre><code>export my_zone=us-central1-a\nexport my_cluster=standard-cluster-1\n</code></pre> <p>Configure kubectl tab completion in Cloud Shell.</p> <pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command:</p> <pre><code>gcloud container clusters get-credentials $my_cluster --zone $my_zone\n</code></pre> <p>In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> <p>Create a soft link as a shortcut to the working directory.</p> <pre><code>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s\n</code></pre> <p>Change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Deployments/\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#create-a-deployment-manifest","title":"Create a deployment manifest","text":"<p>You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n</code></pre> <p>To deploy your manifest, execute the following command:</p> <pre><code>kubectl apply -f ./nginx-deployment.yaml\n</code></pre> <p>To view a list of deployments, execute the following command:</p> <pre><code>kubectl get deployments\n</code></pre> <p>The output should look like this example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY      UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3        3            0           3s\n</code></pre> <p>Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           42s\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-2-manually-scale-up-and-down-the-number-of-pods-in-deployments","title":"Task 2. Manually scale up and down the number of Pods in deployments","text":"<p>Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero.</p> <p>In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#scale-pods-up-and-down-in-the-console","title":"Scale Pods up and down in the console","text":"<p>Switch to the Google Cloud Console tab. On the Navigation menu, click Kubernetes Engine &gt; Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS &gt; Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#scale-pods-up-and-down-in-the-shell","title":"Scale Pods up and down in the shell","text":"<p>Switch back to the Cloud Shell browser tab.</p> <p>In the Cloud Shell, to view a list of Pods in the deployments, execute the following command:</p> <pre><code>kubectl get deployments \n</code></pre> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   1/1     1            1           3m\n</code></pre> <p>To scale the Pod back up to three replicas, execute the following command:</p> <pre><code>kubectl scale --replicas=3 deployment nginx-deployment\n</code></pre> <p>To view a list of Pods in the deployments, execute the following command:</p> <pre><code>kubectl get deployments\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-3-trigger-a-deployment-rollout-and-a-deployment-rollback","title":"Task 3. Trigger a deployment rollout and a deployment rollback","text":"<p>A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout.</p> <p>In this task, you trigger deployment rollout, and then you trigger deployment rollback.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#trigger-a-deployment-rollout","title":"Trigger a deployment rollout","text":"<p>To update the version of nginx in the deployment, execute the following command:</p> <pre><code>kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record\n</code></pre> <p>This updates the container image in your Deployment to nginx v1.9.1. The <code>record</code> flag is being deprecated.</p> <p>To view the rollout status, execute the following command:</p> <pre><code>kubectl rollout status deployment.v1.apps/nginx-deployment\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>Waiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 old replicas pending termination...\nWaiting for rollout to finish: 1 old replicas pending termination...\ndeployment \"nginx-deployment\" successfully rolled out\n</code></pre> <p>To verify the change, get the list of deployments.</p> <pre><code>kubectl get deployments\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           6m\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#view-the-rollout-history-of-the-deployment","title":"View the rollout history of the deployment.","text":"<pre><code>kubectl rollout history deployment nginx-deployment\n</code></pre> <p>The output should look like the example. Your output might not be an exact match.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\"\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#trigger-a-deployment-rollback","title":"Trigger a deployment rollback","text":"<p>To roll back an object's rollout, you can use the kubectl rollout undo command.</p> <p>To roll back to the previous version of the nginx deployment, execute the following command:</p> <pre><code>kubectl rollout undo deployments nginx-deployment\n</code></pre> <p>View the updated rollout history of the deployment.</p> <pre><code>kubectl rollout history deployment nginx-deployment\n</code></pre> <p>The output should look like the example. Your output might not be an exact match.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\"\nREVISION  CHANGE-CAUSE\n2         kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true\n3         &lt;none&gt;\n</code></pre> <p>View the details of the latest deployment revision</p> <pre><code>kubectl rollout history deployment/nginx-deployment --revision=3\n</code></pre> <p>The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\" with revision #3\nPod Template:\n  Labels:       app=nginx\n        pod-template-hash=3123191453\n  Containers:\n   nginx:\n    Image:      nginx:1.7.9\n    Port:       80/TCP\n    Host Port:  0/TCP\n    Environment:        &lt;none&gt;\n    Mounts:     &lt;none&gt;\n  Volumes:      &lt;none&gt;\n</code></pre>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#define-service-types-in-the-manifest","title":"Define service types in the manifest","text":"<p>A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 60000\n    targetPort: 80\n</code></pre> <p>In the Cloud Shell, to deploy your manifest, execute the following command:</p> <pre><code>kubectl apply -f ./service-nginx.yaml\n</code></pre> <p>This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#verify-the-loadbalancer-creation","title":"Verify the LoadBalancer creation","text":"<p>To view the details of the nginx service, execute the following command:</p> <pre><code>kubectl get service nginx\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME      CLUSTER_IP      EXTERNAL_IP      PORT(S)   SELECTOR    AGE\nnginx     10.X.X.X        X.X.X.X          60000/TCP    run=nginx   1m\n</code></pre> <p>When the external IP appears, open http://[EXTERNAL_IP]:60000/ in a new browser tab to see the server being served through network load balancing.</p> <p>It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#task-5-perform-a-canary-deployment","title":"Task 5. Perform a canary deployment","text":"<p>A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-canary\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: canary\n        Version: 1.9.1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n</code></pre> <p>The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment.</p> <p>Create the canary deployment based on the configuration file.</p> <pre><code>kubectl apply -f nginx-canary.yaml\n</code></pre> <p>When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present.</p> <pre><code>kubectl get deployments\n</code></pre> <p>Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page.</p> <p>Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas.</p> <pre><code>kubectl scale --replicas=0 deployment nginx-deployment\n</code></pre> <p>Verify that the only running replica is now the Canary deployment:</p> <pre><code>kubectl get deployments\n</code></pre> <p>Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment.</p>"},{"location":"34-gcp-creating-kubernetes-engine-deployments/#session-affinity","title":"Session affinity","text":"<p>The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections.</p> <p>For example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: LoadBalancer\n  sessionAffinity: ClientIP\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 60000\n    targetPort: 80\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/","title":"Configuring persistent volume for Google Kubernetes Engine","text":""},{"location":"35-gcp-persistent-volume-for-gke/#objectives","title":"Objectives","text":"<p>Create manifests for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) for Google Cloud persistent disks (dynamically created or existing)</p> <p>Mount Google Cloud persistent disk PVCs as volumes in Pods</p> <p>Use manifests to create StatefulSets</p> <p>Mount Google Cloud persistent disk PVCs as volumes in StatefulSets</p> <p>Verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted</p>"},{"location":"35-gcp-persistent-volume-for-gke/#task-1-create-pvs-and-pvcs","title":"Task 1. Create PVs and PVCs","text":"<p>In this task, you create a PVC, which triggers Kubernetes to automatically create a PV.</p> <p>Connect to the lab GKE cluster In Cloud Shell, type the following command to set the environment variable for the zone and cluster name.</p> <pre><code>export my_zone=us-central1-a\nexport my_cluster=standard-cluster-1\n</code></pre> <p>Configure tab completion for the kubectl command-line tool.</p> <pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>Configure access to your cluster for kubectl:</p> <pre><code>gcloud container clusters get-credentials $my_cluster --zone $my_zone\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#create-and-apply-a-manifest-with-a-pvc","title":"Create and apply a manifest with a PVC","text":"<p>Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you.</p> <p>You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: hello-web-disk\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n</code></pre> <p>In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> <p>Create a soft link as a shortcut to the working directory.</p> <pre><code>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s\n</code></pre> <p>Change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Storage/\n</code></pre> <p>To show that you currently have no PVCs, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Output :</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>To create the PVC, execute the following command:</p> <pre><code>kubectl apply -f pvc-demo.yaml\n</code></pre> <p>To show your newly created PVC, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Partial output </p> <pre><code>NAME           STATUS VOLUME        CAP   ACCESS MODES      CLASS   AGE\nhello-web-disk Bound  pvc-8...34   30Gi            RWO   standard    5s\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#task-2-mount-and-verify-google-cloud-persistent-disk-pvcs-in-pods","title":"Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods","text":"<p>In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod.</p>"},{"location":"35-gcp-persistent-volume-for-gke/#mount-the-pvc-to-a-pod","title":"Mount the PVC to a Pod","text":"<p>The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated.</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n    - name: frontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: pvc-demo-volume\n  volumes:\n    - name: pvc-demo-volume\n      persistentVolumeClaim:\n        claimName: hello-web-disk\n</code></pre> <p>To create the Pod with the volume, execute the following command:</p> <pre><code>kubectl apply -f pod-volume-demo.yaml\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output </p> <pre><code>NAME          READY    STATUS              RESTARTS   AGE\npvc-demo-pod  0/1      ContainerCreating   0          18s\n</code></pre> <p>If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\".</p> <p>To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it pvc-demo-pod -- sh\n</code></pre> <p>To create a simple text message as a web page in the Pod enter the following commands:</p> <pre><code>echo Test webpage in a persistent volume!&gt;/var/www/html/index.html\nchmod +x /var/www/html/index.html\n</code></pre> <p>Verify the text file contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output :</p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#test-the-persistence-of-the-pv","title":"Test the persistence of the PV","text":"<p>You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact.</p> <p>Delete the pvc-demo-pod.</p> <pre><code>kubectl delete pod pvc-demo-pod\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output </p> <pre><code>No resources found in default namespace.\n</code></pre> <p>There should be no Pods on the cluster.</p> <p>To show your PVC, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Partial output </p> <pre><code>NAME           STATUS VOLUME        CAP   ACCESS MODES      CLASS   AGE\nhello-web-disk Bound  pvc-8...34   30Gi            RWO   standard   22m\n</code></pre> <p>Your PVC still exists, and was not deleted when the Pod was deleted.</p> <p>Redeploy the pvc-demo-pod.</p> <pre><code>kubectl apply -f pod-volume-demo.yaml\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output </p> <pre><code>NAME           READY     STATUS    RESTARTS   AGE\npvc-demo-pod   1/1       Running   0          15s\n</code></pre> <p>The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created.</p> <p>To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it pvc-demo-pod -- sh\n</code></pre> <p>To verify that the text file still contains your message execute the following command:</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output </p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated.</p> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#task-3-create-statefulsets-with-pvcs","title":"Task 3. Create StatefulSets with PVCs","text":"<p>In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers.</p>"},{"location":"35-gcp-persistent-volume-for-gke/#release-the-pvc","title":"Release the PVC","text":"<p>Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod:</p> <pre><code>kubectl delete pod pvc-demo-pod\n</code></pre> <p>Confirm the Pod has been removed.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output </p> <pre><code>No resources found in default namespace.\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#create-a-statefulset","title":"Create a StatefulSet","text":"<p>The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task.</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: statefulset-demo-service\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: statefulset-demo\nspec:\n  selector:\n    matchLabels:\n      app: MyApp\n  serviceName: statefulset-demo-service\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: MyApp\n    spec:\n      containers:\n      - name: stateful-set-container\n        image: nginx\n        ports:\n        - containerPort: 80\n          name: http\n        volumeMounts:\n        - name: hello-web-disk\n          mountPath: \"/var/www/html\"\n  volumeClaimTemplates:\n  - metadata:\n      name: hello-web-disk\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 30Gi\n</code></pre> <p>To create the StatefulSet with the volume, execute the following command:</p> <pre><code>kubectl apply -f statefulset-demo.yaml\n</code></pre> <p>Output </p> <pre><code>service \"statefulset-demo-service\" created\nstatefulset.apps \"statefulset-demo\" created\n</code></pre> <p>You now have a statefulset running behind a service named statefulset-demo-service.</p> <p>Verify the connection of Pods in StatefulSets Use \"kubectl describe\" to view the details of the StatefulSet:</p> <pre><code>kubectl describe statefulset statefulset-demo\n</code></pre> <p>Note the event status at the end of the output. The service and statefulset created successfully.</p> <pre><code>Normal  SuccessfulCreate  10s   statefulset-controller\nMessage: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success\nNormal  SuccessfulCreate  10s   statefulset-controller\nMessage: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output </p> <pre><code>NAME                 READY     STATUS    RESTARTS   AGE\nstatefulset-demo-0   1/1       Running   0          6m\nstatefulset-demo-1   1/1       Running   0          3m\nstatefulset-demo-2   1/1       Running   0          2m\n</code></pre> <p>To list the PVCs, execute the following command:</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output </p> <pre><code>NAME                          STATUS    VOLUME           CAPACITY ACCESS\nhello-web-disk                Bound     pvc-86117ea6-...34   30Gi    RWO\nhello-web-disk-st...-demo-0   Bound     pvc-92d21d0f-...34   30Gi    RWO\nhello-web-disk-st...-demo-1   Bound     pvc-9bc84d92-...34   30Gi    RWO\nhello-web-disk-st...-demo-2   Bound     pvc-a526ecdf-...34   30Gi    RWO\n</code></pre> <p>The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod.</p> <p>Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet:</p> <pre><code>kubectl describe pvc hello-web-disk-statefulset-demo-0\n</code></pre>"},{"location":"35-gcp-persistent-volume-for-gke/#task-4-verify-the-persistence-of-persistent-volume-connections-to-pods-managed-by-statefulsets","title":"Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets","text":"<p>In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted.</p> <p>To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it statefulset-demo-0 -- sh\n</code></pre> <p>Verify that there is no index.html text file in the /var/www/html directory.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>To create a simple text message as a web page in the Pod enter the following commands:</p> <pre><code>echo Test webpage in a persistent volume!&gt;/var/www/html/index.html\nchmod +x /var/www/html/index.html\n</code></pre> <p>Verify the text file contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output </p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre> <p>Delete the Pod where you updated the file on the PVC.</p> <pre><code>kubectl delete pod statefulset-demo-0\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod.</p> <p>Note: You need to wait until the Pod status shows that it is running again. Connect to the shell on the new statefulset-demo-0 Pod.</p> <pre><code>kubectl exec -it statefulset-demo-0 -- sh\n</code></pre> <p>Verify that the text file still contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output </p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved.</p> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"36-gcloud-basics/","title":"Basic gcloud commands","text":"<p>gcloud: for working with Compute Engine, Google Kubernetes Engine (GKE) and many Google Cloud services gsutil: for working with Cloud Storage kubectl: for working with GKE and Kubernetes bq: for working with BigQuery</p>"},{"location":"36-gcloud-basics/#gcloud","title":"gcloud","text":"<p>List zones in a region</p> <pre><code>gcloud compute zones list | grep $REGION\n</code></pre> <p>Set default zone</p> <pre><code>gcloud config set compute/zone $ZONE\n</code></pre> <p>Create a VM</p> <pre><code>gcloud compute instances create $MY_VMNAME \\\n--machine-type \"e2-standard-2\" \\\n--image-project \"debian-cloud\" \\\n--image-family \"debian-9\" \\\n--subnet \"default\"\n</code></pre> <p>List of virtual machine instances</p> <pre><code>gcloud compute instances list\n</code></pre> <p>Create a service account</p> <pre><code>gcloud iam service-accounts create service-account-name --display-name \"service-account-name\"\n</code></pre> <p>Provide editor access to the service account </p> <pre><code>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member serviceAccount:service-account-name@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --role roles/viewer\n</code></pre> <p>View configuration</p> <pre><code>gcloud config list\n</code></pre> <p>Authenticate as a service account in the cloud shell</p> <pre><code>gcloud auth activate-service-account --key-file credentials.json\n</code></pre> <p>List of active accounts</p> <pre><code>gcloud auth list\n</code></pre> <p>Copy file to a virtual machine</p> <pre><code>gcloud compute scp index.html first-vm:index.nginx-debian.html --zone=us-central1-c\n</code></pre>"},{"location":"36-gcloud-basics/#cloud-storage","title":"Cloud storage","text":"<p>Create bucket</p> <pre><code>gsutil mb gs://$BUCKET\n</code></pre> <p>Copy files</p> <pre><code>gsutil cp gs://$SOURCE DESTINATION\n</code></pre> <p>Get access list of a file</p> <pre><code>gsutil acl get gs://$BUCKET/$FILE\n</code></pre> <p>Set private access</p> <pre><code>gsutil acl set private gs://$BUCKET/$FILE\n</code></pre> <p>Make bucket public</p> <pre><code>gsutil iam ch allUsers:objectViewer gs://$BUCKET\n</code></pre>"},{"location":"37-migrate-for-anthos/","title":"37 migrate for anthos","text":""},{"location":"37-migrate-for-anthos/#steps-for-migration","title":"Steps for migration","text":"<ol> <li>Configure processing cluster</li> <li>Add migration source</li> <li>Generate the migration object, create a plan in yaml</li> <li>Generate artifacts [container images, yaml files] for the migrate</li> <li>Test the artifacts</li> <li>Deploy to production cluster</li> </ol>"},{"location":"37-migrate-for-anthos/#installing-the-necessary-tools","title":"Installing the necessary tools","text":"<p>Create the processing cluster using this command</p> <pre><code>gcloud container --project $PROJECT_ID \\\nclusters create $CLUSTER_NAME \\\n--zone $CLUSTER_ZONE \\\n--cluster-version 1.14 \\\n--machine-type \"n1-standard-4\" \\\n--image-type \"UBUNTU\" \\\n--num-nodes 1 \\\n--enable-stackdriver-kubernetes \\\n--scopes \"cloud-platform\" \\\n--enable-ip-alias \\\n--tags=\"http-server\"\n</code></pre> <p>install migrate for anthos</p> <pre><code>migctl setup install\n</code></pre> <p>Specify location of the migration</p> <pre><code>migctl source create ce my-ce-src --project my-project --zone zone\n</code></pre> <p>This is for migration from GCP. Migrating from other cloud providers might require additional libraries.</p> <p>Create a migration plan</p> <pre><code>migctl migration create test-migration --source my-ce-src --vm-id my-id --intent Image\n</code></pre> <p>This creates the yaml that can be customized based on migration need.</p> <p>We can specify following intents: * Image * Data * Image and Data * PV Based Container</p> <p>Generate the artifacts for the migration</p> <pre><code>migctl migration generate-artifacts my-migration\n</code></pre> <p>This will create two types of images.  * A runnable image * A non-runnable image layer that can be used to update the container in the future</p> <p>Migration yaml are created and stored in a storage bucket as an intermediate step.</p> <p>Get the YAML using this command</p> <pre><code>migctl migration get-artifacts test-migration\n</code></pre> <p>Deploy after editing the YAML</p> <pre><code>kubectl apply -f deployment spec.yaml\n</code></pre>"},{"location":"38-kubectl-basics/","title":"38 kubectl basics","text":""},{"location":"38-kubectl-basics/#kubectl","title":"Kubectl","text":"<p>kubectl is used to communicate with the kube-APIserver on the control plane. </p> <p>It must be configured with the location and credentials of the kubernetes cluster.</p> <p>kubectl configuration is in a config file: $HOME/.kube/config</p> <p>Configuration contains  * Target cluster name * Credentials for the cluster</p> <p>View the configuration</p> <pre><code>kubectl config view\n</code></pre> <p>Retrieve the credentials </p> <pre><code>gcloud container clusters \\\nget-credentuals $CLUSTER_NAME \\\n--zone $ZONE\n</code></pre> <p>This will update the configuration file with appropriate credentials. </p> <p>List the pods</p> <pre><code>kubectl get pods\n</code></pre> <p>Example of deployment object in YAML format </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n    app: my-app\n    spec:\n      containers:\n      - name: my-app\n        image: gcr.io/demo/my-app:1.0\n        ports:\n        - containerPort: 8080\n</code></pre> <p>Deploy using this command</p> <pre><code>kubectl apply -f $DEPLOYMENT_FILE\n</code></pre> <p>It is also possible to deploy imperatively using the <code>run</code> command, that specifies all parameters inline.</p> <pre><code>kubectl run $DEPLOYMENT_NAME \\\n--image gcr.io/demo/my-app:1.0 \\\n--replicas 3 \\\n--labels app:my-app \\\n--port 8080 \\\n--generator deployment/apps.v1 \\\n--save-config\n</code></pre> <p>Inspect the Deployment </p> <pre><code>kubectl get deployment $DEPLOYMENT_NAME\n</code></pre> <p>This can be saved as a YAML for future reference</p> <pre><code>kubectl get deployment $DEPLOYMENT_NAME -o yaml &gt; this.yaml\n</code></pre> <p>Scaling the deployment</p> <pre><code>kubectl scale deployment $DEPLOYMENT_NAME -replicas=5\n</code></pre> <p>Autoscaling the deployment by specifying a minimum and maximum number of replicas</p> <pre><code>kubectl autoscale deployment $DEPLOYMENT_NAME --min=5 --max=15 --cpu-percentage=75\n</code></pre> <p>This will create a Kubernetes object called Horizontal Pod Autoscaler. This object will scale the number of pods to match the cpu utilization. This will scale a particular deployment within a cluster, and not a cluster as a whole.  Update a deployment using these commands </p> <pre><code>kubectl apply -f $DEPLOYMENT_FILE\n</code></pre> <p>This will update the deployment according to the specifications in the YAML.</p> <p>Secondly, deployment can be updated imperatively.</p> <pre><code>kubectl set image deployment $DEPLOYMENT_NAME $IMAGE $IMAGE:$TAG\n</code></pre> <p>Thirdly, this command will open the configuration file in vim editor. If changed and save, the update will be deployed.</p> <pre><code>kubectl edit deployment/$DEPLOYMENT_NAME\n</code></pre> <p>Rollout undo command</p> <pre><code>kubectl rollout undo deployment $DEPLOYMENT_NAME\n</code></pre> <p>Rolling out to a specific version</p> <pre><code>kubectl rollout undo deployment $DEPLOYMENT_NAME --to-revision=2\n</code></pre> <p>View version history</p> <pre><code>kubectl rollout history deployment $DEPLOYMENT_NAME --revision=2\n</code></pre> <p>When a deployment is edited, a rollout is triggered automatically. To change this behavior:</p> <pre><code>kubectl rollout pause deployment $DEPLOYMENT_NAME\n</code></pre> <p>The changes will be deployed in one rollout after its resumed:</p> <pre><code>kubectl rollout resume deployment $DEPLOYMENT_NAME\n</code></pre> <p>We can monitor the rollout status:</p> <pre><code>kubectl rollout status deployment $DEPLOYMENT_NAME\n</code></pre> <p>Delete a deployment</p> <pre><code>kubectl delete deployment $DEPLOYMENT_NAME\n</code></pre> <p>This will lead to deletion of all resources run by the deployment, including running pods.</p>"},{"location":"39-creating-gke-deployments/","title":"39 creating gke deployments","text":""},{"location":"39-creating-gke-deployments/#task-1-create-deployment-manifests-and-deploy-to-the-cluster","title":"Task 1. Create deployment manifests and deploy to the cluster","text":"<p>In this task, you create a deployment manifest for a Pod inside the cluster.</p>"},{"location":"39-creating-gke-deployments/#connect-to-the-lab-gke-cluster","title":"Connect to the lab GKE cluster","text":"<p>In Cloud Shell, type the following command to set the environment variable for the zone and cluster name.</p> <pre><code>export my_zone=us-central1-a\nexport my_cluster=standard-cluster-1\n</code></pre> <p>Configure kubectl tab completion in Cloud Shell.</p> <pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>In Cloud Shell, configure access to your cluster for the kubectl command-line tool, using the following command:</p> <pre><code>gcloud container clusters get-credentials $my_cluster --zone $my_zone\n</code></pre> <p>In Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> <p>Create a soft link as a shortcut to the working directory.</p> <pre><code>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s\n</code></pre> <p>Change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Deployments/\n</code></pre>"},{"location":"39-creating-gke-deployments/#create-a-deployment-manifest","title":"Create a deployment manifest","text":"<p>You will create a deployment using a sample deployment manifest called nginx-deployment.yaml that has been provided for you. This deployment is configured to run three Pod replicas with a single nginx container in each Pod listening on TCP port 80.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n</code></pre> <p>To deploy your manifest, execute the following command:</p> <pre><code>kubectl apply -f ./nginx-deployment.yaml\n</code></pre> <p>To view a list of deployments, execute the following command:</p> <pre><code>kubectl get deployments\n</code></pre> <p>The output should look like this example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY      UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3        3            0           3s\n</code></pre> <p>Wait a few seconds, and repeat the command until the number listed for CURRENT deployments reported by the command matches the number of DESIRED deployments. The final output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           42s\n</code></pre>"},{"location":"39-creating-gke-deployments/#task-2-manually-scale-up-and-down-the-number-of-pods-in-deployments","title":"Task 2. Manually scale up and down the number of Pods in deployments","text":"<p>Sometimes, you want to shut down a Pod instance. Other times, you want ten Pods running. In Kubernetes, you can scale a specific Pod to the desired number of instances. To shut them down, you scale to zero.</p> <p>In this task, you scale Pods up and down in the Google Cloud Console and Cloud Shell.</p>"},{"location":"39-creating-gke-deployments/#scale-pods-up-and-down-in-the-console","title":"Scale Pods up and down in the console","text":"<p>Switch to the Google Cloud Console tab. On the Navigation menu ( 9a951fa6d60a98a5.png), click Kubernetes Engine &gt; Workloads. Click nginx-deployment (your deployment) to open the Deployment details page. At the top, click ACTIONS &gt; Scale. Type 1 and click SCALE. This action scales down your cluster. You should see the Pod status being updated under Managed Pods. You might have to click Refresh.</p>"},{"location":"39-creating-gke-deployments/#scale-pods-up-and-down-in-the-shell","title":"Scale Pods up and down in the shell","text":"<p>Switch back to the Cloud Shell browser tab.</p> <p>In the Cloud Shell, to view a list of Pods in the deployments, execute the following command:</p> <pre><code>kubectl get deployments\n</code></pre> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   1/1     1            1           3m\n</code></pre> <p>To scale the Pod back up to three replicas, execute the following command:</p> <pre><code>kubectl scale --replicas=3 deployment nginx-deployment\n</code></pre> <p>To view a list of Pods in the deployments, execute the following command:</p> <pre><code>kubectl get deployments\n</code></pre> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           4m\n</code></pre>"},{"location":"39-creating-gke-deployments/#task-3-trigger-a-deployment-rollout-and-a-deployment-rollback","title":"Task 3. Trigger a deployment rollout and a deployment rollback","text":"<p>A deployment's rollout is triggered if and only if the deployment's Pod template (that is, .spec.template) is changed, for example, if the labels or container images of the template are updated. Other updates, such as scaling the deployment, do not trigger a rollout.</p> <p>In this task, you trigger deployment rollout, and then you trigger deployment rollback.</p>"},{"location":"39-creating-gke-deployments/#trigger-a-deployment-rollout","title":"Trigger a deployment rollout","text":"<p>To update the version of nginx in the deployment, execute the following command:</p> <pre><code>kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record\n</code></pre> <p>This updates the container image in your Deployment to nginx v1.9.1.</p> <p>To view the rollout status, execute the following command:</p> <pre><code>kubectl rollout status deployment.v1.apps/nginx-deployment\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>Waiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 2 out of 3 new replicas updated...\nWaiting for rollout to finish: 1 old replicas pending termination...\nWaiting for rollout to finish: 1 old replicas pending termination...\ndeployment \"nginx-deployment\" successfully rolled out\n</code></pre> <p>To verify the change, get the list of deployments.</p> <pre><code>kubectl get deployments\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           6m\n</code></pre> <p>View the rollout history of the deployment.</p> <pre><code>kubectl rollout history deployment nginx-deployment\n</code></pre> <p>The output should look like the example. Your output might not be an exact match.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\"\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true\n</code></pre>"},{"location":"39-creating-gke-deployments/#trigger-a-deployment-rollback","title":"Trigger a deployment rollback","text":"<p>To roll back an object's rollout, you can use the kubectl rollout undo command.</p> <p>To roll back to the previous version of the nginx deployment, execute the following command:</p> <pre><code>kubectl rollout undo deployments nginx-deployment\n</code></pre> <p>View the updated rollout history of the deployment.</p> <pre><code>kubectl rollout history deployment nginx-deployment\n</code></pre> <p>The output should look like the example. Your output might not be an exact match.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\"\nREVISION  CHANGE-CAUSE\n2         kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true\n3         &lt;none&gt;\n</code></pre> <p>View the details of the latest deployment revision</p> <pre><code>kubectl rollout history deployment/nginx-deployment --revision=3\n</code></pre> <p>The output should look like the example. Your output might not be an exact match but it will show that the current revision has rolled back to nginx:1.7.9.</p> <p>Output (do not copy)</p> <pre><code>deployments \"nginx-deployment\" with revision #3\nPod Template:\n  Labels:       app=nginx\n        pod-template-hash=3123191453\n  Containers:\n   nginx:\n    Image:      nginx:1.7.9\n    Port:       80/TCP\n    Host Port:  0/TCP\n    Environment:        &lt;none&gt;\n    Mounts:     &lt;none&gt;\n  Volumes:      &lt;none&gt;\n</code></pre>"},{"location":"39-creating-gke-deployments/#task-4-define-the-service-type-in-the-manifest","title":"Task 4. Define the service type in the manifest","text":"<p>In this task, you create and verify a service that controls inbound traffic to an application. Services can be configured as ClusterIP, NodePort or LoadBalancer types. In this lab, you configure a LoadBalancer.</p>"},{"location":"39-creating-gke-deployments/#define-service-types-in-the-manifest","title":"Define service types in the manifest","text":"<p>A manifest file called service-nginx.yaml that deploys a LoadBalancer service type has been provided for you. This service is configured to distribute inbound traffic on TCP port 60000 to port 80 on any containers that have the label app: nginx.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 60000\n    targetPort: 80\n</code></pre> <p>In the Cloud Shell, to deploy your manifest, execute the following command:</p> <pre><code>kubectl apply -f ./service-nginx.yaml\n</code></pre> <p>This manifest defines a service and applies it to Pods that correspond to the selector. In this case, the manifest is applied to the nginx container that you deployed in task 1. This service also applies to any other Pods with the app: nginx label, including any that are created after the service.</p>"},{"location":"39-creating-gke-deployments/#verify-the-loadbalancer-creation","title":"Verify the LoadBalancer creation","text":"<p>To view the details of the nginx service, execute the following command:</p> <pre><code>kubectl get service nginx\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME      CLUSTER_IP      EXTERNAL_IP      PORT(S)   SELECTOR    AGE\nnginx     10.X.X.X        X.X.X.X          60000/TCP    run=nginx   1m\n</code></pre> <p>When the external IP appears, open <code>http://[EXTERNAL_IP]:60000/</code> in a new browser tab to see the server being served through network load balancing. It may take a few seconds before the ExternalIP field is populated for your service. This is normal. Just re-run the kubectl get services nginx command every few seconds until the field is populated.</p>"},{"location":"39-creating-gke-deployments/#task-5-perform-a-canary-deployment","title":"Task 5. Perform a canary deployment","text":"<p>A canary deployment is a separate deployment used to test a new version of your application. A single service targets both the canary and the normal deployments. And it can direct a subset of users to the canary version to mitigate the risk of new releases. The manifest file nginx-canary.yaml that is provided for you deploys a single pod running a newer version of nginx than your main deployment. In this task, you create a canary deployment using this new deployment file.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-canary\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        track: canary\n        Version: 1.9.1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.9.1\n        ports:\n        - containerPort: 80\n</code></pre> <p>The manifest for the nginx Service you deployed in the previous task uses a label selector to target the Pods with the app: nginx label. Both the normal deployment and this new canary deployment have the app: nginx label. Inbound connections will be distributed by the service to both the normal and canary deployment Pods. The canary deployment has fewer replicas (Pods) than the normal deployment, and thus it is available to fewer users than the normal deployment.</p> <p>Create the canary deployment based on the configuration file.</p> <pre><code>kubectl apply -f nginx-canary.yaml\n</code></pre> <p>When the deployment is complete, verify that both the nginx and the nginx-canary deployments are present.</p> <pre><code>kubectl get deployments\n</code></pre> <p>Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page.</p> <p>Switch back to the Cloud Shell and scale down the primary deployment to 0 replicas.</p> <pre><code>kubectl scale --replicas=0 deployment nginx-deployment\n</code></pre> <p>Verify that the only running replica is now the Canary deployment:</p> <pre><code>kubectl get deployments\n</code></pre> <p>Switch back to the browser tab that is connected to the external LoadBalancer service ip and refresh the page. You should continue to see the standard Welcome to nginx page showing that the Service is automatically balancing traffic to the canary deployment.</p>"},{"location":"39-creating-gke-deployments/#session-affinity","title":"Session affinity","text":"<p>The service configuration used in the lab does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal nginx deployment or to the nginx-canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this you can set the sessionAffinity field to ClientIP in the specification of the service if you need a client's first request to determine which Pod will be used for all subsequent connections.</p> <p>For example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  type: LoadBalancer\n  sessionAffinity: ClientIP\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 60000\n    targetPort: 80\n</code></pre>"},{"location":"40-data-reliability-conference/","title":"40 data reliability conference","text":""},{"location":"40-data-reliability-conference/#date-may-25-2022","title":"Date May 25, 2022","text":""},{"location":"40-data-reliability-conference/#value-of-data-reliability","title":"Value of data reliability","text":"<ul> <li>Leads to bad decision</li> <li>Trust</li> <li>Waste of time</li> <li>Biases leading to reputation loss</li> <li>Loss of revenue </li> <li>Increased cost</li> </ul>"},{"location":"40-data-reliability-conference/#steps-to-solve","title":"Steps to solve","text":"<ul> <li>Automate as much as possible </li> <li>Monitor as much as possible </li> <li>Control releases - version control, code review and environments</li> <li>Design simplicity</li> </ul>"},{"location":"40-data-reliability-conference/#pipeline-promotion-without-the-commotion","title":"Pipeline promotion without the commotion","text":"<ul> <li>Great expectations python library helps to set up tests in data pipeline</li> <li>dbt helps create pipeline and put documentaton alongside</li> <li>dbt allows for tests in pipelines, defined as yaml</li> <li>dbt has library <code>dbt_expectation</code> for tests</li> <li>Connects with github for version control</li> </ul>"},{"location":"40-data-reliability-conference/#what-do-data-scientists-look-for-in-data","title":"What do Data scientists look for in data","text":"<ul> <li>discoverable</li> <li>relevant</li> <li>usable</li> <li>accurate</li> <li>timely</li> <li>fit for purpose</li> </ul>"},{"location":"40-data-reliability-conference/#workflow-orchestration-to-reduce-negative-engineering","title":"Workflow orchestration to reduce negative engineering","text":"<ul> <li>If the data team spends 80% of time in negative engineering, reducing my 25% with double productivity.</li> <li>Prefect is open source, python based workflow orchestration</li> <li>List of negative engineering tasks</li> </ul>"},{"location":"40-data-reliability-conference/#controlled-release-of-data-pipeline","title":"Controlled release of data pipeline","text":"<ul> <li>Create notifications with Slack and GitHub actions </li> <li>Implementation will be automatic </li> <li>After pull request, docker images are created automatically and pussed to image registry</li> <li>Kubernetes will take the latest container</li> </ul>"},{"location":"40-data-reliability-conference/#objective-of-data-observability","title":"Objective of data observability","text":"<ul> <li>Constantly monitor data, notify data teams so that data teams knows before the consumers of data</li> <li>Demo platform: https://drecon-workshop.bigeye.com/#0</li> <li>Observability can be set up using SQL queries at a basic level</li> </ul>"},{"location":"40-data-reliability-conference/#tools-to-check","title":"Tools to check","text":"<ul> <li>dbt - SQL workflows</li> <li>bigeye - data observability solution</li> <li>great expectation - data testing library in python</li> <li>MLFlow - hosting ML models</li> <li>ArgoCD - CI/CD for kubernetes</li> <li>Prefect - workflow orchestration tool, gcp has Workflow</li> <li>GitFlow</li> </ul>"},{"location":"40-data-reliability-conference/#data-outage","title":"Data outage","text":"<p>It is very different from software outage, because the spectrum is large - 1. data is old 1. data is missing 1. its wrong 1. the database is missing</p> <p>Easy to estimte cost of ourage in e-Commerce. For data, the cost can be low if no one is looking at the data, or very high cost when the CEO makes a wrog decision.</p> <p>We should be truth seeking, but risk taking. While we should be making investments to improve data reliability, we should not wait to make bold decisions until we have the best data.</p>"},{"location":"40-data-reliability-conference/#data-reliability","title":"Data reliability","text":"<p>What it is? </p>"},{"location":"40-data-reliability-conference/#contacts","title":"Contacts","text":"<ul> <li>Christianna Clark</li> <li>Shailvi Wakhlu</li> <li>Scott Shi  Handled 5B tickets when working for Uber.</li> <li>Randy Pitcher dbt developer advocate. Very good overview of dbt.</li> <li>Egor Gryaznov BigEye founder. </li> <li>Pavani Rangavajhula Good overview of CI/CD of data pipelines.</li> <li>Glen-Erik Cortes Started with MBA and now working as ML Engineering Manager. Talks about ML Ops.</li> <li>Jerry Shen Very articulate data scientist. Works at OpenSea.</li> <li>Miriah Peterson</li> </ul>"},{"location":"41-persistent-storage-gke/","title":"Persistent storage in GKE","text":""},{"location":"41-persistent-storage-gke/#task-1-create-pvs-and-pvcs","title":"Task 1. Create PVs and PVCs","text":"<p>In this task, you create a PVC, which triggers Kubernetes to automatically create a PV.</p>"},{"location":"41-persistent-storage-gke/#connect-to-the-lab-gke-cluster","title":"Connect to the lab GKE cluster","text":"<p>In Cloud Shell, type the following command to set the environment variable for the zone and cluster name.</p> <pre><code>export my_zone=us-central1-a\nexport my_cluster=standard-cluster-1\n</code></pre> <p>Configure tab completion for the kubectl command-line tool.</p> <pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>Configure access to your cluster for kubectl:</p> <pre><code>gcloud container clusters get-credentials $my_cluster --zone $my_zone\n</code></pre>"},{"location":"41-persistent-storage-gke/#create-and-apply-a-manifest-with-a-pvc","title":"Create and apply a manifest with a PVC","text":"<p>Most of the time, you don't need to directly configure PV objects or create Compute Engine persistent disks. Instead, you can create a PVC, and Kubernetes automatically provisions a persistent disk for you.</p> <p>You create the PVC in this task using the pvc-demo.yaml manifest file that has been provided for you. This creates a 30 gigabyte PVC called hello-web-disk that can be mounted as a read-write volume on a single node at a time.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: hello-web-disk\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n</code></pre> <p>In Cloud Shell, enter the following command to clone the repository to the lab Cloud Shell.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n</code></pre> <p>Create a soft link as a shortcut to the working directory.</p> <pre><code>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s\n</code></pre> <p>Change to the directory that contains the sample files for this lab.</p> <pre><code>cd ~/ak8s/Storage/\n</code></pre> <p>To show that you currently have no PVCs, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Output (Do not copy):</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>To create the PVC, execute the following command:</p> <pre><code>kubectl apply -f pvc-demo.yaml\n</code></pre> <p>To show your newly created PVC, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Partial output (Do not copy):</p> <pre><code>NAME           STATUS VOLUME        CAP   ACCESS MODES      CLASS   AGE\nhello-web-disk Bound  pvc-8...34   30Gi            RWO   standard    5s\n</code></pre>"},{"location":"41-persistent-storage-gke/#task-2-mount-and-verify-google-cloud-persistent-disk-pvcs-in-pods","title":"Task 2. Mount and verify Google Cloud persistent disk PVCs in Pods","text":"<p>In this task, you attach your persistent disk PVC to a Pod. You mount the PVC as a volume as part of the manifest for the Pod.</p>"},{"location":"41-persistent-storage-gke/#mount-the-pvc-to-a-pod","title":"Mount the PVC to a Pod","text":"<p>The manifest file pod-volume-demo.yaml deploys an nginx container, attaches the pvc-demo-volume to the Pod and mounts that volume to the path /var/www/html inside the nginx container. Files saved to this directory inside the container will be saved to the persistent volume and persist even if the Pod and the container are shutdown and recreated.</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: pvc-demo-pod\nspec:\n  containers:\n    - name: frontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: pvc-demo-volume\n  volumes:\n    - name: pvc-demo-volume\n      persistentVolumeClaim:\n        claimName: hello-web-disk\n</code></pre> <p>To create the Pod with the volume, execute the following command:</p> <pre><code>kubectl apply -f pod-volume-demo.yaml\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output (Do not copy):</p> <pre><code>NAME          READY    STATUS              RESTARTS   AGE\npvc-demo-pod  0/1      ContainerCreating   0          18s\n</code></pre> <p>If you do this quickly after creating the Pod, you will see the status listed as \"ContainerCreating\" while the volume is mounted before the status changes to \"Running\".</p> <p>To verify the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it pvc-demo-pod -- sh\n</code></pre> <p>To create a simple text message as a web page in the Pod enter the following commands:</p> <pre><code>echo Test webpage in a persistent volume!&gt;/var/www/html/index.html\nchmod +x /var/www/html/index.html\n</code></pre> <p>Verify the text file contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output (Do not copy):</p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"41-persistent-storage-gke/#test-the-persistence-of-the-pv","title":"Test the persistence of the PV","text":"<p>You will now delete the Pod from the cluster, confirm that the PV still exists, then redeploy the Pod and verify the contents of the PV remain intact.</p> <p>Delete the pvc-demo-pod.</p> <pre><code>kubectl delete pod pvc-demo-pod\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output (Do not copy):</p> <pre><code>No resources found in default namespace.\n</code></pre> <p>There should be no Pods on the cluster.</p> <p>To show your PVC, execute the following command:</p> <pre><code>kubectl get persistentvolumeclaim\n</code></pre> <p>Partial output (Do not copy):</p> <pre><code>NAME           STATUS VOLUME        CAP   ACCESS MODES      CLASS   AGE\nhello-web-disk Bound  pvc-8...34   30Gi            RWO   standard   22m\n</code></pre> <p>Your PVC still exists, and was not deleted when the Pod was deleted.</p> <p>Redeploy the pvc-demo-pod.</p> <pre><code>kubectl apply -f pod-volume-demo.yaml\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output (Do not copy):</p> <pre><code>NAME           READY     STATUS    RESTARTS   AGE\npvc-demo-pod   1/1       Running   0          15s\n</code></pre> <p>The Pod will deploy and the status will change to \"Running\" faster this time because the PV already exists and does not need to be created.</p> <p>To verify the PVC is still accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it pvc-demo-pod -- sh\n</code></pre> <p>To verify that the text file still contains your message execute the following command:</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output (Do not copy):</p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>The contents of the persistent volume were not removed, even though the Pod was deleted from the cluster and recreated.</p> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"41-persistent-storage-gke/#task-3-create-statefulsets-with-pvcs","title":"Task 3. Create StatefulSets with PVCs","text":"<p>In this task, you use your PVC in a StatefulSet. A StatefulSet is like a Deployment, except that the Pods are given unique identifiers.</p>"},{"location":"41-persistent-storage-gke/#release-the-pvc","title":"Release the PVC","text":"<p>Before you can use the PVC with the statefulset, you must delete the Pod that is currently using it. Execute the following command to delete the Pod:</p> <pre><code>kubectl delete pod pvc-demo-pod\n</code></pre> <p>Confirm the Pod has been removed.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output (Do not copy):</p> <pre><code>No resources found in default namespace.\n</code></pre>"},{"location":"41-persistent-storage-gke/#create-a-statefulset","title":"Create a StatefulSet","text":"<p>The manifest file statefulset-demo.yaml creates a StatefulSet that includes a LoadBalancer service and three replicas of a Pod containing an nginx container and a volumeClaimTemplate for 30 gigabyte PVCs with the name hello-web-disk. The nginx containers mount the PVC called hello-web-disk at /var/www/html as in the previous task.</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: statefulset-demo-service\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: statefulset-demo\nspec:\n  selector:\n    matchLabels:\n      app: MyApp\n  serviceName: statefulset-demo-service\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: MyApp\n    spec:\n      containers:\n      - name: stateful-set-container\n        image: nginx\n        ports:\n        - containerPort: 80\n          name: http\n        volumeMounts:\n        - name: hello-web-disk\n          mountPath: \"/var/www/html\"\n  volumeClaimTemplates:\n  - metadata:\n      name: hello-web-disk\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 30Gi\n</code></pre> <p>To create the StatefulSet with the volume, execute the following command:</p> <pre><code>kubectl apply -f statefulset-demo.yaml\n</code></pre> <p>Output (Do not copy):</p> <pre><code>service \"statefulset-demo-service\" created\nstatefulset.apps \"statefulset-demo\" created\n</code></pre> <p>You now have a statefulset running behind a service named statefulset-demo-service.</p>"},{"location":"41-persistent-storage-gke/#verify-the-connection-of-pods-in-statefulsets","title":"Verify the connection of Pods in StatefulSets","text":"<p>Use \"kubectl describe\" to view the details of the StatefulSet:</p> <pre><code>kubectl describe statefulset statefulset-demo\n</code></pre> <p>Note the event status at the end of the output. The service and statefulset created successfully.</p> <pre><code>Normal  SuccessfulCreate  10s   statefulset-controller\nMessage: create Claim hello-web-disk-statefulset-demo-0 Pod statefulset-demo-0 in StatefulSet statefulset-demo success\nNormal  SuccessfulCreate  10s   statefulset-controller\nMessage: create Pod statefulset-demo-0 in StatefulSet statefulset-demo successful\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>Output (Do not copy):</p> <pre><code>NAME                 READY     STATUS    RESTARTS   AGE\nstatefulset-demo-0   1/1       Running   0          6m\nstatefulset-demo-1   1/1       Running   0          3m\nstatefulset-demo-2   1/1       Running   0          2m\n</code></pre> <p>To list the PVCs, execute the following command:</p> <pre><code>kubectl get pvc\n</code></pre> <p>Output (Do not copy):</p> <pre><code>NAME                          STATUS    VOLUME           CAPACITY ACCESS\nhello-web-disk                Bound     pvc-86117ea6-...34   30Gi    RWO\nhello-web-disk-st...-demo-0   Bound     pvc-92d21d0f-...34   30Gi    RWO\nhello-web-disk-st...-demo-1   Bound     pvc-9bc84d92-...34   30Gi    RWO\nhello-web-disk-st...-demo-2   Bound     pvc-a526ecdf-...34   30Gi    RWO\n</code></pre> <p>The original hello-web-disk is still there and you can now see the individual PVCs that were created for each Pod in the new statefulset Pod.</p> <p>Use \"kubectl describe\" to view the details of the first PVC in the StatefulSet:</p> <pre><code>kubectl describe pvc hello-web-disk-statefulset-demo-0\n</code></pre>"},{"location":"41-persistent-storage-gke/#task-4-verify-the-persistence-of-persistent-volume-connections-to-pods-managed-by-statefulsets","title":"Task 4. Verify the persistence of Persistent Volume connections to Pods managed by StatefulSets","text":"<p>In this task, you verify the connection of Pods in StatefulSets to particular PVs as the Pods are stopped and restarted.</p> <p>To verify that the PVC is accessible within the Pod, you must gain shell access to your Pod. To start the shell session, execute the following command:</p> <pre><code>kubectl exec -it statefulset-demo-0 -- sh\n</code></pre> <p>Verify that there is no index.html text file in the /var/www/html directory.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>To create a simple text message as a web page in the Pod enter the following commands:</p> <pre><code>echo Test webpage in a persistent volume!&gt;/var/www/html/index.html\nchmod +x /var/www/html/index.html\n</code></pre> <p>Verify the text file contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output (Do not copy):</p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre> <p>Delete the Pod where you updated the file on the PVC.</p> <pre><code>kubectl delete pod statefulset-demo-0\n</code></pre> <p>List the Pods in the cluster.</p> <pre><code>kubectl get pods\n</code></pre> <p>You will see that the StatefulSet is automatically restarting the statefulset-demo-0 Pod.</p> <p>Note: You need to wait until the Pod status shows that it is running again.</p> <p>Connect to the shell on the new statefulset-demo-0 Pod.</p> <pre><code>kubectl exec -it statefulset-demo-0 -- sh\n</code></pre> <p>Verify that the text file still contains your message.</p> <pre><code>cat /var/www/html/index.html\n</code></pre> <p>Output (Do not copy):</p> <pre><code>Test webpage in a persistent volume!\n</code></pre> <p>The StatefulSet restarts the Pod and reconnects the existing dedicated PVC to the new Pod, ensuring that the data for that Pod is preserved.</p> <p>Enter the following command to leave the interactive shell on the nginx container.</p> <pre><code>exit\n</code></pre>"},{"location":"42-creating-gke-deployments-from-shell/","title":"42 creating gke deployments from shell","text":""},{"location":"42-creating-gke-deployments-from-shell/#deploying-google-kubernetes-engine-clusters-from-cloud-shell","title":"Deploying Google Kubernetes Engine Clusters from Cloud Shell","text":""},{"location":"42-creating-gke-deployments-from-shell/#objectives","title":"Objectives","text":"<p>Use <code>kubectl</code> to build and manipulate GKE clusters</p> <p>Use <code>kubectl</code> and configuration files to deploy Pods</p> <p>Use Container Registry to store and deploy containers</p>"},{"location":"42-creating-gke-deployments-from-shell/#task-1-deploy-gke-clusters","title":"Task 1. Deploy GKE clusters","text":"<p>In this task, you use Cloud Shell to deploy GKE clusters.</p> <p>In Cloud Shell, type the following command to set the environment variable for the zone and cluster name.</p> <pre><code>export my_zone=us-central1-a\nexport my_cluster=standard-cluster-1\n</code></pre> <p>In Cloud Shell, type the following command to create a Kubernetes cluster.</p> <pre><code>gcloud container clusters create $my_cluster --num-nodes 3 --zone $my_zone --enable-ip-alias\n</code></pre> <p>This form of the command sets most options to their defaults. To view the entire set of possible options, click here</p> <p>You will see a number of warnings highlighting changes to default GKE cluster settings that were introduced as newer version of Kubernetes have been adopted by GKE.</p>"},{"location":"42-creating-gke-deployments-from-shell/#task-2-modify-gke-clusters","title":"Task 2. Modify GKE clusters","text":"<p>It is easy to modify many of the parameters of existing clusters in Google Cloud Console or Cloud Shell. In this task, you use Cloud Shell to modify the number of nodes in a GKE cluster.</p> <p>In Cloud Shell, execute the following command to modify standard-cluster-1 to have four nodes:</p> <pre><code>gcloud container clusters resize $my_cluster --zone $my_zone --num-nodes=4\n</code></pre> <p>Note: When issuing cluster commands, you typically must specify both the cluster name and the cluster location (region or zone).</p> <p>When prompted with Do you want to continue (Y/n), press y to confirm.</p> <p>Note: You need to wait a few minutes for the cluster deployment to complete.</p> <p>When the operation completes, you should see on the Google Cloud Console Kubernetes Engine &gt; Clusters page that the cluster now has four nodes. You can modify many other cluster parameters by using the gcloud container cluster command.</p>"},{"location":"42-creating-gke-deployments-from-shell/#task-3-connect-to-a-gke-cluster","title":"Task 3. Connect to a GKE cluster","text":"<p>In this task, you use Cloud Shell to authenticate to a GKE cluster and then inspect the kubectl configuration files.</p> <p>Authentication in Kubernetes applies both to communicating with the cluster from an external client through the kube-APIserver running on the master and to cluster containers communicating within the cluster or externally. In Kubernetes, authentication can take several forms. For GKE, authentication is typically handled with OAuth2 tokens and can be managed through Cloud Identity and Access Management across the project as a whole and, optionally, through role-based access control which can be defined and configured within each cluster. In GKE, cluster containers can use service accounts to authenticate to and access external resources.</p> <p>Important</p> <p>For Kubernetes versions before 1.12, client certificates and basic authentication are not disabled by default. These are lower security methods of authentication and should be disabled to increase cluster security. (For versions 1.12 and later both of these methods are disabled by default.).</p> <p>To create a kubeconfig file with the credentials of the current user (to allow authentication) and provide the endpoint details for a specific cluster (to allow communicating with that cluster through the kubectl command-line tool), execute the following command:</p> <pre><code>gcloud container clusters get-credentials $my_cluster --zone $my_zone\n</code></pre> <p>This command creates a .kube directory in your home directory if it doesn't already exist. In the .kube directory, the command creates a file named config if it doesn't already exist, which is used to store the authentication and configuration information. The config file is typically called the kubeconfig file.</p> <p>Open the kubeconfig file with the nano text editor:</p> <pre><code>nano ~/.kube/config\n</code></pre> <p>You can now examine all of the authentication and endpoint configuration data stored in the file. Information for both clusters should appear. The information was populated during cluster creation.</p> <p>Press CTRL+X to exit the nano editor.</p> <p>Note: The kubeconfig file can contain information for many clusters. The currently active context (the cluster that kubectl commands manipulate) is indicated by the current-context property.</p> <p>You don't have to run the gcloud container clusters get-credentials command to populate the kubeconfig file for clusters that you created in the same context (the same user in the same environment), because those clusters already have their details populated when the cluster is created. But you have to run the command to connect to a cluster created by another user or in another environment. The command is also an easy way to switch the active context to a different cluster.</p>"},{"location":"42-creating-gke-deployments-from-shell/#task-4-use-kubectl-to-inspect-a-gke-cluster","title":"Task 4. Use kubectl to inspect a GKE cluster","text":"<p>In this task, you use Cloud Shell and kubectl to inspect a GKE cluster.</p> <p>After the kubeconfig file is populated and the active context is set to a particular cluster, you can use the kubectl command-line tool to execute commands against the cluster. Most such commands ultimately trigger a REST API call against the master API server, which triggers the associated action.</p> <p>In Cloud Shell, execute the following command to print out the content of the kubeconfig file:</p> <pre><code>kubectl config view\n</code></pre> <p>The sensitive certificate data is replaced with DATA+OMITTED.</p> <p>In Cloud Shell, execute the following command to print out the cluster information for the active context:</p> <pre><code>kubectl cluster-info\n</code></pre> <p>The output describes the active context cluster.</p> <p>Output (do not copy)</p> <pre><code>Kubernetes master is running at https://104.155.191.14\nGLBCDefaultBackend is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://104.155.191.14/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <p>In Cloud Shell, execute the following command to print out the active context:</p> <pre><code>kubectl config current-context\n</code></pre> <p>A line of output indicates the active context cluster.</p> <p>Output (do not copy)</p> <pre><code>gke_[PROJECT_ID]_us-central1-a_standard-cluster-1\n</code></pre> <p><code>PROJECT_ID</code> is your project ID. This information is the same as the information in the current-context property of the kubeconfig file.</p> <p>In Cloud Shell, execute the following command to print out some details for all the cluster contexts in the kubeconfig file:</p> <pre><code>kubectl config get-contexts\n</code></pre> <p>Several lines of output indicate details about the cluster you created and an indication of which is the active context cluster. In general, this command lists some details of the clusters present in the user's kubeconfig file, including any other clusters that were created by the user as well as any manually added to the kubeconfig file.</p> <p>In Cloud Shell, execute the following command to change the active context:</p> <pre><code>kubectl config use-context gke_${GOOGLE_CLOUD_PROJECT}_us-central1-a_standard-cluster-1\n</code></pre> <p>In this case you have only one cluster, so this command didn't change anything.</p> <p>However in the future you may have more than one cluster in a project. You can use this approach to switching the active context when your kubeconfig file has the credentials and configuration for several clusters already populated. This approach requires the full name of the cluster, which includes the gke prefix, the project ID, the location, and the display name, all concatenated with underscores.</p> <p>In Cloud Shell, execute the following command to view the resource usage across the nodes of the cluster:</p> <pre><code>kubectl top nodes\n</code></pre> <p>The output should look like the following example.</p> <p>Output (do not copy)</p> <pre><code>NAME                            CPU(cores)   CPU%  MEMORY(bytes)  MEMORY%\ngke-standard-cluster-1-def...   29m          3%    431Mi          16%\ngke-standard-cluster-1-def...   45m          4%    605Mi          22%\ngke-standard-cluster-1-def...   40m          4%    559Mi          21%\ngke-standard-cluster-1-def...   34m          3%    488Mi          18%\n</code></pre> <p>Another top command (<code>kubectl</code> top pods) shows similar information across all the deployed Pods in the cluster.</p> <p>In Cloud Shell, execute the following command to enable bash autocompletion for <code>kubectl</code>:</p> <pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>This command produces no output.</p> <p>In Cloud Shell, type <code>kubectl</code> followed by a space and press the Tab key twice. The shell outputs all the possible commands.</p> <p>In Cloud Shell, type <code>kubectl</code> co and press the Tab key twice. The shell outputs all commands starting with \"co\" (or any other text you type).</p>"},{"location":"42-creating-gke-deployments-from-shell/#task-5-deploy-pods-to-gke-clusters","title":"Task 5. Deploy Pods to GKE clusters","text":"<p>In this task, you use Cloud Shell to deploy Pods to GKE clusters.</p>"},{"location":"42-creating-gke-deployments-from-shell/#use-kubectl-to-deploy-pods-to-gke","title":"Use kubectl to deploy Pods to GKE","text":"<p>Kubernetes introduces the abstraction of a Pod to group one or more related containers as a single entity to be scheduled and deployed as a unit on the same node. You can deploy a Pod that is a single container from a single container image. Or a Pod can contain many containers from many container images.</p> <p>In Cloud Shell, execute the following command to deploy nginx as a Pod named nginx-1:</p> <pre><code>kubectl create deployment --image nginx nginx-1\n</code></pre> <p>This command creates a Pod named nginx with a container running the nginx image. When a repository isn't specified, the default behavior is to try and find the image either locally or in the Docker public registry. In this case, the image is pulled from the Docker public registry.</p> <p>In Cloud Shell, execute the following command to view all the deployed Pods in the active context cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>The output should look like the following example, but with a slightly different Pod name.</p> <p>Output (do not copy)</p> <pre><code>NAME                       READY     STATUS    RESTARTS   AGE\nnginx-1-74c7bbdb84-nvwsc   1/1       Running   0          9s\n</code></pre> <p>You will now enter your Pod name into a variable that we will use throughout this lab. Using variables like this can help you minimize human error when typing long names. You must type your Pod's unique name in place of [<code>your_pod_name</code>].</p> <pre><code>export my_nginx_pod=[your_pod_name]\n</code></pre> <p>Example (do not copy)</p> <pre><code>export my_nginx_pod=nginx-1-74c7bbdb84-nvwsc\n</code></pre> <p>Confirm that you have set the environment variable successfully by having the shell echo the value back to you:</p> <pre><code>echo $my_nginx_pod\n</code></pre> <p>Output (do not copy)</p> <pre><code>nginx-1-74c7bbdb84-nvwsc\n</code></pre> <p>In Cloud Shell, execute the following command to view the complete details of the Pod you just created.</p> <pre><code>kubectl describe pod $my_nginx_pod\n</code></pre> <p>The output should look like the following example. Details of the Pod, as well as its status and conditions and the events in its lifecycle, are displayed.</p> <p>Condensed Output (do not copy, edited to fit screen)</p> <pre><code>Name:           nginx-1-74c7bbdb84-nvwsc\nNamespace:      default\nNode:           gke-standard-cluster-1-default-pool-bc4ec334-0hmk/10.128.0.5\nStart Time:     Sun, 16 Dec 2018 14:29:38 -0500\nLabels:         pod-template-hash=3073668640\n                run=nginx-1\nAnnotations:    kubernetes.io/limit-ranger=LimitRanger plugin set: cpu ...\nStatus:         Running\nIP:             10.8.3.3\nControlled By:  ReplicaSet/nginx-1-74c7bbdb84\nContainers:\n  nginx-1:\n    Container ID:   docker://dce87d274e6d25300b07ec244c265d42806579fee...\n    Image:          nginx:latest\n    Image ID:       docker-pullable://nginx@sha256:87e9b6904b4286b8d41...\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Sun, 16 Dec 2018 14:29:44 -0500\n    Ready:          True\n    Restart Count:  0\n    Requests:\n      cpu:        100m\n    Environment:  &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-tok...\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          True\n  PodScheduled   True\nVolumes:\n  default-token-nphcg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nphcg\n    Optional:    false\nQoS Class:       Burstable\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason  Age   From                         Message\n  ----    ------  ----  ----                         -------\n  Normal  Sche... 1m    default-scheduler            Successf...\n  Normal  Succ... 1m    kubelet, gke-standard-cl...  MountVol...\n  Normal  Pull... 1m    kubelet, gke-standard-cl...  pulling ...\n  Normal  Pull... 1m    kubelet, gke-standard-cl...  Successf...\n  Normal  Crea... 1m    kubelet, gke-standard-cl...  Created ...\n  Normal  Star... 1m    kubelet, gke-standard-cl...  Started ...\n  ```\n\n### Push a file into a container\n\nTo be able to serve static content through the nginx web server, you must create and place a file into the container.\n\nIn Cloud Shell, type the following commands to open a file named test.html in the nano text editor.\n</code></pre> <p>nano ~/test.html</p> <pre><code>\nAdd the following text (shell script) to the empty test.html file:\n</code></pre> <p> This is title  Hello world  <p></p> <pre><code>\nPress CTRL+X, then press Y and enter to save the file and exit the nano editor.\n\nIn Cloud Shell, execute the following command to place the file into the appropriate location within the nginx container in the nginx Pod to be served statically:\n</code></pre> <p>kubectl cp ~/test.html $my_nginx_pod:/usr/share/nginx/html/test.html</p> <pre><code>\nThis command copies the `test.html` file from the local home directory to the `/usr/share/nginx/html` directory of the first container in the nginx Pod. You could specify other containers in a multi-container Pod by using the -c option, followed by the name of the container.\n\n### Expose the Pod for testing\n\nTo expose a Pod to clients outside the cluster requires a service. Services are discussed elsewhere in the course and used extensively in other labs. You can use a simple command to create a service to expose a Pod.\n\nIn Cloud Shell, execute the following command to create a service to expose our nginx Pod externally:\n</code></pre> <p>kubectl expose pod $my_nginx_pod --port 80 --type LoadBalancer</p> <pre><code>\nThis command creates a LoadBalancer service, which allows the nginx Pod to accessed from internet addresses outside of the cluster.\n\nIn Cloud Shell, execute the following command to view details about services in the cluster:\n</code></pre> <p>kubectl get services</p> <pre><code>\nThe output should look like the following example. You use the external IP address in the next step.\n\nNote: You might have to repeat the command a few times before the new service has its external IP populated.\n\nCondensed Output (do not copy, edited to fit screen)\n</code></pre> <p>NAME             TYPE          CLUSTER-IP    EXTERNAL-IP  PORT(S)      AGE kubernetes       ClusterIP     10.11.240.1          443/TCP       1h nginx-1-7...wsc  LoadBalancer  10.11.240.87      80:31695/TCP  3s <pre><code>\nThe kubernetes service is one of the default services created or used by the cluster. The nginx service that you created is also displayed.\n\nYou may need to re-run this command several times before the External IP address is displayed.\n\nCondensed Output (do not copy, edited to fit screen)\n</code></pre> <p>NAME             TYPE         CLUSTER-IP   EXTERNAL-IP    PORT(S)     AGE kubernetes       ClusterIP    10.11.240.1           443/TCP      1h nginx-1-7...wsc  LoadBalancer 10.11.240.87 104.154.177.46 80:31695/TCP 1m <pre><code>\nIn Cloud Shell, execute the following command to verify that the nginx container is serving the static HTML file that you copied.\nYou replace [`EXTERNAL_IP`] with the external IP address of your service that you obtained from the output of the previous step.\n</code></pre> <p>curl http://[EXTERNAL_IP]/test.html</p> <pre><code>\nThe file contents appear in the output. You can go to the same address in your browser to see the file rendered as HTML.\n\nExample (do not copy)\n</code></pre> <p>curl http://104.154.177.46/test.html  This is title  Hello world  <p></p> <pre><code>\nIn Cloud Shell, execute the following command to view the resources being used by the nginx Pod:\n</code></pre> <p>kubectl top pods</p> <pre><code>\nOutput (do not copy)\n</code></pre> <p>NAME                       CPU(cores)   MEMORY(bytes) nginx-1-74c7bbdb84-nvwsc   0m           2Mi</p> <pre><code>\n## Task 6. Introspect GKE Pods\n\nIn this task, you connect to a Pod to adjust settings, edit files, and make other live changes to the Pod.\n\nImportant\n\nUse this process only when troubleshooting or experimenting. Because the changes you make are not made to the source image of the Pod, they won't be present in any replicas.\n\n### Prepare the environment\n\nThe preferred way of deploying Pods and other resources to Kubernetes is through configuration files, which are sometimes called manifest files. Configuration files are typically written in the YAML syntax, specifying the details of the resource. With configuration files, you can more easily specify complex options than with a long line of command-line arguments. YAML syntax is similar to, but more concise than, JSON syntax and it enables the same kind of hierarchical structuring of objects and properties. The source repository for the lab contains sample YAML files that have been prepared for you.\n\nIn Cloud Shell enter the following command to clone the repository to the lab Cloud Shell.\n</code></pre> <p>git clone https://github.com/GoogleCloudPlatform/training-data-analyst</p> <pre><code>\nCreate a soft link as a shortcut to the working directory.\n</code></pre> <p>ln -s ~/training-data-analyst/courses/ak8s/v1.1 ~/ak8s</p> <pre><code>\nChange to the directory that contains the sample files for this lab.\n</code></pre> <p>cd ~/ak8s/GKE_Shell/</p> <pre><code>\nA sample manifest YAML file for a Pod called new-nginx-pod.yaml has been provided for you:\n</code></pre> <p>apiVersion: v1 kind: Pod metadata:   name: new-nginx   labels:     name: new-nginx spec:   containers:   - name: new-nginx     image: nginx     ports:     - containerPort: 80     ```</p> <p>To deploy your manifest, execute the following command:</p> <pre><code>kubectl apply -f ./new-nginx-pod.yaml\n</code></pre> <p>Click Check my progress to verify the objective.</p> <p>Deploy manifest file for a Pod called new-nginx</p> <p>To see a list of Pods, execute the following command:</p> <pre><code>kubectl get pods\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>NAME                       READY     STATUS    RESTARTS   AGE\nnew-nginx                  1/1       Running   0          9s\nnginx-1-74c7bbdb84-nvwsc   1/1       Running   0          55m\n</code></pre> <p>You can see your new nginx Pod as well as the one we created earlier in the lab.</p>"},{"location":"42-creating-gke-deployments-from-shell/#use-shell-redirection-to-connect-to-a-pod","title":"Use shell redirection to connect to a Pod","text":"<p>Some container images include a shell environment that you can launch. This shell environment might be more convenient than executing individual commands with kubectl. For instance, the nginx image includes a bash shell. In this task you use shell redirection to connect to the bash shell in your new nginx pod to carry out a sequence of actions.</p> <p>In Cloud Shell, execute the following command to start an interactive bash shell in the nginx container:</p> <pre><code>kubectl exec -it new-nginx /bin/bash\n</code></pre> <p>A new shell prompt appears.</p> <p>Output (do not copy)</p> <pre><code>root@new-nginx:/#\n</code></pre> <p>You have started an interactive bash shell in the container of the new-nginx Pod. If the Pod had several containers, you could specify one by name with the -c option.</p> <p>Because the nginx container image has no text editing tools by default, you need to install one.</p> <p>In Cloud Shell, in the nginx bash shell, execute the following commands to install the nano text editor:</p> <pre><code>apt-get update\napt-get install nano\n</code></pre> <p>You need to create a test.html file in the static served directory on the nginx container.</p> <p>In Cloud Shell, in the nginx bash shell, execute the following commands to switch to the static files directory and create a test.html file:</p> <pre><code>cd /usr/share/nginx/html\nnano test.html\n</code></pre> <p>In Cloud Shell, in the nginx bash shell nano session, type the following text:</p> <pre><code>&lt;html&gt; &lt;header&gt;&lt;title&gt;This is title&lt;/title&gt;&lt;/header&gt;\n&lt;body&gt; Hello world &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Press CTRL+X, then press Y and enter to save the file and exit the nano editor.</p> <p>In Cloud Shell, in the nginx bash shell, execute the following command to exit the nginx bash shell:</p> <pre><code>exit\n</code></pre> <p>To connect to and test the modified nginx container (with the new static HTML file), you could create a service. An easier way is to use port forwarding to connect to the Pod directly from Cloud Shell.</p> <p>In Cloud Shell, execute the following command to set up port forwarding from Cloud Shell to the nginx Pod (from port 10081 of the Cloud Shell VM to port 80 of the nginx container):</p> <pre><code>kubectl port-forward new-nginx 10081:80\n</code></pre> <p>The output should look like the example.</p> <p>Output (do not copy)</p> <pre><code>Forwarding from 127.0.0.1:10081 -&gt; 80\nForwarding from [::1]:10081 -&gt; 80\n</code></pre> <p>This is a foreground process, so you need to open another Cloud Shell instance to test.</p> <p>In the Cloud Shell menu bar, click the plus sign (+) icon to start a new Cloud Shell session.</p> <p>A second Cloud Shell session appears in your Cloud Shell window. You can switch between sessions by clicking the titles in the menu bar.</p> <p>In the second Cloud Shell session, execute the following command to test the modified nginx container through the port forwarding:</p> <pre><code>curl http://127.0.0.1:10081/test.html\n</code></pre> <p>The HTML text you placed in the test.html file is displayed.</p> <pre><code>&lt;html&gt; &lt;header&gt;&lt;title&gt;This is title&lt;/title&gt;&lt;/header&gt;\n&lt;body&gt; Hello world &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"42-creating-gke-deployments-from-shell/#view-the-logs-of-a-pod","title":"View the logs of a Pod","text":"<p>In the Cloud Shell menu bar, click the plus sign (+) icon to start another new Cloud Shell session. A third Cloud Shell session appears in your Cloud Shell window. As before, you can switch sessions by clicking them in the menu bar.</p> <p>In the third Cloud Shell window, execute the following command to display the logs and to stream new logs as they arrive (and also include timestamps) for the new-nginx Pod:</p> <pre><code>kubectl logs new-nginx -f --timestamps\n</code></pre> <p>You will see the logs display in this new window Return to the second Cloud Shell window and re-run the curl command to generate some traffic on the Pod. Review the additional log messages as they appear in the third Cloud Shell window.</p>"},{"location":"43-setting-up-react/","title":"Steps to run react in local environment","text":""},{"location":"43-setting-up-react/#install-react-cli","title":"install react cli","text":"<pre><code>npm install -g create-react-app\n</code></pre>"},{"location":"43-setting-up-react/#create-app","title":"create app","text":"<pre><code>create-react-app my-react-app-name\n</code></pre>"},{"location":"43-setting-up-react/#go-to-project-folder","title":"go to project folder","text":"<pre><code>cd my-react-app-name\n</code></pre>"},{"location":"43-setting-up-react/#install-dependencies","title":"install dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"43-setting-up-react/#start-live-server","title":"start live server","text":"<pre><code>npm start\n</code></pre>"},{"location":"44-redis-with-gcp-cloud-func/","title":"44 redis with gcp cloud func","text":"<p>The official documentation to connect Redis with cloud functions can be found here</p> <p>Setting up Redis with GCP Cloud Functions require the following steps:</p> <ol> <li>Set up local environment GCP permission </li> <li>Create redis instance </li> <li>Create a VPC network</li> <li>Create a subnet </li> <li>Create a serverless VPC Access Connector</li> <li>Deploy the function with the connector access</li> </ol>"},{"location":"44-redis-with-gcp-cloud-func/#step-1-set-up-the-local-environment-gcp-permission","title":"Step 1: Set up the local environment GCP permission","text":"<p>Set up the variable in terminal:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS='/path/to/your/client_secret.json'\n</code></pre> <p>This will not enable VPC connection from local environment. #ToDo</p>"},{"location":"44-redis-with-gcp-cloud-func/#step-2-redis-instance-creation","title":"Step 2: Redis instance creation","text":"<p>In GCP, the product is called Memorystore for Redis. It is a serverless Redis service.</p> <p>Documentation is here</p> <p>gcloud command:</p> <pre><code>gcloud redis instances create br-pixel-redis \\                                                    \n--size=1 \\\n--region us-central1 \\\n--redis-version redis_6_x\n</code></pre> <p>Save the configuration of the redis instance for using in the function.</p> <pre><code>gcloud redis instances describe br-pixel-redis --region us-central1 &gt; redis-function/redis-conf.yaml\n</code></pre> <p>It will be something like this:</p> <pre><code>authorizedNetwork: projects/my-project/global/networks/default\ncreateTime: '2018-04-09T21:47:56.824081Z'\ncurrentLocationId: us-central1-a\nhost: 10.0.0.27\nlocationId: us-central1-a\nmemorySizeGb: 2\nname: projects/my-project/locations/us-central1/instances/myinstance\nnetworkThroughputGbps: 2\nport: 6379\nredisVersion: REDIS_6_X\nreservedIpRange: 10.0.0.24/29\nstate: READY\ntier: BASIC\n</code></pre> <p>We will be using the <code>host</code> and <code>port</code> to connect with the instance.</p>"},{"location":"44-redis-with-gcp-cloud-func/#step-3-create-a-vpc-network","title":"Step 3: Create a VPC network","text":"<p>Virtual Private Cloud (VPC) networks allow for secured connection between resources within GCP.</p> <p>I have used the <code>default</code> VPC network of the GCP project for avoiding firewall configurations. </p> <p>Should we need to create one, here is the documentation to create a new VPC network.</p>"},{"location":"44-redis-with-gcp-cloud-func/#step-4-create-a-subnet-for-the-network","title":"Step 4: Create a subnet for the network","text":"<p>A network must have at least one subnet before you can use it. Auto mode VPC networks create subnets in each region automatically. Custom mode VPC networks start with no subnets, giving you full control over subnet creation.</p> <p>However, we have create a new one because subnets used for VPC connectors must have a netmask of 28.</p> <p>Here is the documentation to add a subnet.</p> <pre><code>gcloud compute networks subnets create br-pixel-cache-subnet \\                      \n--network default \\\n--range 10.0.0.0/28 \\\n--region us-central1\n</code></pre> <p>Valid network ranges can be found here</p>"},{"location":"44-redis-with-gcp-cloud-func/#step-5-create-a-serverless-vpc-connector","title":"Step 5: Create a serverless VPC Connector","text":"<p>Serverless VPC access enables direct connection to the Virtual Private Cloud network from serverless environments such as Cloud Run, Cloud Functions or App Engine. Documentation is here</p> <pre><code>gcloud compute networks vpc-access connectors create br-pixel-cache-vpc \\               \n--region us-central1 \\\n--subnet br-pixel-cache-subnet\n</code></pre>"},{"location":"44-redis-with-gcp-cloud-func/#step-6-deploy-the-cloud-function","title":"Step 6: Deploy the Cloud Function","text":"<p>The cloud function needs to point to the Connector using the following argument in the deploy command:</p> <pre><code>gcloud functions deploy [FUNCTION_NAME] \\\n--runtime python37 \\\n--trigger-http \\\n--region [REGION] \\\n--vpc-connector projects/[PROJECT_ID]/locations/[REGION]/connectors/[CONNECTOR_NAME] \\\n--set-env-vars REDISHOST=[REDIS_IP],REDISPORT=[REDIS_PORT]\n</code></pre> <p>For example:</p> <pre><code>gcloud functions deploy br-pixel-test \\\n--entry-point=main \\\n--allow-unauthenticated \\\n--runtime=python38 \\\n--region=us-central1 \\\n--trigger-http \\\n--memory 256MB \\\n--min-instances 1 \\\n--vpc-connector projects/recharge-webhooks/locations/us-central1/connectors/br-pixel-cache-vpc\n</code></pre>"},{"location":"45-john-doerr-okr-ted-talk/","title":"OKRs","text":"<p>Objectives &amp; key results, or OKRs, are a simple goal-setting system and they work for organizations, they work for teams, they even work for individuals.</p> <p>The objectives are what you want to have accomplished. The key results are how I\u2019m going to get that done. </p>"},{"location":"45-john-doerr-okr-ted-talk/#the-why-behind-the-goal","title":"The why behind the goal","text":"<p>The teams must develop a shared sense of purpose that defines why they do what they do. Without the \"why\", the what and how of OKR might seem to be a meaningless exercise.</p>"},{"location":"45-john-doerr-okr-ted-talk/#the-what","title":"The what","text":"<p>Objectives are what needs to be achieved. They need to be: * Significant * Concrete * Action oriented * Inspirational</p>"},{"location":"45-john-doerr-okr-ted-talk/#the-hows-are-the-key-results","title":"The how's are the key results","text":"<p>Key results define how to achieve the objectives.</p> <p>They need to be: * Specific * Time bound * Aggressive but realistic * Measurable and verifiable</p>"},{"location":"45-john-doerr-okr-ted-talk/#in-life-and-in-work","title":"In life and in work","text":"<p>OKRs can be applied to life. What are your values, life's goals and how do you want to measure them? Write them down every quarter and evaluate your progress.</p>"},{"location":"45-john-doerr-okr-ted-talk/#references","title":"References","text":"<ol> <li>John Doerr Ted Talk</li> </ol>"}]}